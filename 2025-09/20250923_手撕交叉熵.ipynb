{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f84f180c",
   "metadata": {},
   "source": [
    "### 交叉熵损失函数\n",
    "\n",
    "\n",
    "#### **1. 二分类交叉熵（BCE, Binary Cross-Entropy）**\n",
    "**公式**：\n",
    "$$ L_{BCE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $$\n",
    "\n",
    "**参数说明**：\n",
    "- $ N $：样本总数\n",
    "- $ y_i $：第 i 个样本的真实标签（取值为 0 或 1）\n",
    "- $ \\hat{y}_i $：模型对第 i  个样本的预测概率（需经 Sigmoid 激活，范围 (0,1)）\n",
    "\n",
    "\n",
    "#### **2. 多分类交叉熵（CCE, Categorical Cross-Entropy）**\n",
    "**公式**：\n",
    "$$ L_{CCE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c}) $$\n",
    "\n",
    "**参数说明**：\n",
    "- $ N $：样本总数\n",
    "- $ C $：类别总数\n",
    "- $ y_{i,c} $：第 i 个样本在第 c 类的真实标签（One-Hot 编码，1 表示属于该类，0 表示不属于）\n",
    "- $ \\hat{y}_{i,c} $：模型预测第 i 个样本属于第 c 的概率（需经 Softmax 激活，满足 $ \\sum_{c=1}^{C} \\hat{y}_{i,c} = 1 $）\n",
    "\n",
    "\n",
    "#### **核心区别**\n",
    "| **场景**| 二分类交叉熵| 多分类交叉熵|\n",
    "|----------------|----------------------------|----------------------------|\n",
    "| **标签类型**| 单个二值标签（0/1）| One-Hot 向量（类别互斥）|\n",
    "| **激活函数**| Sigmoid（输出单个概率）| Softmax（输出概率分布）|\n",
    "| **求和维度**| 无类别维度（单标签）| 需对类别维度求和（多标签） |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0f040",
   "metadata": {},
   "source": [
    "## 手撕交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5acd18b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def manual_softmax(logits):\n",
    "    # 防止数值溢出，减去最大值\n",
    "    max_vals = torch.max(logits, dim=-1, keepdim=True).values\n",
    "    exp_logits = torch.exp(logits - max_vals)\n",
    "    return exp_logits / torch.sum(exp_logits, dim=-1, keepdim=True)\n",
    "\n",
    "def manual_cross_entropy(logits, labels):\n",
    "    # 计算 softmax 概率\n",
    "    probs = manual_softmax(logits)\n",
    "\n",
    "    # 数值稳定性处理：防止log(0)\n",
    "    epsilon = 1e-7\n",
    "    probs = torch.clamp(probs, min=epsilon, max=1.0 - epsilon)\n",
    "    \n",
    "    # 将标签转为 one-hot 编码\n",
    "    one_hot = torch.zeros_like(probs)\n",
    "    one_hot[torch.arange(len(labels)), labels] = 1.0\n",
    "    # 计算交叉熵: -sum(p * log(q))\n",
    "    log_probs = torch.log(probs)\n",
    "    cross_entropy = -torch.sum(one_hot * log_probs) / len(labels)\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39bcd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ebe4ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model raw logits:\n",
      " tensor([[-0.0532,  0.6314,  0.4303],\n",
      "        [-0.1757,  1.1589,  0.5889]], grad_fn=<AddmmBackward0>)\n",
      "Multiclass Cross-Entropy Loss: 1.3504061698913574\n",
      "PyTorch CrossEntropyLoss: 1.3504061698913574\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# --- 使用示例 ---\n",
    "# 假设一个简单的多分类模型（例如，一个线性层，输出3个值）\n",
    "class SimpleMultiClassNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) # 输出原始logits，不在这里做Softmax\n",
    "\n",
    "# 创建模型（输入特征2维，输出3类）\n",
    "model = SimpleMultiClassNN(input_size=2, num_classes=3)\n",
    "\n",
    "# 示例数据：2个样本，每个样本2个特征\n",
    "features = torch.tensor([[0.1, 0.2],\n",
    "                         [0.7, 0.8]], dtype=torch.float32)\n",
    "# 真实标签：第一个样本属于第0类，第二个样本属于第2类\n",
    "labels = torch.tensor([0, 2], dtype=torch.long) # 注意 dtype 是 long\n",
    "\n",
    "# 前向传播，得到原始logits\n",
    "logits = model(features)\n",
    "print(\"Model raw logits:\\n\", logits)\n",
    "\n",
    "# 计算损失\n",
    "loss = manual_cross_entropy(logits, labels)\n",
    "print(\"Multiclass Cross-Entropy Loss:\", loss.item())\n",
    "\n",
    "# 实际上，PyTorch提供了更高效且数值稳定的内置函数：\n",
    "# nn.CrossEntropyLoss (它已经将Softmax和交叉熵计算合并优化)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss_pytorch = criterion(logits, labels)\n",
    "print(\"PyTorch CrossEntropyLoss:\", loss_pytorch.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamafactory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
