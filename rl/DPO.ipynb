{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11b993230>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# DPO（Direct Preference Optimization）算法实现\n",
    "# DPO通过人类偏好数据直接优化语言模型，使其生成更符合人类偏好的输出\n",
    "# 这里面使用了一个偏好prefer以及两个reject的格式\n",
    "# ===============================================================================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "# 创建简化版的Llama模型作为策略模型（将被优化的模型）\n",
    "policy_model = LlamaForCausalLM(config=LlamaConfig(vocab_size=12, num_hidden_layers=3, hidden_size=32))\n",
    "# 创建参考模型（通常是SFT模型，在训练过程中保持不变）\n",
    "reference_model = deepcopy(policy_model)  # 深度复制确保两个模型初始参数完全相同\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "beta = 0.1  # DPO的温度系数，控制策略模型与参考模型的偏离程度，值越小允许偏离越大\n",
    "\n",
    "# 准备训练数据\n",
    "# 在DPO中，我们需要提示(prompt)、优选回答(chosen/good)和拒绝回答(rejected/bad)\n",
    "prompt_ids = [1, 2, 3, 4, 5, 6]  # 输入提示的token IDs\n",
    "good_response_ids = [7, 8, 9, 2]  # 优质回答的token IDs\n",
    "# 多个低质量回答的示例，每个都是token IDs的列表\n",
    "bad_response_ids_list = [[1, 2, 0, 0], [4, 5, 6, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 2],\n",
       "        [1, 2, 3, 4, 5, 6, 1, 2, 0, 0],\n",
       "        [1, 2, 3, 4, 5, 6, 4, 5, 6, 0]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建模型输入：将提示与回答拼接\n",
    "# 创建包含多个序列的批次：[提示+优质回答, 提示+低质回答1, 提示+低质回答2, ...]\n",
    "input_ids = torch.LongTensor(\n",
    "    [prompt_ids + good_response_ids, *[prompt_ids + bad_response_ids for bad_response_ids in bad_response_ids_list]]\n",
    ")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100,    7,    8,    9,    2],\n",
       "        [-100, -100, -100, -100, -100, -100,    1,    2,    0,    0],\n",
       "        [-100, -100, -100, -100, -100, -100,    4,    5,    6,    0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备用于计算语言模型损失的标签\n",
    "# 在语言模型训练中，标签是输入向右移动一位（预测下一个token）\n",
    "# -100表示在计算损失时忽略该位置（这里忽略提示部分）\n",
    "labels = torch.LongTensor(\n",
    "    [\n",
    "        [-100] * len(prompt_ids) + good_response_ids,\n",
    "        *[[-100] * len(prompt_ids) + bad_response_ids for bad_response_ids in bad_response_ids_list]\n",
    "    ]\n",
    ")# 向右移动一位，因为我们预测的是下一个token\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 9])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels[:, 1:]  # 向右移动一位，因为我们预测的是下一个token\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建掩码，用于标识哪些位置参与损失计算（即回答部分）\n",
    "# 注意：labels现在已经被截断了，所以loss_mask也需要相应调整\n",
    "loss_mask = (labels != -100)\n",
    "print(loss_mask.shape)\n",
    "loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 7, 8, 9, 2],\n",
       "        [0, 0, 0, 0, 0, 1, 2, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 4, 5, 6, 0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将-100替换为0，因为在gather操作中-100是无效索引\n",
    "labels[labels == -100] = 0\n",
    "print(labels.shape)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([3, 10, 12])\n",
      "past_key_values: <class 'transformers.cache_utils.DynamicCache'>\n"
     ]
    }
   ],
   "source": [
    "output = policy_model(input_ids)\n",
    "for key, value in output.items():  # 如果是ModelOutput对象，可以用output.__dict__.items()\n",
    "    if hasattr(value, \"shape\"):\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9, 12])\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# 计算策略模型（policy model）的对数概率\n",
    "# ===============================================================================\n",
    "# 前向传播，获取每个token位置的预测logits\n",
    "logits = policy_model(input_ids)[\"logits\"][:, :-1, :]  # 去掉最后一个位置，与label对齐\n",
    "print(logits.shape)\n",
    "# logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 9])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将logits转换为对数概率，并提取每个位置上正确token的对数概率\n",
    "per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "per_token_logps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -9.9317,  -9.9118, -10.1279], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 仅对回答部分（loss_mask=True的位置）求和，得到每个序列的总对数概率\n",
    "all_logps = (per_token_logps * loss_mask).sum(-1)\n",
    "all_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分离优质回答和低质量回答的对数概率\n",
    "policy_good_logps, policy_bad_logps = all_logps[:1], all_logps[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.9317], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_good_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -9.9118, -10.1279], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_bad_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9, 12])\n",
      "logits:\n",
      " tensor([[[ 0.0866, -0.0973,  0.0498,  0.1155, -0.0526,  0.0442,  0.0228,\n",
      "           0.1734, -0.1017, -0.1903,  0.1076,  0.1274],\n",
      "         [ 0.0451, -0.0140,  0.1971, -0.0085, -0.1340,  0.1808, -0.0080,\n",
      "           0.3333, -0.0461, -0.0078,  0.0195, -0.1018],\n",
      "         [-0.1777,  0.0361, -0.0626, -0.0751, -0.2104,  0.1253, -0.0744,\n",
      "           0.0623, -0.0447, -0.0415, -0.0341, -0.0650],\n",
      "         [ 0.1003,  0.0340, -0.1017,  0.1587, -0.1250,  0.0822, -0.1308,\n",
      "           0.1367,  0.0546, -0.0573,  0.0363,  0.1325],\n",
      "         [ 0.1913,  0.0734,  0.1178,  0.0973, -0.1439,  0.0687, -0.0017,\n",
      "           0.2215, -0.0793,  0.0571, -0.0413,  0.1225],\n",
      "         [-0.0388,  0.1097, -0.1538,  0.0450, -0.0282, -0.1039,  0.0024,\n",
      "           0.1332,  0.0980, -0.0092,  0.0684,  0.0841],\n",
      "         [-0.1127, -0.1991,  0.2202,  0.0476,  0.0858,  0.0211,  0.1059,\n",
      "          -0.0135, -0.0940, -0.1337, -0.1585, -0.0056],\n",
      "         [-0.0317,  0.0170, -0.1491,  0.1799,  0.2007, -0.1777,  0.2137,\n",
      "          -0.1639,  0.1450,  0.0409, -0.0538,  0.2372],\n",
      "         [ 0.1398,  0.1857,  0.0517,  0.1341, -0.2243,  0.1527,  0.0229,\n",
      "           0.0583, -0.0788,  0.3040, -0.1246,  0.0633]],\n",
      "\n",
      "        [[ 0.0866, -0.0973,  0.0498,  0.1155, -0.0526,  0.0442,  0.0228,\n",
      "           0.1734, -0.1017, -0.1903,  0.1076,  0.1274],\n",
      "         [ 0.0451, -0.0140,  0.1971, -0.0085, -0.1340,  0.1808, -0.0080,\n",
      "           0.3333, -0.0461, -0.0078,  0.0195, -0.1018],\n",
      "         [-0.1777,  0.0361, -0.0626, -0.0751, -0.2104,  0.1253, -0.0744,\n",
      "           0.0623, -0.0447, -0.0415, -0.0341, -0.0650],\n",
      "         [ 0.1003,  0.0340, -0.1017,  0.1587, -0.1250,  0.0822, -0.1308,\n",
      "           0.1367,  0.0546, -0.0573,  0.0363,  0.1325],\n",
      "         [ 0.1913,  0.0734,  0.1178,  0.0973, -0.1439,  0.0687, -0.0017,\n",
      "           0.2215, -0.0793,  0.0571, -0.0413,  0.1225],\n",
      "         [-0.0388,  0.1097, -0.1538,  0.0450, -0.0282, -0.1039,  0.0024,\n",
      "           0.1332,  0.0980, -0.0092,  0.0684,  0.0841],\n",
      "         [ 0.0582, -0.1518,  0.0500, -0.0268, -0.0447, -0.0035, -0.0795,\n",
      "           0.0316, -0.0550, -0.2344,  0.1275,  0.2449],\n",
      "         [ 0.0490, -0.0615,  0.1909, -0.0592, -0.1169,  0.1324, -0.0816,\n",
      "           0.3045, -0.0194, -0.0907,  0.0738, -0.0781],\n",
      "         [-0.1353, -0.1085, -0.1615,  0.0538, -0.0436, -0.2422,  0.0429,\n",
      "           0.2730, -0.0025,  0.0523,  0.1363, -0.0233]],\n",
      "\n",
      "        [[ 0.0866, -0.0973,  0.0498,  0.1155, -0.0526,  0.0442,  0.0228,\n",
      "           0.1734, -0.1017, -0.1903,  0.1076,  0.1274],\n",
      "         [ 0.0451, -0.0140,  0.1971, -0.0085, -0.1340,  0.1808, -0.0080,\n",
      "           0.3333, -0.0461, -0.0078,  0.0195, -0.1018],\n",
      "         [-0.1777,  0.0361, -0.0626, -0.0751, -0.2104,  0.1253, -0.0744,\n",
      "           0.0623, -0.0447, -0.0415, -0.0341, -0.0650],\n",
      "         [ 0.1003,  0.0340, -0.1017,  0.1587, -0.1250,  0.0822, -0.1308,\n",
      "           0.1367,  0.0546, -0.0573,  0.0363,  0.1325],\n",
      "         [ 0.1913,  0.0734,  0.1178,  0.0973, -0.1439,  0.0687, -0.0017,\n",
      "           0.2215, -0.0793,  0.0571, -0.0413,  0.1225],\n",
      "         [-0.0388,  0.1097, -0.1538,  0.0450, -0.0282, -0.1039,  0.0024,\n",
      "           0.1332,  0.0980, -0.0092,  0.0684,  0.0841],\n",
      "         [ 0.1060, -0.0173, -0.1268,  0.1407, -0.0923,  0.0039, -0.1663,\n",
      "           0.0919,  0.0906, -0.0861,  0.0872,  0.1251],\n",
      "         [ 0.1998,  0.0819,  0.1310,  0.1058, -0.1417,  0.0663, -0.0073,\n",
      "           0.2277, -0.0591,  0.0586, -0.0246,  0.1072],\n",
      "         [-0.0307,  0.0940, -0.1423,  0.0466, -0.0238, -0.1003,  0.0327,\n",
      "           0.1332,  0.0729, -0.0054,  0.0595,  0.0636]]])\n",
      "torch.Size([3, 9])\n",
      "per_token_logps:\n",
      " tensor([[-2.4276, -2.4864, -2.6197, -2.4163, -2.3558, -2.3725, -2.5663, -2.4931,\n",
      "         -2.4997],\n",
      "        [-2.4276, -2.4864, -2.6197, -2.4163, -2.3558, -2.3960, -2.4352, -2.4645,\n",
      "         -2.6161],\n",
      "        [-2.4276, -2.4864, -2.6197, -2.4163, -2.3558, -2.5339, -2.4994, -2.5594,\n",
      "         -2.5352]])\n",
      "torch.Size([3])\n",
      "all_logps\n",
      " tensor([ -9.9317,  -9.9118, -10.1279])\n",
      "torch.Size([1])\n",
      "reference_good_logps:\n",
      " tensor([-9.9317])\n",
      "torch.Size([2])\n",
      "reference_bad_logps\n",
      " tensor([ -9.9118, -10.1279])\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# 计算参考模型（reference model）的对数概率\n",
    "# ===============================================================================\n",
    "with torch.no_grad():  # 不计算梯度，因为参考模型不需要更新\n",
    "    # 重复与策略模型相同的步骤\n",
    "    logits = reference_model(input_ids)[\"logits\"][:, :-1, :]\n",
    "    print(logits.shape)\n",
    "    print(\"logits:\\n\",logits)\n",
    "    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "    print(per_token_logps.shape)\n",
    "    print(\"per_token_logps:\\n\",per_token_logps)\n",
    "    all_logps = (per_token_logps * loss_mask).sum(-1)\n",
    "    print(all_logps.shape)\n",
    "    print(\"all_logps\\n\",all_logps)\n",
    "    reference_good_logps, reference_bad_logps = all_logps[:1], all_logps[1:]\n",
    "    print(reference_good_logps.shape)\n",
    "    print(\"reference_good_logps:\\n\",reference_good_logps)\n",
    "    print(reference_bad_logps.shape)\n",
    "    print(\"reference_bad_logps\\n\",reference_bad_logps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# 计算DPO损失\n",
    "# DPO的核心思想：增大策略模型对优质回答的概率，同时减小对低质量回答的概率\n",
    "# ===============================================================================\n",
    "# 计算DPO的logits：(策略模型相对于参考模型对好回答的提升) - (对坏回答的提升)\n",
    "logits = (policy_good_logps - reference_good_logps) - (policy_bad_logps - reference_bad_logps)\n",
    "# 应用logsigmoid函数并乘以beta控制优化强度，取负值（因为要最小化损失）\n",
    "loss = -F.logsigmoid(beta * logits).mean()  # 对所有样本取平均\n",
    "\n",
    "# 输出损失值\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamafactory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
