{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26556cf8",
   "metadata": {},
   "source": [
    "\n",
    "## 大模型中，编码器结构和解码器结构的区别？\n",
    "编码器用于处理理解任务（如分类，语义分析），推理时可一次性处理全部输入，故而训练时，mask是双向建模；\n",
    "\n",
    "解码器用于处理生成任务（如翻译，文本生成），推理时时逐个token预测，之前预测的token会当作当前的输入，故而训练时，mask时单方向的。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| 特性              | 解码器 (Decoder Layer)       | 编码器 (Encoder Layer)       |\n",
    "|-------------------|------------------------------|------------------------------|\n",
    "| 自注意力掩码      | 因果掩码（只能看历史信息）| 无掩码（所有 Token 互相可见） |\n",
    "| 额外注意力层      | 有（编码器-解码器注意力）| 无                           |\n",
    "| 训练/推理行为     | 推理时需自回归生成（串行）| 始终并行计算                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fa231",
   "metadata": {},
   "source": [
    "### 编码器实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd063b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, batch_size, d_model]\n",
    "        # 双向Attention：所有Token相互可见，无Mask\n",
    "        attn_output, _ = self.self_attn(x, x, x, need_weights=False)\n",
    "        x = x + attn_output  # 残差连接\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# 示例用法\n",
    "encoder = TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "src = torch.randn(10, 32, 512)  # [seq_len, batch_size, d_model]\n",
    "output = encoder(src)  # 一次性处理全部输入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea35d1",
   "metadata": {},
   "source": [
    "### 解码器结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e93403e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n",
      "torch.Size([2, 32, 512])\n",
      "torch.Size([3, 32, 512])\n",
      "torch.Size([4, 32, 512])\n",
      "torch.Size([5, 32, 512])\n",
      "torch.Size([6, 32, 512])\n",
      "torch.Size([7, 32, 512])\n",
      "torch.Size([8, 32, 512])\n",
      "torch.Size([9, 32, 512])\n",
      "torch.Size([10, 32, 512])\n",
      "torch.Size([11, 32, 512])\n",
      "torch.Size([12, 32, 512])\n",
      "torch.Size([13, 32, 512])\n",
      "torch.Size([14, 32, 512])\n",
      "torch.Size([15, 32, 512])\n",
      "torch.Size([16, 32, 512])\n",
      "torch.Size([17, 32, 512])\n",
      "torch.Size([18, 32, 512])\n",
      "torch.Size([19, 32, 512])\n",
      "torch.Size([20, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, nhead)  # 可选（用于Encoder-Decoder结构）\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output=None):\n",
    "        # x: [tgt_seq_len, batch_size, d_model]\n",
    "        # 1. 自回归Self-Attention：因果Mask（仅能看到历史信息）\n",
    "        seq_len = x.size(0)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(x.device)\n",
    "        self_attn_output, _ = self.self_attn(x, x, x, attn_mask=causal_mask)\n",
    "        x = x + self_attn_output\n",
    "        \n",
    "        # 2. 如果是Encoder-Decoder结构，可加入Cross-Attention（无Mask）\n",
    "        if encoder_output is not None:\n",
    "            cross_attn_output, _ = self.cross_attn(x, encoder_output, encoder_output)\n",
    "            x = x + cross_attn_output\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# 示例用法（自回归生成）\n",
    "decoder = TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "tgt = torch.randn(1, 32, 512)  # 初始输入（如<BOS> Token）\n",
    "encoder_output = torch.randn(10, 32, 512)  # 假设编码器输出\n",
    "\n",
    "# 模拟自回归生成（逐个Token预测）\n",
    "for _ in range(20):  # 生成20个Token\n",
    "    output = decoder(tgt, encoder_output)\n",
    "    print(output.shape)\n",
    "    next_token = output[-1:, :, :]  # 取最后一个Token作为预测结果\n",
    "    tgt = torch.cat([tgt, next_token], dim=0)  # 将预测结果追加到输入中"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
