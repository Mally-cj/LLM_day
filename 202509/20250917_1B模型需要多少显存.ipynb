{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6608d0c",
   "metadata": {},
   "source": [
    "# 🙋1B 模型需要多少显存？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529ee9b",
   "metadata": {},
   "source": [
    "### 一. 1B的概念？\n",
    "1B指的是有10亿（1 billion）参数量\n",
    "\n",
    "如果以float32存储的话，那就是每个参数用4字节，则1B模型要$4*10^{9}$个Bytes\n",
    "\n",
    "由于\n",
    "- 1GB = 1024 MB\n",
    "- 1MB = 1024 KB\n",
    "- 1KB = 1024 Bytes\n",
    "\n",
    "则 $4*10^{10}/(1024*1024)= 3814 GB$ 或者 3.73 GB显存\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4f0370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1B模型以float32存储大约需要显存: 3814.70 MB\n",
      "1B模型以float32存储大约需要显存: 3.73 GB\n"
     ]
    }
   ],
   "source": [
    "MB=4*(10**9)/(1024**2)\n",
    "print(\"1B模型以float32存储大约需要显存: %.2f MB\" % MB)\n",
    "GB=MB/1024\n",
    "print(\"1B模型以float32存储大约需要显存: %.2f GB\" % GB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e19edc",
   "metadata": {},
   "source": [
    "#### 其他精度\n",
    "\n",
    "| 数据类型       | 单个参数占用字节 | 说明（应用场景）                     |\n",
    "|----------------|------------------|--------------------------------------|\n",
    "| FP32（单精度） | 4 字节           | 高精度场景（如模型训练），占用空间最大 |\n",
    "| FP16（半精度） | 2 字节           | 主流推理 / 训练场景，平衡精度与空间   |\n",
    "| BF16（脑半精度）| 2 字节           | 类似 FP16，部分芯片（如 GPU）优化支持 |\n",
    "| INT8（8 位整数） | 1 字节           | 低精度推理（如边缘设备、轻量化部署）  |\n",
    "| INT4（4 位整数） | 0.5 字节         | 超轻量化场景（极致压缩，精度有损失）  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d3608",
   "metadata": {},
   "source": [
    "### 二. 训练1B模型的显存开销有哪些？\n",
    "\n",
    "如果只使用随机梯度下降（SGD）的优化方法，那么显存的开销只有三部分：\n",
    "- 模型参数 :1B参数\n",
    "- 梯度：反向传播时，每个参数计算1个梯度，即1B参数\n",
    "- 前向传播中间激活值，这和batchsize，seqlenth相关。\n",
    "\n",
    "而如果用AdamW 优化器，还要新增加2B参数作为优化器状态。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b487465",
   "metadata": {},
   "source": [
    "#### 对比SGD和 AdamW 的更新公式：\n",
    "\n",
    "- SGD: $ \\theta_t = \\theta_{t-1} - \\eta \\cdot g_t $\n",
    "\n",
    "\n",
    "- AdamW: $  \\theta_t = \\theta_{t-1} - \\eta \\cdot \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\theta_{t-1} \\right) $\n",
    "    - $\\theta_t$ 是第 $t$ 步更新后的参数；\n",
    "    - $\\theta_{t-1}$ 是第 $t-1$ 步的参数；\n",
    "    - $\\eta$ 是学习率；\n",
    "    - $g_t$ 是第 $t$ 步计算得到的梯度（基于单个样本或小批量样本计算的梯度）。\n",
    "    - $m_t$  是当前的一阶矩估计（动量）， $  m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t$\n",
    "    - $v_t$  是当前的二阶矩估计（平方梯度的指数移动平均），$v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2$\n",
    "\n",
    "\n",
    "故而使用AdamW需要使用额外2倍空间\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a6eb9",
   "metadata": {},
   "source": [
    "## 三. 待补充问题\n",
    "\n",
    "中间激活值和batchsize，seqlength是怎么关联的，线性关系吗？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
