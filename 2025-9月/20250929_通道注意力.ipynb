{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8787b2fc",
   "metadata": {},
   "source": [
    "## 通道注意力\n",
    "\n",
    "**通道注意力是什么?**\n",
    "\n",
    "通道注意力通过对各通道进行加权组合，从整体上突出重要通道、抑制次要通道；若从单个通道角度来看，其效果可等价为一次仿射变换（缩放与平移）。\n",
    "\n",
    "\n",
    "\n",
    "**不同任务下的注意力机制含义?**\n",
    "\n",
    "- NLP 任务：主要在序列维度上建模（词与词之间的关系），注意力机制用于突出文本序列中关键的 token。\n",
    "\n",
    "- 图像任务：注意力可以作用于不同维度\n",
    "    - 通道注意力：强调重要通道的信息，抑制次要通道。\n",
    "    - 空间注意力：突出不同 patch 或像素的位置重要性。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**位置编码问题**\n",
    "\n",
    "- 文本注意力：一维位置编码是必须的，用于保留序列顺序信息。\n",
    "\n",
    "- 图像空间注意力：二维位置编码通常是必须的，尤其在全局注意力或 Transformer 架构中，用于保留像素或 patch 的空间关系。\n",
    "\n",
    "- 图像通道注意力：输入通常是经过卷积提取的特征图，已经隐含空间信息，因此不需要额外的位置编码。\n",
    "\n",
    "\n",
    "下面代码是通道注意力的实现\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b48c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mq\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mshape\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mtorch\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mSize\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;245m[\u001b[39m\u001b[38;5;36m2\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m512\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m16\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m16\u001b[39m\u001b[38;5;245m]\u001b[39m\u001b[38;5;245m)\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mq\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mshape\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mtorch\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mSize\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;245m[\u001b[39m\u001b[38;5;36m2\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m256\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m512\u001b[39m\u001b[38;5;245m]\u001b[39m\u001b[38;5;245m)\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mw_\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mshape\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mtorch\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mSize\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;245m[\u001b[39m\u001b[38;5;36m2\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m256\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m256\u001b[39m\u001b[38;5;245m]\u001b[39m\u001b[38;5;245m)\u001b[39m\n",
      "\u001b[38;5;247mic\u001b[39m\u001b[38;5;245m|\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mout\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mshape\u001b[39m\u001b[38;5;245m:\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;247mtorch\u001b[39m\u001b[38;5;245m.\u001b[39m\u001b[38;5;247mSize\u001b[39m\u001b[38;5;245m(\u001b[39m\u001b[38;5;245m[\u001b[39m\u001b[38;5;36m2\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m512\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m16\u001b[39m\u001b[38;5;245m,\u001b[39m\u001b[38;5;245m \u001b[39m\u001b[38;5;36m16\u001b[39m\u001b[38;5;245m]\u001b[39m\u001b[38;5;245m)\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 16, 16])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from icecream import ic\n",
    "def Normalize(in_channels, num_groups=32):\n",
    "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "\n",
    "\n",
    "def nonlinearity(x):\n",
    "    # swish\n",
    "    return x*torch.sigmoid(x)\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.k = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.v = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1,\n",
    "                                        padding=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "        ic(q.shape)\n",
    "\n",
    "        # compute attention\n",
    "        b,c,h,w = q.shape\n",
    "        q = q.reshape(b,c,h*w)\n",
    "        q = q.permute(0,2,1)   # b,hw,c\n",
    "        k = k.reshape(b,c,h*w) # b,c,hw\n",
    "        ic(q.shape)\n",
    "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
    "        ic(w_.shape)\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = v.reshape(b,c,h*w)\n",
    "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
    "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
    "        h_ = h_.reshape(b,c,h,w)\n",
    "\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x+h_\n",
    "\n",
    "\n",
    "img=torch.rand(2,512,16,16)\n",
    "attn=AttnBlock(img.shape[1])\n",
    "\n",
    "out=attn(img)\n",
    "ic(out.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
