{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86aa2074",
   "metadata": {},
   "source": [
    "\n",
    "## 碎片化读论文 《DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models》\n",
    "\n",
    "https://arxiv.org/pdf/2501.12948\n",
    "\n",
    "\n",
    "2025-09-30 18:06:04 Tuesday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14953442",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 读摘要\n",
    "\n",
    "> 摘要原文： Mathematical reasoning poses a significant challenge for language models due to its complex\n",
    "and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre\u0002training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common\n",
    "Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an\n",
    "impressive score of 51.7% on the competition-level MATH benchmark without relying on\n",
    "external toolkits and voting techniques, approaching the performance level of Gemini-Ultra\n",
    "and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\n",
    "The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First,\n",
    "we harness the significant potential of publicly available web data through a meticulously\n",
    "engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization\n",
    "(GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning\n",
    "abilities while concurrently optimizing the memory usage of PPO.\n",
    "\n",
    "模型名称：DeepSeekMath 7B\n",
    "\n",
    "模型训练方法：\n",
    "- 基础的预训练模型：DeepSeek-Coder-Base-v1.5 7B\n",
    "- 预训练数据集：来自 ​Common Crawl的120 B数据\n",
    "\n",
    "\n",
    "测试：\n",
    "- 测试方式：竞赛级别的MATH基准测试\n",
    "- 结果： 不依赖外部工具包和投票技术，就取得了51.7%的优异成绩，接近Gemini-Ultra和GPT-4的性能水平。\n",
    "- 对DeepSeekMath 7B的64个样本进行自一致性测试，在MATH上达到了60.9%。\n",
    "\n",
    "DeepSeekMath的数学推理能力得益于两个关键因素：\n",
    "- 充分从公开网络数据集获取\n",
    "- grpo算法创新\n",
    "    - 算法优点：增强数学推理能力的同时，还能优化PPO的内存使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634bb83",
   "metadata": {},
   "source": [
    "## 预训练数据集的构建过程？\n",
    "第一阶段：数据集获取\n",
    "1. 训练数学问题分类器：\n",
    "使用一个已知的高质量、小规模数学数据集（如 ​OpenWebMath）作为初始正样本（数学网页），并从Common Crawl中随机抽取网页作为负样本（非数学网页），用这些正负样本训练一个 ​FastText 文本分类器，让它学会区分网页是否与数学相关。\n",
    "2. 用​FastText 分类器召回数学问题\n",
    "从Common Crawl再次召回\n",
    "3. 对召回结果做筛选\n",
    "    - 先规则筛选，一个域名下要超过10%d网页判断为数学相关\n",
    "    - 后人工筛选，把这个域名下的url都人工标记为可用。\n",
    "4. 再次回到步骤1，用扩充的数据集训练​FastText分类器。 经过几轮迭代，得到120B的语料库。\n",
    "\n",
    "第二阶段： 数据清洗\n",
    "- 目标：移除其中包含标准测试题（如MATH、GSM8K中的题目）的网页，防止模型“作弊”。\n",
    "- 方法：使用严格的n-gram精确匹配技术，扫描每一篇网页，一旦发现与测试集完全相同的片段（连续的n个字符），就将整个网页从训练集中剔除。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb192cf3",
   "metadata": {},
   "source": [
    "## 为什么选择用数学问题作为训练集？\n",
    "文章中提到是发现数学问题能提高模型的通用推理能力。\n",
    "> we observe the math training also improves model capability on MMLU\n",
    "(Hendrycks et al., 2020) and BBH benchmarks (Suzgun et al., 2022), indicating it does not only\n",
    "enhance the model’s mathematical abilities but also amplifies general reasoning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adedc4f3",
   "metadata": {},
   "source": [
    "## 代码训练能提高推理能力吗？\n",
    "\n",
    "文章实验发现，在数学训练之前进行代码训练，能明显提升模型解决数学问题的能力（无论是否使用工具），即能提高数学推理的能力，故而作者认为也可以提高通用推理能力。\n",
    "\n",
    "这种导致的原因可能是  代码和数学都强调逻辑结构、符号运算和算法思维，二者具有内在一致性。\n",
    "\n",
    "> Code training prior to math\n",
    "training improves models’ ability to solve mathematical problems both with and without\n",
    "tool use. This offers a partial answer to the long-standing question: does code training\n",
    "improve reasoning abilities? We believe it does, at least for mathematical reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad18f0c",
   "metadata": {},
   "source": [
    "## puzzle\n",
    "1. 什么是一致性测试 self-consistency？\n",
    "\n",
    "2. 统一的范式 理解拒绝采样微调（RFT），DPO，PPO，GRPO？\n",
    "\n",
    "3. 一些在线与离线的实验结果怎么样？\n",
    "\n",
    "4. rl的有效的原因是什么，作者是怎么分析的？\n",
    "    > Based on our unified paradigm, we explore the reasons behind the effectiveness of rein\u0002forcement learning, and summarize several potential directions to achieve more effective\n",
    "reinforcement learning of LLMs\n",
    "\n",
    "\n",
    "5. n-gram是什么？"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
