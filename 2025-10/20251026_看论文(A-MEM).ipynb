{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2c26bf",
   "metadata": {},
   "source": [
    "\n",
    "2025-10-26 23:53:44 Sunday\n",
    "\n",
    "\n",
    "### 基础：大模型的memory\n",
    "\n",
    "对于普通大模型 / LLM Agent 而言，\n",
    "Memory 通常分为三层：\n",
    "\n",
    "- 短期记忆（Short-Term） — 当前上下文，放prompt里\n",
    "- 长期记忆（Long-Term） — 外部知识存储，放rag里，\n",
    "- 语义记忆（Semantic / Episodic） — 抽象知识、总结偏好，放类似json这样的文件里，调用llm检索和问题相关性，再加入到prompt中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1af8b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 📄 基本信息\n",
    "\n",
    "* **标题**：A-MEM: Agentic Memory for LLM Agents\n",
    "* **作者**：Minghao Liu, Rui Zhang, et al.\n",
    "* **单位**：南洋理工大学（Nanyang Technological University）及合作机构\n",
    "* **年份**：2025\n",
    "* **论文链接**：[https://arxiv.org/pdf/2502.12110](https://arxiv.org/pdf/2502.12110)\n",
    "* **创新参考**：基于 MemGPT 与 RAG（Retrieval-Augmented Generation） 的长期记忆体系扩展\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 一句话总结\n",
    "\n",
    "> 这篇论文提出了一个新的 **长期记忆系统 A-MEM**，\n",
    "> 能让大模型像人一样“整理、连接和更新记忆”。\n",
    "> 它显著提升了模型在**长对话、多步推理**（multi-hop reasoning）等复杂任务中的稳定性和效率。\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953ebe1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧩 1. 研究背景与问题定义\n",
    "\n",
    "### 🎯 研究目标\n",
    "\n",
    "- 构建一个 可扩展的 Agentic Memory 框架，实现记忆内容的结构化表达、语义关联与动态更新；\n",
    "\n",
    "- 通过引入「原子笔记（atomic note）」表示形式与「记忆进化（memory evolution）」机制，使记忆体系支持 语义一致性维护与时序更新；\n",
    "\n",
    "- 解决 LLM 在长时间、多任务交互中出现的 上下文遗忘（context forgetting） 与 多跳推理链断裂（multi-hop reasoning degradation）；\n",
    "\n",
    "- 在保持推理准确度的同时，降低检索与上下文拼接的 token 成本，实现 高效的长期记忆检索与生成一体化。\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ 现有问题\n",
    "\n",
    "在目前的大语言模型系统（例如 ChatGPT、MemGPT、RAG）中，记忆大致分两类：\n",
    "\n",
    "| 类型                   | 特点                 | 问题                 |\n",
    "| -------------------- | ------------------ | ------------------ |\n",
    "| **短期记忆**（Short-term） | 模型一次能看到的上下文，比如当前对话 | 过短，超过窗口就会忘记        |\n",
    "| **长期记忆**（Long-term）  | 用数据库保存旧内容，通过相似度检索  | 只能“查找”，不能“更新”或“整理” |\n",
    "\n",
    "所以问题在于：\n",
    "\n",
    "* 传统记忆机制（如 RAG、MemGPT）仅具备“存储-检索”功能，缺乏自组织与自更新能力；\n",
    "* 老记忆永远不会更新；\n",
    "* 随记忆量增长，检索耗时与 token 消耗显著增加；\n",
    "* 当问题需要跨多步逻辑（比如先回忆 A → 再想到 B → 再结合 C）时，性能明显下降。\n",
    "\n",
    "> 🧠 举例：\n",
    "> 假设模型曾经和你聊过“你喜欢咖啡”，后来又说“你改喝茶了”。\n",
    "> 普通模型会记得这两件事，但不知道哪一条更新、哪个更重要。\n",
    "> A-MEM 的目标是让模型像人一样**自动整理这些记忆**。\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 作者动机\n",
    "\n",
    "作者受人类笔记方法（例如 **Zettelkasten 卡片笔记法**）启发，提出：\n",
    "\n",
    "> 用“原子笔记 + 动态链接 + 记忆进化”三步法，让模型的记忆能自己成长。\n",
    "\n",
    "* **原子笔记**：把每条记忆拆成小单元，方便管理；\n",
    "* **动态链接**：让模型自动判断哪些记忆之间有关联；\n",
    "* **记忆进化**：当有新知识时，模型会主动修改旧记忆，保持一致。\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 小科普：什么是「多跳推理（Multi-hop Reasoning）」？\n",
    "\n",
    "> 「多跳」意思是模型回答一个问题时，要跨越多个记忆节点或知识点，像人类“联想思维”那样逐步推理。\n",
    "\n",
    "🧩 举例：\n",
    "\n",
    "> 问题：“谁是喜欢喝咖啡的老师的学生？”\n",
    ">\n",
    "> * 第一步：先找到“喜欢喝咖啡的老师是谁”；\n",
    "> * 第二步：再找到这个老师的学生是谁。\n",
    "\n",
    "这类问题需要模型在多条记忆之间**连续推理**，\n",
    "而普通的 RAG 或 MemGPT 在这方面经常“断链”或忘上下文。\n",
    "A-MEM 的设计目标就是让模型能跨节点地**跳跃思考**，像脑内网络一样串联信息。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448bebd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ⚙️ 2. 方法概述（核心创新）\n",
    "\n",
    "A-MEM 的方法可以拆成四个主要环节：\n",
    "\n",
    "1. 记忆单元构建（Note Construction）\n",
    "2. 语义链接生成（Link Generation）\n",
    "3. 记忆进化（Memory Evolution）\n",
    "4. 记忆检索与推理（Memory Retrieval & Usage）\n",
    "\n",
    "这四步连起来，构成了一个“自组织、可更新的长期语义记忆系统”。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 记忆单元构建（Note Construction）\n",
    "\n",
    "**谁在做：** 记忆管理模块（Memory Manager）（由普通逻辑代码实现）调用大语言模型（LLM）。\n",
    "\n",
    "**用什么：** 新产生的交互内容 / 任务过程 / 对话片段；以及一个基础 LLM（例如 GPT-4 级别）和一个文本嵌入模型（embedding encoder）。\n",
    "\n",
    "**做什么：** 把“刚发生的一次事件”变成一个结构化的、可检索的“原子笔记”（atomic note）。\n",
    "\n",
    "具体步骤是这样的：\n",
    "\n",
    "1. 系统在某个时间点获得一条新信息（例如一轮对话、一条用户偏好、一次执行日志）。我们记它为原始内容 $c_i$，带时间戳 $t_i$。\n",
    "2. 系统调用 LLM，对 $c_i$ 进行语义加工，自动生成：\n",
    "\n",
    "   * 关键词/关键实体 $K_i$（比如“航班预订”、“上海出差”）\n",
    "   * 主题标签 $G_i$（比如 `{\"用户偏好\": 旅行, \"地点\": 上海}`）\n",
    "   * 语义摘要 $X_i$（对这条记忆的简洁解释，面向后续检索）\n",
    "3. 系统把这些字段拼起来（内容 $c_i$、关键词 $K_i$、标签 $G_i$、摘要 $X_i$），送入一个嵌入编码器 $f_{\\text{enc}}$，生成向量嵌入 $e_i$。\n",
    "   数学上可以写成：\n",
    "   $$\n",
    "   e_i = f_{\\text{enc}}([c_i, K_i, G_i, X_i])\n",
    "   $$\n",
    "4. 系统把这些信息打包成一条“原子笔记”：\n",
    "   $$\n",
    "   m_i = {c_i, t_i, K_i, G_i, X_i, e_i, L_i}\n",
    "   $$\n",
    "   其中 $L_i$ 先留空（还没有链接关系）。\n",
    "\n",
    "▶ 总结：\n",
    "\n",
    "* **谁**：Memory Manager + LLM + embedding encoder\n",
    "* **用什么**：最新产生的交互内容\n",
    "* **做什么**：生成结构化“记忆节点” $m_i$，以后检索、更新、关联都围绕它进行\n",
    "* **意义**：记忆不再是纯文本堆积，而是标准化的知识单元（节点）\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 语义链接生成（Link Generation）\n",
    "\n",
    "**谁在做：** Memory Manager 和 LLM 共同完成。\n",
    "\n",
    "**用什么：** 新的笔记 $m_n$ 的向量表示 $e_n$，以及历史笔记库中所有已有向量 ${e_j}$。\n",
    "\n",
    "**做什么：** 判断这条新记忆和哪些旧记忆语义相关，并在它们之间建立永久的“语义链接”。\n",
    "\n",
    "这一步分两层过滤：\n",
    "\n",
    "1. **相似度初筛（向量空间层）**\n",
    "\n",
    "   * 系统计算新记忆 $m_n$ 的向量 $e_n$ 和历史记忆 $m_j$ 的向量 $e_j$ 的余弦相似度：\n",
    "     $$\n",
    "     s_{n,j} = \\frac{e_n \\cdot e_j}{|e_n||e_j|}\n",
    "     $$\n",
    "   * 选出 top-$k$ 个最接近的候选集合 $M_n^{\\text{near}}$。\n",
    "   * 这一步是纯算法的，不需要 LLM，速度快。\n",
    "\n",
    "2. **语义判断（语言层）**\n",
    "\n",
    "   * 对于 $m_n$ 和候选集合 $M_n^{\\text{near}}$，系统把它们的内容、标签、摘要一并喂给 LLM，使用一个专门 prompt（论文称为 $P_2$）。\n",
    "   * 让 LLM 判断：哪些旧记忆和这条新记忆“语义上确实相关”，不仅是词相似，而是同一主题 / 同一用户偏好 / 同一长期目标。\n",
    "   * LLM 返回一个列表：哪些历史记忆应与 $m_n$ 建立链接。这个列表成为 $L_n$。\n",
    "\n",
    "最后，系统把这些链接关系写回存储，使记忆库逐渐变成一个**语义图（graph）**，而不是一堆孤立的段落。\n",
    "\n",
    "▶ 总结：\n",
    "\n",
    "* **谁**：先是 embedding 相似度模块做初筛，再由 LLM 做最终判定\n",
    "* **用什么**：新笔记 $m_n$，历史笔记集合\n",
    "* **做什么**：建立语义链接 $L_n$，把相关记忆节点彼此连起来\n",
    "* **意义**：模型将“我知道的东西”组织成网，而不是散乱的列表；这为后面多跳推理提供基础（模型可以顺着链接往回找上下文）\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 记忆进化（Memory Evolution）\n",
    "\n",
    "**谁在做：** LLM 主导，Memory Manager 触发。\n",
    "\n",
    "**用什么：** 新增记忆 $m_n$、与它相关的历史记忆 $m_j \\in M_n^{\\text{near}}$，以及它们之间的语义关系。\n",
    "\n",
    "**做什么：** 不仅插入新记忆，还“回头重写旧记忆”，让旧记忆保持最新、正确的抽象描述。\n",
    "\n",
    "流程是这样：\n",
    "\n",
    "1. 系统对每个已存在的相关记忆 $m_j$，会把如下信息打包交给 LLM：\n",
    "\n",
    "   * 旧记忆 $m_j$ 的内容（旧的摘要、标签、关键词等）\n",
    "   * 新记忆 $m_n$ 的内容\n",
    "   * 其它相似记忆的上下文（帮助 LLM 看到大局）\n",
    "   * 一个控制式 prompt（论文里叫 $P_3$）\n",
    "\n",
    "2. LLM 会输出一个“更新后的版本”：\n",
    "   $$\n",
    "   m_j^* = \\text{LLM}(m_n \\parallel (M_n^{\\text{near}} \\setminus m_j) \\parallel m_j \\parallel P_3)\n",
    "   $$\n",
    "   这一步可以修改：\n",
    "\n",
    "   * 标签（G）\n",
    "   * 摘要（X）\n",
    "   * 关键词（K）\n",
    "   * 甚至可以把“这是一次性的事实”上升为“这是持续偏好”\n",
    "\n",
    "3. 系统用 $m_j^*$ 覆盖原来的 $m_j$。\n",
    "\n",
    "也就是说：**旧记忆不是死的**。\n",
    "它会被反复重写，以吸收新信息，保持一致。\n",
    "\n",
    "▶ 总结：\n",
    "\n",
    "* **谁**：LLM 扮演“笔记编辑器”的角色\n",
    "* **用什么**：新记忆 + 相关旧记忆 + prompt 规则\n",
    "* **做什么**：对旧记忆做“增量重写”，使它们更加准确、抽象和一致\n",
    "* **意义（非常关键）**：\n",
    "\n",
    "  * 避免信息冲突（“他喜欢咖啡” vs “他现在改喝茶了”）\n",
    "  * 让语义记忆层不只是堆日志，而是会逐步沉淀成“用户的稳定画像 / 世界状态的当前版本”\n",
    "  * 这一步正是论文所谓“自进化”的核心\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 记忆检索与推理（Memory Retrieval & Usage）\n",
    "\n",
    "**谁在做：** Memory Manager + 检索模块 + LLM。\n",
    "\n",
    "**用什么：** 用户当前问题 $q$，整个记忆库（节点 + 链接），以及 embedding 检索。\n",
    "\n",
    "**做什么：** 在需要回答问题或规划行动时，从记忆网络里提取最相关的上下文，拼接进 prompt，让 LLM 用来推理。\n",
    "\n",
    "运行方式：\n",
    "\n",
    "1. 当有查询 / 新任务 $q$：\n",
    "\n",
    "   * 系统把 $q$ 送进相同的嵌入编码器 $f_{\\text{enc}}$，得到向量 $e_q$。\n",
    "   * 系统用 $e_q$ 去匹配哪些记忆节点 $m_i$ 的向量 $e_i$ 最相近（标准近邻检索 / 余弦相似度）。\n",
    "\n",
    "2. 对于被检索到的每个节点 $m_i$，系统还会把它们的链接 $L_i$（也就是它连接到的其他相关记忆）一起提取出来。\n",
    "\n",
    "   * 这一步是论文最强调的点之一：你不只是拿到“单条记忆”，而是拿到“一小片语义邻域”。\n",
    "   * 这等价于在推理时，模型可以跨多个节点做“多跳整合”。\n",
    "\n",
    "3. 系统把这些记忆块（摘要、标签、关键事实）拼成一段上下文，放在 prompt 的前部，作为“检索到的背景知识”。\n",
    "\n",
    "4. LLM 读完这些上下文，再去生成回答 / 做推理 / 做计划。\n",
    "\n",
    "▶ 总结：\n",
    "\n",
    "* **谁**：检索模块负责召回，LLM 负责最终推理\n",
    "* **用什么**：用户问题 $q$ + 向量检索 + 记忆链接图\n",
    "* **做什么**：把相关的历史记忆+其关联节点打包进 prompt\n",
    "* **意义**：\n",
    "\n",
    "  * 支持“多跳推理（multi-hop reasoning）”，即把多条零散记忆串起来回答复杂问题\n",
    "  * 在长对话场景下保持一致性（不会突然忘了用户是谁、用户要什么）\n",
    "  * 限制了 prompt 长度（只注入相关子图，而不是整段历史），降低 token 费用\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 小结\n",
    "\n",
    "* **构建：** Memory Manager 调 LLM，把每次交互变成结构化“原子笔记”；\n",
    "* **连接：** 检索相似节点 + LLM判断关联 → 建立语义链接，形成记忆图；\n",
    "* **进化：** LLM 回头修改旧笔记，保证长期记忆始终是最新、干净、抽象的版本；\n",
    "* **检索：** 遇到问题时，从这张语义记忆图里拉出相关子图，拼进 prompt，帮助 LLM 做多跳推理。\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
