{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77999b06",
   "metadata": {},
   "source": [
    "2025-10-12 23:43:17 Sunday\n",
    "### 你能讲讲 GRPO（Group Relative Policy Optimization）的核心思想吗?\n",
    "GRPO 用 **组内相对奖励差异** 替代 PPO 的 critic。  \n",
    "对每个输入 query，从旧策略生成多条回答 \\(y_i\\)，计算奖励 \\(r_i\\)，并在组内标准化：\n",
    "\n",
    "$$\n",
    "\\hat A_i = \\frac{r_i - \\text{mean}(r)}{\\text{std}(r)}\n",
    "$$\n",
    "\n",
    "得到相对优势。优化目标是最大化高 reward 回答的生成概率：\n",
    "\n",
    "$$\n",
    "J_{\\text{GRPO}}(\\theta)\n",
    "= \\mathbb E_{q,\\{o_i\\}}\\Bigg[\n",
    "\\frac{1}{G}\\sum_{i,t}\n",
    "\\min\\big(r_{i,t}\\hat A_{i,t},\\,\\text{clip}(r_{i,t},1-\\varepsilon,1+\\varepsilon)\\hat A_{i,t}\\big)\n",
    "-\\beta\\,D_{\\mathrm{KL}}(\\pi_\\theta\\|\\pi_{\\text{ref}})\n",
    "\\Bigg]\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$\n",
    "r_{i,t} = \\frac{\\pi_\\theta(o_{i,t}|q,o_{i,<t})}{\\pi_{\\text{old}}(o_{i,t}|q,o_{i,<t})}\n",
    "$$\n",
    "\n",
    "优势函数乘的项是 **概率比值**，而不是策略熵。  \n",
    "策略熵可选作正则项以鼓励探索多样性：\n",
    "\n",
    "$$\n",
    "H(\\pi) = -\\sum_x \\pi(x) \\log \\pi(x)\n",
    "$$\n",
    "\n",
    "\n",
    "#### 追问：KL 项的作用？\n",
    "\n",
    "1️⃣ **限制策略分布漂移** —— 防止更新过猛、训练不稳定；  \n",
    "2️⃣ **保持语言与参考模型一致** —— 防止语义退化、语气异常；  \n",
    "3️⃣ **与熵项协同** —— 共同抑制模式塌陷（即模型输出单一模板化）。\n",
    "\n",
    "\n",
    "#### 追问：与 PPO 的区别？\n",
    "\n",
    "- 无需 critic（value 网络）；  \n",
    "- 优势来自 **组内归一化**，而非时序估计 (GAE)；  \n",
    "- KL 惩罚作为 **独立正则项**，不混入 reward；  \n",
    "- 对组大小敏感：组太小方差大，组太大梯度稀释。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
