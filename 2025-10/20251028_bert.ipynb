{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf974e83",
   "metadata": {},
   "source": [
    "2025-10-28 23:38:55 Tuesday\n",
    "\n",
    "\n",
    "面试问题记录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e38ac5",
   "metadata": {},
   "source": [
    "## 1.bert的输出\n",
    "\n",
    "bert的输出有两类结果\n",
    "\n",
    "| 输出                       | 形状                                   | 含义                                       |\n",
    "| ------------------------ | ------------------------------------ | ---------------------------------------- |\n",
    "| **1. last_hidden_state** | `[batch_size, seq_len, hidden_size]` | 每个 token 的上下文向量表示                        |\n",
    "| **2. pooler_output**     | `[batch_size, hidden_size]`          | [CLS] 位置经过一个 `tanh` 层后的句向量（BERT 的默认句子表示） |\n",
    "\n",
    "\n",
    "下面是一个代码实现，注意看结尾注释\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ecc3d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== tokenizer 输出张量 ===\n",
      "input_ids torch.Size([1, 11]) tensor([[ 101, 7592, 2088, 1012,  102, 2129, 2024, 2017, 2651, 1029,  102]])\n",
      "token_type_ids torch.Size([1, 11]) tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n",
      "attention_mask torch.Size([1, 11]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "=== BERT 输出 ===\n",
      "last_hidden_state: torch.Size([1, 11, 768])\n",
      "pooler_output: torch.Size([1, 768])\n",
      "\n",
      "[CLS] embedding shape: torch.Size([1, 768])\n",
      "First real token embedding shape: torch.Size([1, 768])\n",
      "\n",
      "[CLS] embedding (前10维):\n",
      "tensor([-0.2158,  0.2033, -0.0384, -0.3991, -0.4776, -0.2867,  0.7173,  0.7658,\n",
      "        -0.0283, -0.3685])\n",
      "\n",
      "第一个token的embedding (前10维):\n",
      "tensor([ 0.0255,  0.0870,  0.7774, -0.7172,  0.2613, -0.0265,  0.5924,  0.6962,\n",
      "        -0.6278, -1.4433])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1. 加载预训练的中文 or 英文 BERT\n",
    "# 如果你主要是英文文本，用 'bert-base-uncased'\n",
    "# 如果你想中文，用 'bert-base-chinese'\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2. 准备输入文本（单句 or 句对都行）\n",
    "sentence_a = \"Hello world.\"\n",
    "sentence_b = \"How are you today?\"\n",
    "# sentence_c = \"How are you today?\"\n",
    "\n",
    "\n",
    "# 3. tokenizer：把文本 → token ids / segment ids / attention mask\n",
    "encoded = tokenizer(\n",
    "    sentence_a,\n",
    "    sentence_b,                    # 传第二个句子 -> 会自动加 [SEP] 并生成 token_type_ids区分句A/B\n",
    "    # sentence_c,\n",
    "    \n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",           # 直接返回 PyTorch tensor\n",
    ")\n",
    "\n",
    "print(\"=== tokenizer 输出张量 ===\")\n",
    "for k, v in encoded.items():\n",
    "    print(k, v.shape, v)\n",
    "\n",
    "# encoded 里典型有：\n",
    "# - input_ids:        token id 序列\n",
    "# - token_type_ids:   句子A是0，句子B是1\n",
    "# - attention_mask:   1=有效token，0=padding\n",
    "\n",
    "# 4. 前向：送进 BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "\n",
    "# 5. 拿结果\n",
    "last_hidden_state = outputs.last_hidden_state    # [batch, seq_len, hidden]\n",
    "pooler_output    = outputs.pooler_output         # [batch, hidden]，等价于\"句子向量\"\n",
    "\n",
    "print(\"\\n=== BERT 输出 ===\")\n",
    "print(\"last_hidden_state:\", last_hidden_state.shape)\n",
    "print(\"pooler_output:\", pooler_output.shape)\n",
    "\n",
    "# 6. 看看 [CLS] 向量，和任意一个 token 的向量\n",
    "cls_embedding = last_hidden_state[:, 0, :]       # 第0个位置是 [CLS]\n",
    "tok1_embedding = last_hidden_state[:, 1, :]      # 第1个位置是第一个真实token\n",
    "\n",
    "print(\"\\n[CLS] embedding shape:\", cls_embedding.shape)\n",
    "print(\"First real token embedding shape:\", tok1_embedding.shape)\n",
    "\n",
    "# 7. 举个实际数值片段\n",
    "print(\"\\n[CLS] embedding (前10维):\")\n",
    "print(cls_embedding[0, :10])\n",
    "\n",
    "print(\"\\n第一个token的embedding (前10维):\")\n",
    "print(tok1_embedding[0, :10])\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 📌 关键解读（看看打印出来的东西时可以对照下面读）\n",
    "##########################################\n",
    "\n",
    "# 1) input_ids\n",
    "#    这是 tokenizer 把 \"[CLS] Hello world . [SEP] How are you today ? [SEP]\"\n",
    "#    全部换成词表ID后的整数序列。\n",
    "#\n",
    "# 2) token_type_ids\n",
    "#    同长度向量，比如 [0,0,0,0,0,1,1,1,1,1,1,...]\n",
    "#    0 表示属于句子A，1 表示属于句子B。\n",
    "#\n",
    "# 3) attention_mask\n",
    "#    和 seq_len 等长，比如 [1,1,1,1,1,1,...,1,0,0,0]\n",
    "#    告诉BERT哪些位置是真实token(1)，哪些是padding(0)，\n",
    "#    注意力会在padding上被屏蔽，不会影响别的词。\n",
    "#\n",
    "# 4) last_hidden_state\n",
    "#    形状 [batch_size, seq_len, hidden_size]\n",
    "#    每个token一个向量（768维 for bert-base）。\n",
    "#    适合做 token-level 任务，例如 NER、问答里的span抽取。\n",
    "#\n",
    "# 5) pooler_output\n",
    "#    形状 [batch_size, hidden_size]\n",
    "#    这是 BERT 官方\"句子向量\"：它取 last_hidden_state[:,0,:] 也就是 [CLS]，\n",
    "#    但还过了一层线性+Tanh。常拿来做句子分类。\n",
    "#\n",
    "# 6) cls_embedding\n",
    "#    我们手动取的 last_hidden_state[:,0,:]。\n",
    "#    跟 pooler_output 很接近，但 pooler_output 还多一层变换。\n",
    "#\n",
    "# ✅ 总结:\n",
    "# - 如果你要句子分类，用 pooler_output 或 [CLS] 向量接线性层。\n",
    "# - 如果你要逐词标注，用 last_hidden_state 的每个位置。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99715839",
   "metadata": {},
   "source": [
    "## 2. BERT 预训练阶段的两个任务\n",
    "\n",
    "### 2.1 Masked Language Modeling (MLM)\n",
    "\n",
    "给定被随机掩蔽的输入序列，模型需要预测被掩蔽 token 的原始词表 ID。  \n",
    "与传统的单向语言模型（仅依赖左或右上下文）不同，BERT 通过 **双向 self-attention** 同时利用左、右两侧上下文信息。  \n",
    "损失函数为掩蔽位置处的交叉熵损失：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{MLM}} = -\\sum_{i \\in M} \\log p_\\theta(x_i \\mid x_{\\setminus i})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Next Sentence Prediction (NSP)\n",
    "\n",
    "该任务用于学习句子间关系。输入由句子 A 和句子 B 组成，其中 50% 的样本中 B 为 A 的真实下一句（IsNext），其余 50% 为随机句（NotNext）。  \n",
    "模型使用 `[CLS]` token 的输出向量，经线性层和 softmax，预测两者是否连续。对应的损失为二分类交叉熵：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NSP}} = -\\log p_\\theta(y \\mid \\text{[CLS]})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 总体损失\n",
    "\n",
    "预训练阶段优化以下总损失函数：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{MLM}} + \\mathcal{L}_{\\text{NSP}}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
