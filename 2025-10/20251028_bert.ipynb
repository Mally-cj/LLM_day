{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf974e83",
   "metadata": {},
   "source": [
    "2025-10-28 23:38:55 Tuesday\n",
    "\n",
    "\n",
    "é¢è¯•é—®é¢˜è®°å½•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e38ac5",
   "metadata": {},
   "source": [
    "## 1.bertçš„è¾“å‡º\n",
    "\n",
    "bertçš„è¾“å‡ºæœ‰ä¸¤ç±»ç»“æœ\n",
    "\n",
    "| è¾“å‡º                       | å½¢çŠ¶                                   | å«ä¹‰                                       |\n",
    "| ------------------------ | ------------------------------------ | ---------------------------------------- |\n",
    "| **1. last_hidden_state** | `[batch_size, seq_len, hidden_size]` | æ¯ä¸ª token çš„ä¸Šä¸‹æ–‡å‘é‡è¡¨ç¤º                        |\n",
    "| **2. pooler_output**     | `[batch_size, hidden_size]`          | [CLS] ä½ç½®ç»è¿‡ä¸€ä¸ª `tanh` å±‚åçš„å¥å‘é‡ï¼ˆBERT çš„é»˜è®¤å¥å­è¡¨ç¤ºï¼‰ |\n",
    "\n",
    "\n",
    "ä¸‹é¢æ˜¯ä¸€ä¸ªä»£ç å®ç°ï¼Œæ³¨æ„çœ‹ç»“å°¾æ³¨é‡Š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ecc3d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== tokenizer è¾“å‡ºå¼ é‡ ===\n",
      "input_ids torch.Size([1, 11]) tensor([[ 101, 7592, 2088, 1012,  102, 2129, 2024, 2017, 2651, 1029,  102]])\n",
      "token_type_ids torch.Size([1, 11]) tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n",
      "attention_mask torch.Size([1, 11]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "=== BERT è¾“å‡º ===\n",
      "last_hidden_state: torch.Size([1, 11, 768])\n",
      "pooler_output: torch.Size([1, 768])\n",
      "\n",
      "[CLS] embedding shape: torch.Size([1, 768])\n",
      "First real token embedding shape: torch.Size([1, 768])\n",
      "\n",
      "[CLS] embedding (å‰10ç»´):\n",
      "tensor([-0.2158,  0.2033, -0.0384, -0.3991, -0.4776, -0.2867,  0.7173,  0.7658,\n",
      "        -0.0283, -0.3685])\n",
      "\n",
      "ç¬¬ä¸€ä¸ªtokençš„embedding (å‰10ç»´):\n",
      "tensor([ 0.0255,  0.0870,  0.7774, -0.7172,  0.2613, -0.0265,  0.5924,  0.6962,\n",
      "        -0.6278, -1.4433])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1. åŠ è½½é¢„è®­ç»ƒçš„ä¸­æ–‡ or è‹±æ–‡ BERT\n",
    "# å¦‚æœä½ ä¸»è¦æ˜¯è‹±æ–‡æ–‡æœ¬ï¼Œç”¨ 'bert-base-uncased'\n",
    "# å¦‚æœä½ æƒ³ä¸­æ–‡ï¼Œç”¨ 'bert-base-chinese'\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2. å‡†å¤‡è¾“å…¥æ–‡æœ¬ï¼ˆå•å¥ or å¥å¯¹éƒ½è¡Œï¼‰\n",
    "sentence_a = \"Hello world.\"\n",
    "sentence_b = \"How are you today?\"\n",
    "# sentence_c = \"How are you today?\"\n",
    "\n",
    "\n",
    "# 3. tokenizerï¼šæŠŠæ–‡æœ¬ â†’ token ids / segment ids / attention mask\n",
    "encoded = tokenizer(\n",
    "    sentence_a,\n",
    "    sentence_b,                    # ä¼ ç¬¬äºŒä¸ªå¥å­ -> ä¼šè‡ªåŠ¨åŠ  [SEP] å¹¶ç”Ÿæˆ token_type_idsåŒºåˆ†å¥A/B\n",
    "    # sentence_c,\n",
    "    \n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",           # ç›´æ¥è¿”å› PyTorch tensor\n",
    ")\n",
    "\n",
    "print(\"=== tokenizer è¾“å‡ºå¼ é‡ ===\")\n",
    "for k, v in encoded.items():\n",
    "    print(k, v.shape, v)\n",
    "\n",
    "# encoded é‡Œå…¸å‹æœ‰ï¼š\n",
    "# - input_ids:        token id åºåˆ—\n",
    "# - token_type_ids:   å¥å­Aæ˜¯0ï¼Œå¥å­Bæ˜¯1\n",
    "# - attention_mask:   1=æœ‰æ•ˆtokenï¼Œ0=padding\n",
    "\n",
    "# 4. å‰å‘ï¼šé€è¿› BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "\n",
    "# 5. æ‹¿ç»“æœ\n",
    "last_hidden_state = outputs.last_hidden_state    # [batch, seq_len, hidden]\n",
    "pooler_output    = outputs.pooler_output         # [batch, hidden]ï¼Œç­‰ä»·äº\"å¥å­å‘é‡\"\n",
    "\n",
    "print(\"\\n=== BERT è¾“å‡º ===\")\n",
    "print(\"last_hidden_state:\", last_hidden_state.shape)\n",
    "print(\"pooler_output:\", pooler_output.shape)\n",
    "\n",
    "# 6. çœ‹çœ‹ [CLS] å‘é‡ï¼Œå’Œä»»æ„ä¸€ä¸ª token çš„å‘é‡\n",
    "cls_embedding = last_hidden_state[:, 0, :]       # ç¬¬0ä¸ªä½ç½®æ˜¯ [CLS]\n",
    "tok1_embedding = last_hidden_state[:, 1, :]      # ç¬¬1ä¸ªä½ç½®æ˜¯ç¬¬ä¸€ä¸ªçœŸå®token\n",
    "\n",
    "print(\"\\n[CLS] embedding shape:\", cls_embedding.shape)\n",
    "print(\"First real token embedding shape:\", tok1_embedding.shape)\n",
    "\n",
    "# 7. ä¸¾ä¸ªå®é™…æ•°å€¼ç‰‡æ®µ\n",
    "print(\"\\n[CLS] embedding (å‰10ç»´):\")\n",
    "print(cls_embedding[0, :10])\n",
    "\n",
    "print(\"\\nç¬¬ä¸€ä¸ªtokençš„embedding (å‰10ç»´):\")\n",
    "print(tok1_embedding[0, :10])\n",
    "\n",
    "\n",
    "##########################################\n",
    "# ğŸ“Œ å…³é”®è§£è¯»ï¼ˆçœ‹çœ‹æ‰“å°å‡ºæ¥çš„ä¸œè¥¿æ—¶å¯ä»¥å¯¹ç…§ä¸‹é¢è¯»ï¼‰\n",
    "##########################################\n",
    "\n",
    "# 1) input_ids\n",
    "#    è¿™æ˜¯ tokenizer æŠŠ \"[CLS] Hello world . [SEP] How are you today ? [SEP]\"\n",
    "#    å…¨éƒ¨æ¢æˆè¯è¡¨IDåçš„æ•´æ•°åºåˆ—ã€‚\n",
    "#\n",
    "# 2) token_type_ids\n",
    "#    åŒé•¿åº¦å‘é‡ï¼Œæ¯”å¦‚ [0,0,0,0,0,1,1,1,1,1,1,...]\n",
    "#    0 è¡¨ç¤ºå±äºå¥å­Aï¼Œ1 è¡¨ç¤ºå±äºå¥å­Bã€‚\n",
    "#\n",
    "# 3) attention_mask\n",
    "#    å’Œ seq_len ç­‰é•¿ï¼Œæ¯”å¦‚ [1,1,1,1,1,1,...,1,0,0,0]\n",
    "#    å‘Šè¯‰BERTå“ªäº›ä½ç½®æ˜¯çœŸå®token(1)ï¼Œå“ªäº›æ˜¯padding(0)ï¼Œ\n",
    "#    æ³¨æ„åŠ›ä¼šåœ¨paddingä¸Šè¢«å±è”½ï¼Œä¸ä¼šå½±å“åˆ«çš„è¯ã€‚\n",
    "#\n",
    "# 4) last_hidden_state\n",
    "#    å½¢çŠ¶ [batch_size, seq_len, hidden_size]\n",
    "#    æ¯ä¸ªtokenä¸€ä¸ªå‘é‡ï¼ˆ768ç»´ for bert-baseï¼‰ã€‚\n",
    "#    é€‚åˆåš token-level ä»»åŠ¡ï¼Œä¾‹å¦‚ NERã€é—®ç­”é‡Œçš„spanæŠ½å–ã€‚\n",
    "#\n",
    "# 5) pooler_output\n",
    "#    å½¢çŠ¶ [batch_size, hidden_size]\n",
    "#    è¿™æ˜¯ BERT å®˜æ–¹\"å¥å­å‘é‡\"ï¼šå®ƒå– last_hidden_state[:,0,:] ä¹Ÿå°±æ˜¯ [CLS]ï¼Œ\n",
    "#    ä½†è¿˜è¿‡äº†ä¸€å±‚çº¿æ€§+Tanhã€‚å¸¸æ‹¿æ¥åšå¥å­åˆ†ç±»ã€‚\n",
    "#\n",
    "# 6) cls_embedding\n",
    "#    æˆ‘ä»¬æ‰‹åŠ¨å–çš„ last_hidden_state[:,0,:]ã€‚\n",
    "#    è·Ÿ pooler_output å¾ˆæ¥è¿‘ï¼Œä½† pooler_output è¿˜å¤šä¸€å±‚å˜æ¢ã€‚\n",
    "#\n",
    "# âœ… æ€»ç»“:\n",
    "# - å¦‚æœä½ è¦å¥å­åˆ†ç±»ï¼Œç”¨ pooler_output æˆ– [CLS] å‘é‡æ¥çº¿æ€§å±‚ã€‚\n",
    "# - å¦‚æœä½ è¦é€è¯æ ‡æ³¨ï¼Œç”¨ last_hidden_state çš„æ¯ä¸ªä½ç½®ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99715839",
   "metadata": {},
   "source": [
    "## 2. BERT é¢„è®­ç»ƒé˜¶æ®µçš„ä¸¤ä¸ªä»»åŠ¡\n",
    "\n",
    "### 2.1 Masked Language Modeling (MLM)\n",
    "\n",
    "ç»™å®šè¢«éšæœºæ©è”½çš„è¾“å…¥åºåˆ—ï¼Œæ¨¡å‹éœ€è¦é¢„æµ‹è¢«æ©è”½ token çš„åŸå§‹è¯è¡¨ IDã€‚  \n",
    "ä¸ä¼ ç»Ÿçš„å•å‘è¯­è¨€æ¨¡å‹ï¼ˆä»…ä¾èµ–å·¦æˆ–å³ä¸Šä¸‹æ–‡ï¼‰ä¸åŒï¼ŒBERT é€šè¿‡ **åŒå‘ self-attention** åŒæ—¶åˆ©ç”¨å·¦ã€å³ä¸¤ä¾§ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚  \n",
    "æŸå¤±å‡½æ•°ä¸ºæ©è”½ä½ç½®å¤„çš„äº¤å‰ç†µæŸå¤±ï¼š\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{MLM}} = -\\sum_{i \\in M} \\log p_\\theta(x_i \\mid x_{\\setminus i})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Next Sentence Prediction (NSP)\n",
    "\n",
    "è¯¥ä»»åŠ¡ç”¨äºå­¦ä¹ å¥å­é—´å…³ç³»ã€‚è¾“å…¥ç”±å¥å­ A å’Œå¥å­ B ç»„æˆï¼Œå…¶ä¸­ 50% çš„æ ·æœ¬ä¸­ B ä¸º A çš„çœŸå®ä¸‹ä¸€å¥ï¼ˆIsNextï¼‰ï¼Œå…¶ä½™ 50% ä¸ºéšæœºå¥ï¼ˆNotNextï¼‰ã€‚  \n",
    "æ¨¡å‹ä½¿ç”¨ `[CLS]` token çš„è¾“å‡ºå‘é‡ï¼Œç»çº¿æ€§å±‚å’Œ softmaxï¼Œé¢„æµ‹ä¸¤è€…æ˜¯å¦è¿ç»­ã€‚å¯¹åº”çš„æŸå¤±ä¸ºäºŒåˆ†ç±»äº¤å‰ç†µï¼š\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NSP}} = -\\log p_\\theta(y \\mid \\text{[CLS]})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 æ€»ä½“æŸå¤±\n",
    "\n",
    "é¢„è®­ç»ƒé˜¶æ®µä¼˜åŒ–ä»¥ä¸‹æ€»æŸå¤±å‡½æ•°ï¼š\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{MLM}} + \\mathcal{L}_{\\text{NSP}}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
