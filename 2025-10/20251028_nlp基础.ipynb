{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf974e83",
   "metadata": {},
   "source": [
    "2025-10-28 23:38:55 Tuesday\n",
    "\n",
    "\n",
    "é¢è¯•é—®é¢˜è®°å½•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d8f43",
   "metadata": {},
   "source": [
    "\n",
    "## 1.åˆ†ç±»ä»»åŠ¡ä¸å¼‚å¸¸æ£€æµ‹çš„åŒºåˆ«\n",
    "\n",
    "**åˆ†ç±»ä»»åŠ¡**ï¼šç›®æ ‡æ˜¯åŒºåˆ†å¤šä¸ªâ€œå·²çŸ¥ç±»åˆ«â€ï¼Œå‡è®¾æ¯ä¸ªç±»åˆ«éƒ½æœ‰å……è¶³çš„æ ‡æ³¨æ ·æœ¬ã€‚æ¨¡å‹é€šè¿‡å­¦ä¹ ä¸åŒç±»åˆ«ä¹‹é—´çš„åˆ¤åˆ«è¾¹ç•Œæ¥å®ç°åˆ†ç±»ï¼Œå¸¸é‡‡ç”¨**äº¤å‰ç†µæŸå¤±ï¼ˆcross-entropy lossï¼‰**ä½œä¸ºè®­ç»ƒç›®æ ‡ã€‚\n",
    "\n",
    "**å¼‚å¸¸æ£€æµ‹**ï¼šç›®æ ‡æ˜¯å‘ç°â€œæœªçŸ¥æˆ–ç½•è§çš„å¼‚å¸¸â€ï¼Œå‡è®¾åªæœ‰â€œæ­£å¸¸ç±»â€æ ·æœ¬å……è¶³ï¼Œè€Œå¼‚å¸¸æ ·æœ¬æå°‘ç”šè‡³å®Œå…¨æ²¡æœ‰æ ‡ç­¾ã€‚æ­¤ç±»ä»»åŠ¡é€šå¸¸ç¼ºä¹å®Œæ•´çš„ç›‘ç£ä¿¡å·ï¼Œå› æ­¤æ¨¡å‹æ›´å…³æ³¨å­¦ä¹ â€œæ­£å¸¸æ ·æœ¬çš„åˆ†å¸ƒç‰¹å¾â€ï¼Œå¹¶é€šè¿‡**è·ç¦»åº¦é‡**æˆ–**é‡æ„è¯¯å·®**æ¥è¯†åˆ«é‚£äº›åç¦»æ­£å¸¸åˆ†å¸ƒçš„æ ·æœ¬ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### å°†å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ç›´æ¥å»ºæ¨¡ä¸ºåˆ†ç±»ä»»åŠ¡çš„é—®é¢˜\n",
    "\n",
    "åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä¾‹å¦‚â€œé£é™©é—®é¢˜è¯†åˆ«â€åœºæ™¯ä¸‹ï¼Œé£é™©æ ·æœ¬å¾€å¾€æä¸ºç¨€å°‘ï¼Œè€Œæ­£å¸¸é—®é¢˜å ç»å¤§å¤šæ•°ã€‚\n",
    "å¦‚æœç›´æ¥å°†ä»»åŠ¡å»ºæ¨¡ä¸ºâ€œæœ‰é£é™© / æ— é£é™©â€çš„äºŒåˆ†ç±»é—®é¢˜ï¼Œæ¨¡å‹ä¼šå› æ•°æ®æåº¦ä¸å¹³è¡¡è€Œéš¾ä»¥å­¦ä¹ åˆ°å¯é çš„åˆ¤åˆ«è¾¹ç•Œï¼Œå¾€å¾€è¡¨ç°ä¸ºï¼š\n",
    "\n",
    "* é£é™©è¯†åˆ«çš„å‡†ç¡®ç‡ä¸å¬å›ç‡éš¾ä»¥åŒæ—¶æå‡ï¼ŒäºŒè€…æ­¤æ¶ˆå½¼é•¿ï¼›\n",
    "* å¯¹æœªè§è¿‡çš„æˆ–æ–°å‹é£é™©ç±»å‹ç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### æ”¹é€ æ€è·¯ï¼šå°†ä»»åŠ¡è½¬åŒ–ä¸ºå¼‚å¸¸æ£€æµ‹\n",
    "\n",
    "ä¸ºç¼“è§£ä¸Šè¿°é—®é¢˜ï¼Œå¯ä»¥å°†ä»»åŠ¡æ”¹é€ ä¸º**å¼‚å¸¸æ£€æµ‹é—®é¢˜**ï¼š\n",
    "é¦–å…ˆï¼Œä»…åˆ©ç”¨å¤§é‡â€œæ­£å¸¸â€é—®é¢˜æ ·æœ¬ï¼Œé€šè¿‡ **BERT ç¼–ç å™¨**æå–æ¯æ¡æ–‡æœ¬çš„è¯­ä¹‰å‘é‡ï¼Œä½¿æ–‡æœ¬åœ¨è¯­ä¹‰ç©ºé—´ä¸­è·å¾—ç»“æ„åŒ–çš„è¯­ä¹‰è¡¨ç¤ºï¼›\n",
    "ç„¶åï¼Œä½¿ç”¨è¿™äº›è¯­ä¹‰å‘é‡è®­ç»ƒä¸€ä¸ª **Auto-Encoderï¼ˆè‡ªç¼–ç å™¨ï¼‰** ç½‘ç»œï¼Œä½¿æ¨¡å‹å­¦ä¹ â€œæ­£å¸¸é—®é¢˜â€çš„è¯­ä¹‰åˆ†å¸ƒè§„å¾‹ã€‚\n",
    "å°†ç¨€å°‘çš„é£é™©æ ·æœ¬ä½œä¸ºéªŒè¯é›†ï¼Œç”¨äº**é˜ˆå€¼æ ¡å‡†ï¼ˆthreshold calibrationï¼‰**ï¼Œç¡®å®šåˆé€‚çš„é‡æ„è¯¯å·®é˜ˆå€¼ã€‚\n",
    "\n",
    "åœ¨æ¨ç†é˜¶æ®µï¼Œå½“æ–°çš„è¾“å…¥æ ·æœ¬åˆ°æ¥æ—¶ï¼Œå°†å…¶è¯­ä¹‰å‘é‡è¾“å…¥ Auto-Encoder å¹¶è®¡ç®—é‡æ„è¯¯å·®â€”â€”è‹¥è¯¥è¯¯å·®æ˜¾è‘—å¤§äºæ ¡å‡†é˜ˆå€¼ï¼Œå³è¡¨ç¤ºè¯¥é—®é¢˜åœ¨è¯­ä¹‰ä¸Šåç¦»äº†æ­£å¸¸åˆ†å¸ƒï¼Œå¯è¢«åˆ¤å®šä¸ºâ€œæ½œåœ¨é£é™©â€æˆ–â€œå¼‚å¸¸æ ·æœ¬â€ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ç›¸è¾ƒäºç›´æ¥äºŒåˆ†ç±»çš„ä¼˜åŠ¿\n",
    "\n",
    "1. **é¿å…æ•°æ®ä¸å¹³è¡¡å½±å“**ï¼šè®­ç»ƒé˜¶æ®µä»…ä½¿ç”¨æ­£å¸¸æ ·æœ¬ï¼Œæ— éœ€ä¾èµ–å¤§é‡é£é™©æ ·æœ¬æ ‡æ³¨ã€‚\n",
    "2. **å…·å¤‡æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›**ï¼šæ¨¡å‹å­¦ä¹ çš„æ˜¯â€œæ­£å¸¸æ ·æœ¬çš„åˆ†å¸ƒâ€ï¼Œå› æ­¤å³ä¾¿å‡ºç°æ–°çš„é£é™©ç±»å‹ï¼Œåªè¦å…¶è¯­ä¹‰ç‰¹å¾ä¸æ­£å¸¸æ ·æœ¬å·®å¼‚æ˜¾è‘—ï¼Œä¹Ÿèƒ½è¢«è¯†åˆ«ä¸ºå¼‚å¸¸ã€‚\n",
    "3. **é£é™©æ§åˆ¶æ›´å…·å¯è§£é‡Šæ€§**ï¼šé€šè¿‡é‡æ„è¯¯å·®æˆ–è¯­ä¹‰è·ç¦»å¯ä»¥é‡åŒ–â€œæ ·æœ¬åç¦»æ­£å¸¸åˆ†å¸ƒçš„ç¨‹åº¦â€ï¼Œä»è€Œå®ç°åŠ¨æ€é˜ˆå€¼è°ƒæ•´ä¸åˆ†çº§é£æ§å¤„ç†ã€‚\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e38ac5",
   "metadata": {},
   "source": [
    "## 2.bertçš„è¾“å‡º\n",
    "\n",
    "bertçš„è¾“å‡ºæœ‰ä¸¤ç±»ç»“æœ\n",
    "\n",
    "| è¾“å‡º                       | å½¢çŠ¶                                   | å«ä¹‰                                       |\n",
    "| ------------------------ | ------------------------------------ | ---------------------------------------- |\n",
    "| **1. last_hidden_state** | `[batch_size, seq_len, hidden_size]` | æ¯ä¸ª token çš„ä¸Šä¸‹æ–‡å‘é‡è¡¨ç¤º                        |\n",
    "| **2. pooler_output**     | `[batch_size, hidden_size]`          | [CLS] ä½ç½®ç»è¿‡ä¸€ä¸ª `tanh` å±‚åçš„å¥å‘é‡ï¼ˆBERT çš„é»˜è®¤å¥å­è¡¨ç¤ºï¼‰ |\n",
    "\n",
    "\n",
    "ä¸‹é¢æ˜¯ä¸€ä¸ªä»£ç å®ç°ï¼Œæ³¨æ„çœ‹ç»“å°¾æ³¨é‡Š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ecc3d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== tokenizer è¾“å‡ºå¼ é‡ ===\n",
      "input_ids torch.Size([1, 11]) tensor([[ 101, 7592, 2088, 1012,  102, 2129, 2024, 2017, 2651, 1029,  102]])\n",
      "token_type_ids torch.Size([1, 11]) tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n",
      "attention_mask torch.Size([1, 11]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "=== BERT è¾“å‡º ===\n",
      "last_hidden_state: torch.Size([1, 11, 768])\n",
      "pooler_output: torch.Size([1, 768])\n",
      "\n",
      "[CLS] embedding shape: torch.Size([1, 768])\n",
      "First real token embedding shape: torch.Size([1, 768])\n",
      "\n",
      "[CLS] embedding (å‰10ç»´):\n",
      "tensor([-0.2158,  0.2033, -0.0384, -0.3991, -0.4776, -0.2867,  0.7173,  0.7658,\n",
      "        -0.0283, -0.3685])\n",
      "\n",
      "ç¬¬ä¸€ä¸ªtokençš„embedding (å‰10ç»´):\n",
      "tensor([ 0.0255,  0.0870,  0.7774, -0.7172,  0.2613, -0.0265,  0.5924,  0.6962,\n",
      "        -0.6278, -1.4433])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1. åŠ è½½é¢„è®­ç»ƒçš„ä¸­æ–‡ or è‹±æ–‡ BERT\n",
    "# å¦‚æœä½ ä¸»è¦æ˜¯è‹±æ–‡æ–‡æœ¬ï¼Œç”¨ 'bert-base-uncased'\n",
    "# å¦‚æœä½ æƒ³ä¸­æ–‡ï¼Œç”¨ 'bert-base-chinese'\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2. å‡†å¤‡è¾“å…¥æ–‡æœ¬ï¼ˆå•å¥ or å¥å¯¹éƒ½è¡Œï¼‰\n",
    "sentence_a = \"Hello world.\"\n",
    "sentence_b = \"How are you today?\"\n",
    "# sentence_c = \"How are you today?\"\n",
    "\n",
    "\n",
    "# 3. tokenizerï¼šæŠŠæ–‡æœ¬ â†’ token ids / segment ids / attention mask\n",
    "encoded = tokenizer(\n",
    "    sentence_a,\n",
    "    sentence_b,                    # ä¼ ç¬¬äºŒä¸ªå¥å­ -> ä¼šè‡ªåŠ¨åŠ  [SEP] å¹¶ç”Ÿæˆ token_type_idsåŒºåˆ†å¥A/B\n",
    "    # sentence_c,\n",
    "    \n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",           # ç›´æ¥è¿”å› PyTorch tensor\n",
    ")\n",
    "\n",
    "print(\"=== tokenizer è¾“å‡ºå¼ é‡ ===\")\n",
    "for k, v in encoded.items():\n",
    "    print(k, v.shape, v)\n",
    "\n",
    "# encoded é‡Œå…¸å‹æœ‰ï¼š\n",
    "# - input_ids:        token id åºåˆ—\n",
    "# - token_type_ids:   å¥å­Aæ˜¯0ï¼Œå¥å­Bæ˜¯1\n",
    "# - attention_mask:   1=æœ‰æ•ˆtokenï¼Œ0=padding\n",
    "\n",
    "# 4. å‰å‘ï¼šé€è¿› BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "\n",
    "# 5. æ‹¿ç»“æœ\n",
    "last_hidden_state = outputs.last_hidden_state    # [batch, seq_len, hidden]\n",
    "pooler_output    = outputs.pooler_output         # [batch, hidden]ï¼Œç­‰ä»·äº\"å¥å­å‘é‡\"\n",
    "\n",
    "print(\"\\n=== BERT è¾“å‡º ===\")\n",
    "print(\"last_hidden_state:\", last_hidden_state.shape)\n",
    "print(\"pooler_output:\", pooler_output.shape)\n",
    "\n",
    "# 6. çœ‹çœ‹ [CLS] å‘é‡ï¼Œå’Œä»»æ„ä¸€ä¸ª token çš„å‘é‡\n",
    "cls_embedding = last_hidden_state[:, 0, :]       # ç¬¬0ä¸ªä½ç½®æ˜¯ [CLS]\n",
    "tok1_embedding = last_hidden_state[:, 1, :]      # ç¬¬1ä¸ªä½ç½®æ˜¯ç¬¬ä¸€ä¸ªçœŸå®token\n",
    "\n",
    "print(\"\\n[CLS] embedding shape:\", cls_embedding.shape)\n",
    "print(\"First real token embedding shape:\", tok1_embedding.shape)\n",
    "\n",
    "# 7. ä¸¾ä¸ªå®é™…æ•°å€¼ç‰‡æ®µ\n",
    "print(\"\\n[CLS] embedding (å‰10ç»´):\")\n",
    "print(cls_embedding[0, :10])\n",
    "\n",
    "print(\"\\nç¬¬ä¸€ä¸ªtokençš„embedding (å‰10ç»´):\")\n",
    "print(tok1_embedding[0, :10])\n",
    "\n",
    "\n",
    "##########################################\n",
    "# ğŸ“Œ å…³é”®è§£è¯»ï¼ˆçœ‹çœ‹æ‰“å°å‡ºæ¥çš„ä¸œè¥¿æ—¶å¯ä»¥å¯¹ç…§ä¸‹é¢è¯»ï¼‰\n",
    "##########################################\n",
    "\n",
    "# 1) input_ids\n",
    "#    è¿™æ˜¯ tokenizer æŠŠ \"[CLS] Hello world . [SEP] How are you today ? [SEP]\"\n",
    "#    å…¨éƒ¨æ¢æˆè¯è¡¨IDåçš„æ•´æ•°åºåˆ—ã€‚\n",
    "#\n",
    "# 2) token_type_ids\n",
    "#    åŒé•¿åº¦å‘é‡ï¼Œæ¯”å¦‚ [0,0,0,0,0,1,1,1,1,1,1,...]\n",
    "#    0 è¡¨ç¤ºå±äºå¥å­Aï¼Œ1 è¡¨ç¤ºå±äºå¥å­Bã€‚\n",
    "#\n",
    "# 3) attention_mask\n",
    "#    å’Œ seq_len ç­‰é•¿ï¼Œæ¯”å¦‚ [1,1,1,1,1,1,...,1,0,0,0]\n",
    "#    å‘Šè¯‰BERTå“ªäº›ä½ç½®æ˜¯çœŸå®token(1)ï¼Œå“ªäº›æ˜¯padding(0)ï¼Œ\n",
    "#    æ³¨æ„åŠ›ä¼šåœ¨paddingä¸Šè¢«å±è”½ï¼Œä¸ä¼šå½±å“åˆ«çš„è¯ã€‚\n",
    "#\n",
    "# 4) last_hidden_state\n",
    "#    å½¢çŠ¶ [batch_size, seq_len, hidden_size]\n",
    "#    æ¯ä¸ªtokenä¸€ä¸ªå‘é‡ï¼ˆ768ç»´ for bert-baseï¼‰ã€‚\n",
    "#    é€‚åˆåš token-level ä»»åŠ¡ï¼Œä¾‹å¦‚ NERã€é—®ç­”é‡Œçš„spanæŠ½å–ã€‚\n",
    "#\n",
    "# 5) pooler_output\n",
    "#    å½¢çŠ¶ [batch_size, hidden_size]\n",
    "#    è¿™æ˜¯ BERT å®˜æ–¹\"å¥å­å‘é‡\"ï¼šå®ƒå– last_hidden_state[:,0,:] ä¹Ÿå°±æ˜¯ [CLS]ï¼Œ\n",
    "#    ä½†è¿˜è¿‡äº†ä¸€å±‚çº¿æ€§+Tanhã€‚å¸¸æ‹¿æ¥åšå¥å­åˆ†ç±»ã€‚\n",
    "#\n",
    "# 6) cls_embedding\n",
    "#    æˆ‘ä»¬æ‰‹åŠ¨å–çš„ last_hidden_state[:,0,:]ã€‚\n",
    "#    è·Ÿ pooler_output å¾ˆæ¥è¿‘ï¼Œä½† pooler_output è¿˜å¤šä¸€å±‚å˜æ¢ã€‚\n",
    "#\n",
    "# âœ… æ€»ç»“:\n",
    "# - å¦‚æœä½ è¦å¥å­åˆ†ç±»ï¼Œç”¨ pooler_output æˆ– [CLS] å‘é‡æ¥çº¿æ€§å±‚ã€‚\n",
    "# - å¦‚æœä½ è¦é€è¯æ ‡æ³¨ï¼Œç”¨ last_hidden_state çš„æ¯ä¸ªä½ç½®ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
