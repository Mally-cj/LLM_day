{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf974e83",
   "metadata": {},
   "source": [
    "2025-10-28 23:38:55 Tuesday\n",
    "\n",
    "\n",
    "面试问题记录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d8f43",
   "metadata": {},
   "source": [
    "\n",
    "## 1.分类任务与异常检测的区别\n",
    "\n",
    "**分类任务**：目标是区分多个“已知类别”，假设每个类别都有充足的标注样本。模型通过学习不同类别之间的判别边界来实现分类，常采用**交叉熵损失（cross-entropy loss）**作为训练目标。\n",
    "\n",
    "**异常检测**：目标是发现“未知或罕见的异常”，假设只有“正常类”样本充足，而异常样本极少甚至完全没有标签。此类任务通常缺乏完整的监督信号，因此模型更关注学习“正常样本的分布特征”，并通过**距离度量**或**重构误差**来识别那些偏离正常分布的样本。\n",
    "\n",
    "---\n",
    "\n",
    "### 将异常检测任务直接建模为分类任务的问题\n",
    "\n",
    "在实际应用中，例如“风险问题识别”场景下，风险样本往往极为稀少，而正常问题占绝大多数。\n",
    "如果直接将任务建模为“有风险 / 无风险”的二分类问题，模型会因数据极度不平衡而难以学习到可靠的判别边界，往往表现为：\n",
    "\n",
    "* 风险识别的准确率与召回率难以同时提升，二者此消彼长；\n",
    "* 对未见过的或新型风险类型缺乏泛化能力。\n",
    "\n",
    "---\n",
    "\n",
    "### 改造思路：将任务转化为异常检测\n",
    "\n",
    "为缓解上述问题，可以将任务改造为**异常检测问题**：\n",
    "首先，仅利用大量“正常”问题样本，通过 **BERT 编码器**提取每条文本的语义向量，使文本在语义空间中获得结构化的语义表示；\n",
    "然后，使用这些语义向量训练一个 **Auto-Encoder（自编码器）** 网络，使模型学习“正常问题”的语义分布规律。\n",
    "将稀少的风险样本作为验证集，用于**阈值校准（threshold calibration）**，确定合适的重构误差阈值。\n",
    "\n",
    "在推理阶段，当新的输入样本到来时，将其语义向量输入 Auto-Encoder 并计算重构误差——若该误差显著大于校准阈值，即表示该问题在语义上偏离了正常分布，可被判定为“潜在风险”或“异常样本”。\n",
    "\n",
    "---\n",
    "\n",
    "### 相较于直接二分类的优势\n",
    "\n",
    "1. **避免数据不平衡影响**：训练阶段仅使用正常样本，无需依赖大量风险样本标注。\n",
    "2. **具备更强的泛化能力**：模型学习的是“正常样本的分布”，因此即便出现新的风险类型，只要其语义特征与正常样本差异显著，也能被识别为异常。\n",
    "3. **风险控制更具可解释性**：通过重构误差或语义距离可以量化“样本偏离正常分布的程度”，从而实现动态阈值调整与分级风控处理。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e38ac5",
   "metadata": {},
   "source": [
    "## 2.bert的输出\n",
    "\n",
    "bert的输出有两类结果\n",
    "\n",
    "| 输出                       | 形状                                   | 含义                                       |\n",
    "| ------------------------ | ------------------------------------ | ---------------------------------------- |\n",
    "| **1. last_hidden_state** | `[batch_size, seq_len, hidden_size]` | 每个 token 的上下文向量表示                        |\n",
    "| **2. pooler_output**     | `[batch_size, hidden_size]`          | [CLS] 位置经过一个 `tanh` 层后的句向量（BERT 的默认句子表示） |\n",
    "\n",
    "\n",
    "下面是一个代码实现，注意看结尾注释\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ecc3d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== tokenizer 输出张量 ===\n",
      "input_ids torch.Size([1, 11]) tensor([[ 101, 7592, 2088, 1012,  102, 2129, 2024, 2017, 2651, 1029,  102]])\n",
      "token_type_ids torch.Size([1, 11]) tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n",
      "attention_mask torch.Size([1, 11]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "=== BERT 输出 ===\n",
      "last_hidden_state: torch.Size([1, 11, 768])\n",
      "pooler_output: torch.Size([1, 768])\n",
      "\n",
      "[CLS] embedding shape: torch.Size([1, 768])\n",
      "First real token embedding shape: torch.Size([1, 768])\n",
      "\n",
      "[CLS] embedding (前10维):\n",
      "tensor([-0.2158,  0.2033, -0.0384, -0.3991, -0.4776, -0.2867,  0.7173,  0.7658,\n",
      "        -0.0283, -0.3685])\n",
      "\n",
      "第一个token的embedding (前10维):\n",
      "tensor([ 0.0255,  0.0870,  0.7774, -0.7172,  0.2613, -0.0265,  0.5924,  0.6962,\n",
      "        -0.6278, -1.4433])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 1. 加载预训练的中文 or 英文 BERT\n",
    "# 如果你主要是英文文本，用 'bert-base-uncased'\n",
    "# 如果你想中文，用 'bert-base-chinese'\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2. 准备输入文本（单句 or 句对都行）\n",
    "sentence_a = \"Hello world.\"\n",
    "sentence_b = \"How are you today?\"\n",
    "# sentence_c = \"How are you today?\"\n",
    "\n",
    "\n",
    "# 3. tokenizer：把文本 → token ids / segment ids / attention mask\n",
    "encoded = tokenizer(\n",
    "    sentence_a,\n",
    "    sentence_b,                    # 传第二个句子 -> 会自动加 [SEP] 并生成 token_type_ids区分句A/B\n",
    "    # sentence_c,\n",
    "    \n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",           # 直接返回 PyTorch tensor\n",
    ")\n",
    "\n",
    "print(\"=== tokenizer 输出张量 ===\")\n",
    "for k, v in encoded.items():\n",
    "    print(k, v.shape, v)\n",
    "\n",
    "# encoded 里典型有：\n",
    "# - input_ids:        token id 序列\n",
    "# - token_type_ids:   句子A是0，句子B是1\n",
    "# - attention_mask:   1=有效token，0=padding\n",
    "\n",
    "# 4. 前向：送进 BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded)\n",
    "\n",
    "# 5. 拿结果\n",
    "last_hidden_state = outputs.last_hidden_state    # [batch, seq_len, hidden]\n",
    "pooler_output    = outputs.pooler_output         # [batch, hidden]，等价于\"句子向量\"\n",
    "\n",
    "print(\"\\n=== BERT 输出 ===\")\n",
    "print(\"last_hidden_state:\", last_hidden_state.shape)\n",
    "print(\"pooler_output:\", pooler_output.shape)\n",
    "\n",
    "# 6. 看看 [CLS] 向量，和任意一个 token 的向量\n",
    "cls_embedding = last_hidden_state[:, 0, :]       # 第0个位置是 [CLS]\n",
    "tok1_embedding = last_hidden_state[:, 1, :]      # 第1个位置是第一个真实token\n",
    "\n",
    "print(\"\\n[CLS] embedding shape:\", cls_embedding.shape)\n",
    "print(\"First real token embedding shape:\", tok1_embedding.shape)\n",
    "\n",
    "# 7. 举个实际数值片段\n",
    "print(\"\\n[CLS] embedding (前10维):\")\n",
    "print(cls_embedding[0, :10])\n",
    "\n",
    "print(\"\\n第一个token的embedding (前10维):\")\n",
    "print(tok1_embedding[0, :10])\n",
    "\n",
    "\n",
    "##########################################\n",
    "# 📌 关键解读（看看打印出来的东西时可以对照下面读）\n",
    "##########################################\n",
    "\n",
    "# 1) input_ids\n",
    "#    这是 tokenizer 把 \"[CLS] Hello world . [SEP] How are you today ? [SEP]\"\n",
    "#    全部换成词表ID后的整数序列。\n",
    "#\n",
    "# 2) token_type_ids\n",
    "#    同长度向量，比如 [0,0,0,0,0,1,1,1,1,1,1,...]\n",
    "#    0 表示属于句子A，1 表示属于句子B。\n",
    "#\n",
    "# 3) attention_mask\n",
    "#    和 seq_len 等长，比如 [1,1,1,1,1,1,...,1,0,0,0]\n",
    "#    告诉BERT哪些位置是真实token(1)，哪些是padding(0)，\n",
    "#    注意力会在padding上被屏蔽，不会影响别的词。\n",
    "#\n",
    "# 4) last_hidden_state\n",
    "#    形状 [batch_size, seq_len, hidden_size]\n",
    "#    每个token一个向量（768维 for bert-base）。\n",
    "#    适合做 token-level 任务，例如 NER、问答里的span抽取。\n",
    "#\n",
    "# 5) pooler_output\n",
    "#    形状 [batch_size, hidden_size]\n",
    "#    这是 BERT 官方\"句子向量\"：它取 last_hidden_state[:,0,:] 也就是 [CLS]，\n",
    "#    但还过了一层线性+Tanh。常拿来做句子分类。\n",
    "#\n",
    "# 6) cls_embedding\n",
    "#    我们手动取的 last_hidden_state[:,0,:]。\n",
    "#    跟 pooler_output 很接近，但 pooler_output 还多一层变换。\n",
    "#\n",
    "# ✅ 总结:\n",
    "# - 如果你要句子分类，用 pooler_output 或 [CLS] 向量接线性层。\n",
    "# - 如果你要逐词标注，用 last_hidden_state 的每个位置。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
