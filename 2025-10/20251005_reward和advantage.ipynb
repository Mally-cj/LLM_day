{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65045471",
   "metadata": {},
   "source": [
    "# 为什么有reward，还要有advantage？\n",
    "\n",
    "\n",
    "因为场景是对一个问题进行回答，然后对回答给奖励，这个回答是多样的，使得奖励也是多样随机的，而策略梯度中，梯度更新的方向是由奖励决定的，于是奖励很重要，奖励方差太大会导致方向乱偏，故而引入基线值，这样由绝对奖励获得了相对奖励，即advantage。\n",
    "\n",
    "\n",
    "\n",
    "因为在强化学习中，一个问题可能有多种不同的回答方式（或动作序列），这些回答的结果具有随机性，使得得到的奖励（reward）也会呈现出较大的方差。\n",
    "\n",
    "在策略梯度方法中，策略参数的更新方向由奖励信号决定。如果奖励方差太大，就会导致梯度方向不稳定，从而使训练震荡甚至发散。\n",
    "\n",
    "为了减小这种方差，我们引入一个**基线值（baseline）**，通常取当前状态下的平均期望回报 (V(s))。\n",
    "将原始的绝对奖励转化为**相对奖励**（即 advantage）： \n",
    "$$A(s,a) = R - V(s)$$\n",
    "这样更新时只关注“比平均水平好多少”，既保持了期望不变，又显著降低了方差，使训练更稳定。\n",
    "\n",
    "\n",
    "## 强化学习分为policy-based 和 value-base，他们有什么区别？\n",
    "\n",
    "强化学习的核心是一个智能体（agent）通过与环境（environment）互动、不断试错，从而学会在不同情况下采取最佳行动的过程。\n",
    "智能体的目标是学习一个策略 $\\pi(a|s)$，使得它能在长期内 获得尽可能高的总奖励。\n",
    "\n",
    "这里有两种方法，使得奖励最大，一种\n",
    "在当前状态s下，有很多动作选择，\n",
    "现在要让动作的价值最大，\n",
    "\n",
    "Q(s,a) 表示在状态 s 下执行动作 a 后，未来预期的总奖励。\n",
    "枚举动作，然后选择Q(s,a)最大，这就是value-based。它的特点就是先知道奖励再做动作。\n",
    "那么它的缺陷就是需要能知道s下做动作a的奖励会是多少。\n",
    "\n",
    "policy-based 就是在状态s下先执行动作a，然后看奖励是多少，再指引梯度更新的幅度，如果奖励是正数，那么梯度也就前进，否则就后退。\n",
    "公式为奖励 max_delta_r=0,\n",
    "\n",
    "## policy-based的损失函数是怎么推导出来的？\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a36599",
   "metadata": {},
   "source": [
    "\n",
    "> 因为在强化学习中，一个问题可能有多种不同的回答方式（或动作序列），这些回答的结果具有随机性，使得得到的奖励（reward）也会呈现出较大的方差。\n",
    ">\n",
    "> 在策略梯度方法中，策略参数的更新方向由奖励信号决定。如果奖励方差太大，就会导致梯度方向不稳定，从而使训练震荡甚至发散。\n",
    ">\n",
    "> 为了减小这种方差，我们引入一个**基线值（baseline）**，通常取当前状态下的平均期望回报 (V(s))。\n",
    "> 将原始的绝对奖励转化为**相对奖励**（即 advantage）：\n",
    "> [\n",
    "> A(s,a) = R - V(s)\n",
    "> ]\n",
    "> 这样更新时只关注“比平均水平好多少”，既保持了期望不变，又显著降低了方差，使训练更稳定。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb21b93",
   "metadata": {},
   "source": [
    "## 强化学习的两大范式：Value-based 与 Policy-based\n",
    "\n",
    "强化学习（Reinforcement Learning, RL）的核心思想是：\n",
    "一个智能体（Agent）通过与环境（Environment）不断交互、试错和反馈，\n",
    "逐渐学会在不同状态下采取最优行动，从而最大化长期回报（Reward）。\n",
    "\n",
    "智能体的目标是学习一个策略（Policy）：\n",
    "[\n",
    "\\pi(a|s)\n",
    "]\n",
    "它表示在状态 ( s ) 下选择动作 ( a ) 的概率。\n",
    "强化学习的本质目标是最大化期望回报：\n",
    "[\n",
    "J(\\theta) = \\mathbb{E}*{\\pi*\\theta}[R_t]\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### 一、Value-based 方法：学习“动作有多好”\n",
    "\n",
    "**核心思想：**\n",
    "是学习一个“动作价值函数”：\n",
    "[\n",
    "Q(s,a) = \\mathbb{E}[R_t \\mid s_t = s, a_t = a]\n",
    "]\n",
    "即在状态 ( s ) 下执行动作 ( a ) 后，未来能获得的期望总奖励。\n",
    "\n",
    "然后智能体通过：\n",
    "[\n",
    "a^* = \\arg\\max_a Q(s,a)\n",
    "]\n",
    "来选择当前最优动作。\n",
    "\n",
    "这类方法称为 **Value-based**（基于值的方法）。\n",
    "典型代表有 **Q-learning**、**DQN（Deep Q-Network）** 等。\n",
    "\n",
    "其优化目标源于 **Bellman 方程** 的自一致性：\n",
    "[\n",
    "Q^*(s,a) = \\mathbb{E}\\big[r + \\gamma \\max_{a'} Q^*(s',a')\\big]\n",
    "]\n",
    "\n",
    "于是定义损失函数为：\n",
    "[\n",
    "L(\\theta) = \\big(Q_\\theta(s,a) - [r + \\gamma \\max_{a'} Q_\\theta(s',a')]\\big)^2\n",
    "]\n",
    "\n",
    "通过最小化这个 TD 误差（Temporal Difference Error），\n",
    "模型学会准确估计各动作的价值。\n",
    "\n",
    "> 💬 直观理解：\n",
    "> 在每个状态下，智能体评估所有动作的价值，然后选择“最有前途”的那个。\n",
    "> 它是“先算好再选”，因此更像是“基于预测的决策”。\n",
    "\n",
    "---\n",
    "\n",
    "### 二、Policy-based 方法：直接学“怎么选得更好”\n",
    "\n",
    "与其学习动作的价值，不如**直接学习策略本身**。\n",
    "即参数化一个策略：\n",
    "[\n",
    "\\pi_\\theta(a|s)\n",
    "]\n",
    "并使得长期回报的期望最大化：\n",
    "[\n",
    "J(\\theta) = \\mathbb{E}*{\\pi*\\theta}[R_t]\n",
    "]\n",
    "\n",
    "根据 **策略梯度定理（Policy Gradient Theorem）**，\n",
    "目标函数的梯度为：\n",
    "[\n",
    "\\nabla_\\theta J(\\theta)\n",
    "= \\mathbb{E}*{\\pi*\\theta}\\left[\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a|s) , R_t\n",
    "\\right]\n",
    "]\n",
    "\n",
    "这意味着我们可以用采样得到的奖励信号 ( R_t )\n",
    "来指导策略参数的更新方向：\n",
    "\n",
    "* 若奖励高（(R_t > 0)），则增大对应动作的概率；\n",
    "* 若奖励低（(R_t < 0)），则减小对应动作的概率。\n",
    "\n",
    "于是得到优化目标：\n",
    "[\n",
    "L(\\theta) = - \\mathbb{E}[\\log \\pi_\\theta(a|s) R_t]\n",
    "]\n",
    "\n",
    "> 💬 直观理解：\n",
    "> policy-based 是“先做再学”——通过试错探索环境，\n",
    "> 根据获得的奖励信号调整“动作概率”的方向。\n",
    "> 它能天然处理**连续动作空间**，是强化学习在复杂任务（如机器人控制）中的主流。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
