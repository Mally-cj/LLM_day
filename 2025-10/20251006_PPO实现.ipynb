{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b35e2b",
   "metadata": {},
   "source": [
    "# PPO的代码实现\n",
    "代码来自于，https://github.com/nikhilbarhate99/PPO-PyTorch\n",
    "\n",
    "总体流程为：\n",
    "\n",
    "    环境交互:\n",
    "        ↓\n",
    "    PPO.select_action(state)\n",
    "        ↓\n",
    "    环境返回 reward, done\n",
    "        ↓\n",
    "    数据存入 RolloutBuffer\n",
    "        ↓\n",
    "    每 N 步后:\n",
    "        ↓\n",
    "    PPO.update()\n",
    "        ├─ 计算 G_t 和优势 A_t\n",
    "        ├─ 计算 ratio = π/π_old\n",
    "        ├─ 优化 min(surr1, surr2)\n",
    "        ├─ 更新 Critic\n",
    "        └─ 同步 policy_old ← policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec02671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "################################## set device ##################################\n",
    "print(\"============================================================================================\")\n",
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "################################## PPO Policy ##################################\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "        \n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "        # actor\n",
    "        if has_continuous_action_space :\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Tanh()\n",
    "                        )\n",
    "        else:\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 1)\n",
    "                    )\n",
    "        \n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def act(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_val = self.critic(state)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            \n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var).to(device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "            \n",
    "            # For Single Action Environments.\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    def select_action(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "            \n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss  \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "   \n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        \n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abba93",
   "metadata": {},
   "source": [
    "## PPO 任务示例\n",
    "\n",
    "我们让 GPT-2 学会生成“正向情绪”的句子。\n",
    "系统提供一些简单的 **prompt（提示语）**，例如：\n",
    "\n",
    "```text\n",
    "\"Write a short product review:\"\n",
    "\"Describe your day:\"\n",
    "\"Give quick feedback on this service:\"\n",
    "```\n",
    "\n",
    "模型生成一段续写文本。\n",
    "然后我们根据生成文本中的**情感词汇**给出奖励：\n",
    "\n",
    "| 奖励规则                                                        | 示例   |\n",
    "| ----------------------------------------------------------- | ---- |\n",
    "| 若文本中包含 `\"good\"`, `\"great\"`, `\"excellent\"`, `\"wonderful\"` 等词 | +1.0 |\n",
    "| 否则                                                          | −0.5 |\n",
    "\n",
    "因此模型会逐渐学会偏向使用这些积极词汇。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926fa26c",
   "metadata": {},
   "source": [
    "### ⚙️ 模型结构（Architecture）\n",
    "\n",
    "整体使用一个 **Actor–Critic** 框架：\n",
    "\n",
    "```\n",
    "        +-----------------------------+\n",
    "        |        GPT-2 backbone       |\n",
    "        +-----------------------------+\n",
    "              ↙                   ↘\n",
    "        Policy (actor)         Value (critic)\n",
    "         → 输出 logits          → 线性层预测 V(s)\n",
    "```\n",
    "\n",
    "* **Actor**：GPT-2 的语言建模头，输出每个 token 的概率分布。\n",
    "* **Critic**：在最后一层 hidden 上加线性层，预测状态价值 (V(s_t))。\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 PPO 算法要点\n",
    "\n",
    "1. **Ratio (重要性采样比率)**\n",
    "   $$r_t = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\text{old}}(a_t|s_t)}$$\n",
    "   \n",
    "\n",
    "2. **剪切目标 (Clipped Surrogate Objective)**\n",
    "   \n",
    "   $$L^{clip} = \\min\\big(r_t A_t,, \\text{clip}(r_t,1-\\epsilon,1+\\epsilon) A_t\\big)$$\n",
    "   \n",
    "\n",
    "3. **Critic Loss (值函数误差)**\n",
    "   \n",
    "   $$L^{V} = (V_\\phi(s_t)-R_t)^2$$\n",
    "\n",
    "4. **Entropy Bonus (熵正则)**\n",
    "   鼓励策略保持多样性。\n",
    "\n",
    "5. **最终目标（要最大化）**\n",
    "   \n",
    "   $$L = \\mathbb{E}[L^{clip} - c_1 L^{V} + c_2 \\mathcal{H}]$$\n",
    "   实现时取负号作为 loss 进行最小化。\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 优势与回报计算\n",
    "\n",
    "使用 GAE(λ)（广义优势估计）：\n",
    "\n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "$$A_t = \\sum_l (\\gamma \\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "\n",
    "$$R_t = A_t + V(s_t)$$\n",
    "\n",
    "在这个 demo 中：\n",
    "\n",
    "* 每个生成序列只有**一个终局奖励**；\n",
    "* 奖励分配到最后一个 token，并通过 GAE 反向传播；\n",
    "* 非生成部分（prompt）mask 掉，不参与更新。\n",
    "\n",
    "---\n",
    "\n",
    "### 🧰 训练流程\n",
    "\n",
    "1. **Rollout 收集数据**\n",
    "\n",
    "   * 用当前策略 GPT-2 生成文本。\n",
    "   * 记录：input_ids, attention_mask, logits, value, logp_old。\n",
    "   * 计算奖励（正向词检测）。\n",
    "\n",
    "2. **计算优势与回报**\n",
    "\n",
    "   * 用 critic 的值估计结合 reward 算 GAE。\n",
    "   * 生成 `advantages` 和 `returns`。\n",
    "\n",
    "3. **PPO 更新**\n",
    "\n",
    "   * 多次小批量（minibatch）训练。\n",
    "   * 计算新策略 logp，与旧策略比 ratio。\n",
    "   * 进行 **剪切策略目标** + **value loss** + **entropy** 优化。\n",
    "   * 使用梯度裁剪防止发散。\n",
    "\n",
    "4. **监控指标**\n",
    "\n",
    "   * loss 组件（policy / value / entropy）\n",
    "   * KL 散度（防止策略漂移）\n",
    "   * 平均奖励变化\n",
    "\n",
    "---\n",
    "\n",
    "### 数据整体流动逻辑总结\n",
    "prompt\n",
    "  ↓\n",
    "生成 token_1, token_2, ... token_T\n",
    "  ↓\n",
    "critic 给出每步 V(s_t)\n",
    "  ↓\n",
    "reward 仅在最后 token 给出 (+1 或 -0.5)\n",
    "  ↓\n",
    "GAE 根据 V 和 r_t 把“未来好处”往前传\n",
    "  ↓\n",
    "得到每步优势 A_t\n",
    "  ↓\n",
    "policy_loss 用 A_t 更新 actor（GPT2 logits）\n",
    "value_loss 用 (R_t - V(s_t))^2 更新 critic\n",
    "\n",
    "\n",
    "### 📊 代码结构概览\n",
    "\n",
    "```\n",
    "├── CFG                # 参数与任务配置\n",
    "├── ActorCritic        # GPT2 + Value Head 模型\n",
    "├── reward_fn()        # 简易奖励函数\n",
    "├── compute_logprobs() # token级log概率\n",
    "├── compute_returns_advantages() # GAE\n",
    "├── iterate_minibatches()        # 小批量迭代器\n",
    "│\n",
    "├── rollout & PPO loop           # 主训练循环\n",
    "│\n",
    "└── sample_text()                # 前后生成对比\n",
    "```\n",
    "\n",
    "---\n",
    "t2_minimal_demo.py\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 🧭 结果示例（示意）\n",
    "\n",
    "```\n",
    "=== Sampling BEFORE training ===\n",
    "Prompt: Write a short product review:\n",
    "Write a short product review: It was a terrible day, the food was cold...\n",
    "\n",
    "=== Sampling AFTER training ===\n",
    "Prompt: Write a short product review:\n",
    "Write a short product review: The service was great and the food was excellent!\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b2785",
   "metadata": {},
   "source": [
    "### 参数与任务配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo_gpt2_minimal_demo.py\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast as GPT2Tokenizer\n",
    "\n",
    "# ===================== 配置 =====================\n",
    "@dataclass\n",
    "class CFG:\n",
    "    model_name: str = \"gpt2\"\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 42\n",
    "\n",
    "    # rollout\n",
    "    prompts: List[str] = (\n",
    "        \"Write a short product review:\",\n",
    "        \"Describe your day:\",\n",
    "        \"Tell me something about your favorite movie:\",\n",
    "        \"How was your meal?\",\n",
    "        \"Give quick feedback on this service:\",\n",
    "    )\n",
    "    max_new_tokens: int = 24\n",
    "    eos_token_id: int = None   # 自动从tokenizer里拿\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 50\n",
    "\n",
    "    # PPO\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_eps: float = 0.2\n",
    "    value_clip_eps: float = 0.2\n",
    "    c1_value: float = 0.5\n",
    "    c2_entropy: float = 0.01\n",
    "    lr: float = 1e-5\n",
    "    max_grad_norm: float = 0.5\n",
    "\n",
    "    # 训练\n",
    "    rollout_batch_size: int = 6     # 每轮收集多少条序列\n",
    "    ppo_epochs: int = 3             # 对同一批数据做几次小批量更新\n",
    "    minibatch_size: int = 3\n",
    "    train_steps: int = 5            # 总共做多少个 PPO outer steps（演示：很小）\n",
    "    print_every: int = 1\n",
    "\n",
    "    # 简单奖励词表（命中加分）\n",
    "    pos_words: Tuple[str,...] = (\"good\", \"great\", \"excellent\", \"wonderful\")\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# ===================== 随机种子 =====================\n",
    "random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "# ===================== 模型 & tokenizer =====================\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(cfg.model_name)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "cfg.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b98231",
   "metadata": {},
   "source": [
    "### 奖励函数实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea27e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简易奖励函数：生成文本包含正向词则高奖励，否则低奖励\n",
    "def reward_fn(texts: List[str], pos_words: Tuple[str,...]) -> torch.Tensor:\n",
    "    rewards = []\n",
    "    for t in texts:\n",
    "        t_low = t.lower()\n",
    "        matched = any(w in t_low for w in pos_words)\n",
    "        rewards.append(1.0 if matched else -0.5)\n",
    "    return torch.tensor(rewards, dtype=torch.float32)\n",
    "\n",
    "# 把序列末端奖励分配到动作token上，并做 GAE\n",
    "def compute_returns_advantages(\n",
    "    rewards: torch.Tensor,              # [B] 每条序列一个终局奖励\n",
    "    values: torch.Tensor,               # [B, T] 每个位置的V(s_t)\n",
    "    dones: torch.Tensor,                # [B] 终止标志（这里全1）\n",
    "    action_mask: torch.Tensor,          # [B, T] 只对生成部分为1，prompt部分为0\n",
    "    gamma: float,\n",
    "    lam: float\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # 我们让每条序列的最后一个 action token 接收 reward，其它 action token 也能通过 GAE 回传\n",
    "    B, T = values.shape\n",
    "    device = values.device\n",
    "\n",
    "    # 构建 per-step reward：默认为0，只在每条序列最后一个有效 action 位置放入终局奖励\n",
    "    step_rewards = torch.zeros_like(values)\n",
    "    last_idx = (action_mask.cumsum(dim=1) == action_mask.sum(dim=1, keepdim=True)).long().argmax(dim=1)  # 每条序列最后一个action索引\n",
    "    for b in range(B):\n",
    "        step_rewards[b, last_idx[b]] = rewards[b]\n",
    "\n",
    "    # 计算 deltas 与 GAE，仅在 action_mask==1 的位置有效\n",
    "    deltas = step_rewards + gamma * F.pad(values[:, 1:], (0,1)) * (1.0 - 0.0) - values\n",
    "    deltas = deltas * action_mask\n",
    "\n",
    "    advantages = torch.zeros_like(values)\n",
    "    last_gae = torch.zeros(B, device=device)\n",
    "    for t in reversed(range(T)):\n",
    "        mask_t = action_mask[:, t]\n",
    "        delta_t = deltas[:, t]\n",
    "        last_gae = delta_t + gamma * lam * last_gae\n",
    "        advantages[:, t] = last_gae * mask_t  # 非action位置为0\n",
    "\n",
    "        # 在非action位置维持 last_gae 不被无意义地传播\n",
    "        last_gae = last_gae * (mask_t > 0).float() + last_gae * (mask_t == 0).float()\n",
    "\n",
    "    returns = advantages + values\n",
    "    return returns.detach(), advantages.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d50ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/trl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sampling BEFORE training ===\n",
      "\n",
      "[Before] Prompt: Write a short product review:\n",
      "Write a short product review:\n",
      "\n",
      "Product Reviews Write about the products of the following company: CXV, General Dynamics, Intel\n",
      "\n",
      "This\n",
      "\n",
      "\n",
      "[Before] Prompt: Describe your day:\n",
      "Describe your day: What was it like for you? A? B ? I. B a? Or C ? I ? E ? I\n",
      "\n",
      "\n",
      "=== Sampling AFTER training ===\n",
      "\n",
      "[Before] Prompt: Write a short product review:\n",
      "Write a short product review: www.goodreads.com/reviews/lulu-coco-coco-no...\n",
      "\n",
      "\n",
      "[Before] Prompt: Describe your day:\n",
      "Describe your day: what does the man know? What is that one thing?\n",
      "\n",
      "Do you feel like a man? What is life\n",
      "\n",
      "\n",
      "[Before] Prompt: Tell me something about your favorite movie:\n",
      "Tell me something about your favorite movie: \"I'm like a woman, you know.\" There's an intense intensity to the character, and if you're young\n",
      "\n",
      "\n",
      "[Before] Prompt: How was your meal?\n",
      "How was your meal? How many bottles are there? I've found to be more expensive than you're probably expecting.\n",
      "\n",
      "My first night\n",
      "\n",
      "[Final Demo]\n",
      "Write a short product review:\n",
      "\n",
      "You'll get no more time than you need to write and review reviews for every product.\n",
      "\n",
      "We offer\n"
     ]
    }
   ],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, base_model: GPT2LMHeadModel):\n",
    "        super().__init__()\n",
    "        self.actor = base_model\n",
    "        hidden = base_model.config.n_embd\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # 为了拿到hidden states，打开输出\n",
    "        out = self.actor(input_ids=input_ids,\n",
    "                         attention_mask=attention_mask,\n",
    "                         output_hidden_states=True,\n",
    "                         return_dict=True)\n",
    "        logits = out.logits                                  # [B, T, V]\n",
    "        last_hidden = out.hidden_states[-1]                  # [B, T, H]\n",
    "        values = self.value_head(last_hidden).squeeze(-1)    # [B, T]\n",
    "        return logits, values\n",
    "\n",
    "    def generate(self, input_ids, attention_mask, max_new_tokens, temperature=1.0, top_k=50, eos_token_id=None):\n",
    "        # 手写 sampling（也可用 model.generate，这里为了更清晰拿logprob）\n",
    "        self.eval()\n",
    "        B = input_ids.size(0)\n",
    "        cur_input_ids = input_ids\n",
    "        cur_attn = attention_mask\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits, _ = self.forward(cur_input_ids, cur_attn)\n",
    "                next_logits = logits[:, -1, :] / max(1e-6, temperature)\n",
    "                if top_k is not None and top_k > 0:\n",
    "                    topv, topi = torch.topk(next_logits, top_k)\n",
    "                    mask = next_logits < topv[:, -1].unsqueeze(-1)\n",
    "                    next_logits = next_logits.masked_fill(mask, -float(\"inf\"))\n",
    "                probs = F.softmax(next_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)  # [B,1]\n",
    "                cur_input_ids = torch.cat([cur_input_ids, next_token], dim=1)\n",
    "                cur_attn = torch.ones_like(cur_input_ids, device=cur_input_ids.device)\n",
    "                if eos_token_id is not None:\n",
    "                    # 如果所有样本都产生了eos，提前终止\n",
    "                    if (next_token.squeeze(-1) == eos_token_id).all():\n",
    "                        break\n",
    "        self.train()\n",
    "        return cur_input_ids, cur_attn\n",
    "\n",
    "# logprob 工具\n",
    "def compute_logprobs(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "    # logits: [B,T,V], labels: [B,T]，返回每个位置 token 的 log p(token)\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    lp = logp.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)  # [B,T]\n",
    "    return lp\n",
    "\n",
    "\n",
    "# mini-batch 迭代器\n",
    "def iterate_minibatches(batch: Dict[str, torch.Tensor], mb_size: int):\n",
    "    N = batch[\"input_ids\"].size(0)\n",
    "    idx = torch.randperm(N)\n",
    "    for start in range(0, N, mb_size):\n",
    "        mb_idx = idx[start : start + mb_size]\n",
    "        yield {k: v[mb_idx] for k, v in batch.items()}\n",
    "\n",
    "# ===================== 构建模型与优化器 =====================\n",
    "base = GPT2LMHeadModel.from_pretrained(cfg.model_name)\n",
    "model = ActorCritic(base).to(cfg.device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "\n",
    "\n",
    "# ===================== PPO 训练循环 =====================\n",
    "for step in range(1, cfg.train_steps + 1):\n",
    "    # -------- Rollout 收集 on-policy 轨迹 --------\n",
    "    batch_prompts = [random.choice(cfg.prompts) for _ in range(cfg.rollout_batch_size)]\n",
    "    enc = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True).to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 旧策略下采样序列\n",
    "        full_ids, full_attn = model.generate(\n",
    "            enc.input_ids, enc.attention_mask,\n",
    "            max_new_tokens=cfg.max_new_tokens,\n",
    "            temperature=cfg.temperature,\n",
    "            top_k=cfg.top_k,\n",
    "            eos_token_id=cfg.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # 计算旧策略的 logits/values 和 logp_old\n",
    "        logits_old, values_old = model(full_ids, attention_mask=full_attn)\n",
    "        # labels = 下一个token（自回归），用 -100 屏蔽最后一个位置\n",
    "        labels = full_ids[:, 1:].contiguous()\n",
    "        logits_old_trim = logits_old[:, :-1, :].contiguous()\n",
    "        values_old_trim = values_old[:, :-1].contiguous()\n",
    "        attn_trim = full_attn[:, :-1].contiguous()\n",
    "        logp_old = compute_logprobs(logits_old_trim, labels)\n",
    "\n",
    "    # 构造 action_mask：只对生成出的新token部分为1（去掉prompt token部分）\n",
    "    prompt_len = (enc.attention_mask.sum(dim=1)).to(full_ids.device)  # [B]\n",
    "    T = labels.size(1)\n",
    "    action_mask = torch.zeros_like(values_old_trim, dtype=torch.float32)\n",
    "    for b in range(cfg.rollout_batch_size):\n",
    "        # 可训练的 token 范围：从 prompt_len[b]-1 起到末尾（因为 labels 对应的是预测第 t+1 个 token）\n",
    "        start = int(prompt_len[b].item()) - 1\n",
    "        if start < 0: start = 0\n",
    "        action_mask[b, start:] = 1.0\n",
    "\n",
    "    # 文本解码用于奖励\n",
    "    decoded = tokenizer.batch_decode(full_ids, skip_special_tokens=True)\n",
    "    rewards = reward_fn(decoded, cfg.pos_words).to(cfg.device)          # [B]\n",
    "    dones = torch.ones_like(rewards)\n",
    "\n",
    "    # 计算 returns & advantages（只在 action_mask==1 的位置）\n",
    "    returns, advantages = compute_returns_advantages(\n",
    "        rewards=rewards,\n",
    "        values=values_old_trim,\n",
    "        dones=dones,\n",
    "        action_mask=action_mask,\n",
    "        gamma=cfg.gamma,\n",
    "        lam=cfg.gae_lambda,\n",
    "    )\n",
    "\n",
    "    # --------- 把 rollout 打包为训练 batch ----------\n",
    "    train_batch = {\n",
    "        \"input_ids\": full_ids[:, :-1].detach(),          # 和 logits_old_trim 对齐\n",
    "        \"attention_mask\": full_attn[:, :-1].detach(),\n",
    "        \"labels\": labels.detach(),\n",
    "        \"logp_old\": logp_old.detach(),\n",
    "        \"values_old\": values_old_trim.detach(),\n",
    "        \"returns\": returns.detach(),\n",
    "        \"advantages\": advantages.detach(),\n",
    "        \"action_mask\": action_mask.detach(),\n",
    "    }\n",
    "\n",
    "    # --------- PPO 多轮小批量更新 ----------\n",
    "    for epoch in range(cfg.ppo_epochs):\n",
    "        for mb in iterate_minibatches(train_batch, cfg.minibatch_size):\n",
    "            logits, values = model(mb[\"input_ids\"].to(cfg.device),\n",
    "                                   attention_mask=mb[\"attention_mask\"].to(cfg.device))\n",
    "            # 对齐\n",
    "            logits = logits\n",
    "            values = values\n",
    "\n",
    "            # 重新计算新策略的 logp\n",
    "            logp_new = compute_logprobs(logits, mb[\"labels\"].to(cfg.device))\n",
    "\n",
    "            # ratio & 策略损失（clip）\n",
    "            ratio = torch.exp(logp_new - mb[\"logp_old\"].to(cfg.device))                   # [B,T]\n",
    "            adv = mb[\"advantages\"].to(cfg.device)\n",
    "            # 标准化优势（仅对 action_mask 生效）\n",
    "            mask = mb[\"action_mask\"].to(cfg.device)\n",
    "            adv_masked = adv[mask > 0]\n",
    "            adv_norm = (adv - adv_masked.mean()) / (adv_masked.std() + 1e-8)\n",
    "\n",
    "            pg1 = ratio * adv_norm\n",
    "            pg2 = torch.clamp(ratio, 1.0 - cfg.clip_eps, 1.0 + cfg.clip_eps) * adv_norm\n",
    "            policy_loss = -(torch.min(pg1, pg2) * mask).sum() / (mask.sum() + 1e-8)\n",
    "\n",
    "            # value 损失（带 value clipping）\n",
    "            values_old = mb[\"values_old\"].to(cfg.device)\n",
    "            returns = mb[\"returns\"].to(cfg.device)\n",
    "            v_clipped = values_old + (values - values_old).clamp(-cfg.value_clip_eps, cfg.value_clip_eps)\n",
    "            v_loss1 = (values - returns) ** 2\n",
    "            v_loss2 = (v_clipped - returns) ** 2\n",
    "            value_loss = 0.5 * torch.max(v_loss1, v_loss2)\n",
    "            value_loss = (value_loss * mask).sum() / (mask.sum() + 1e-8)\n",
    "\n",
    "            # 熵奖励（越大越好，这里用负号并入loss）\n",
    "            with torch.no_grad():\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            entropy = -(probs * log_probs).sum(dim=-1)          # [B,T]\n",
    "            entropy_loss = -(entropy * mask).sum() / (mask.sum() + 1e-8)\n",
    "\n",
    "            loss = policy_loss + cfg.c1_value * value_loss + cfg.c2_entropy * entropy_loss\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "            optim.step()\n",
    "\n",
    "    if step % cfg.print_every == 1:\n",
    "        with torch.no_grad():\n",
    "            # 监控一下 KL（经验上监控旧→新）\n",
    "            kl = (mb[\"logp_old\"].to(cfg.device) - logp_new).mean().item()\n",
    "            avg_ret = rewards.mean().item()\n",
    "            print(f\"[Step {step}] loss={loss.item():.4f}  policy={policy_loss.item():.4f}  \"\n",
    "                  f\"value={value_loss.item():.4f}  entropy={-entropy_loss.item():.4f}  \"\n",
    "                  f\"KL={kl:.4f}  avg_reward={avg_ret:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089692f0",
   "metadata": {},
   "source": [
    "### 训练结果观察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e72b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sampling BEFORE training ===\n",
      "\n",
      "[Before] Prompt: Write a short product review:\n",
      "Write a short product review: https://www.groupon.org/medicine/apparel\n",
      "\n",
      "Donate to Planned Parenthood - http://\n",
      "\n",
      "\n",
      "[Before] Prompt: Describe your day:\n",
      "Describe your day:\n",
      "\n",
      "Monday, September 13\n",
      "\n",
      "The first thing you should know is that, without an actual work day in the\n",
      "\n",
      "\n",
      "=== Sampling AFTER training ===\n",
      "\n",
      "[Before] Prompt: Write a short product review:\n",
      "Write a short product review:\n",
      "\n",
      "Review a short product review:\n",
      "\n",
      "Write a short product review:\n",
      "\n",
      "Write a short product review:\n",
      "\n",
      "\n",
      "[Before] Prompt: Describe your day:\n",
      "Describe your day: In my opinion, this is a recipe for good wine.  It tastes like nothing but sherry.\n",
      "So far\n",
      "\n",
      "\n",
      "[Before] Prompt: Tell me something about your favorite movie:\n",
      "Tell me something about your favorite movie: One where you have children, and you play in an army?\n",
      "\n",
      "Advertisement\n",
      "\n",
      "A: This was a great\n",
      "\n",
      "\n",
      "[Before] Prompt: How was your meal?\n",
      "How was your meal?\n",
      "\n",
      "We ate from lunch with friends in Houston. We got to know each other better than most and I think we\n",
      "\n",
      "[Final Demo]\n",
      "Write a short product review:\n",
      "\n",
      "Review:\n",
      "\n",
      "Review: *What can I add to this place? *Please send me feedback through the\n"
     ]
    }
   ],
   "source": [
    "# ===================== 训练前：观察一次生成 =====================\n",
    "def sample_text(prompts: List[str], num=2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for p in prompts[:num]:\n",
    "            enc = tokenizer(p, return_tensors=\"pt\").to(cfg.device)\n",
    "            out_ids, _ = model.generate(\n",
    "                enc.input_ids, enc.attention_mask,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                temperature=cfg.temperature,\n",
    "                top_k=cfg.top_k,\n",
    "                eos_token_id=cfg.eos_token_id,\n",
    "            )\n",
    "            text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "            print(f\"\\n[Before] Prompt: {p}\\n{text}\\n\")\n",
    "    model.train()\n",
    "\n",
    "print(\"=== Sampling BEFORE training ===\")\n",
    "sample_text(cfg.prompts, num=2)\n",
    "\n",
    "\n",
    "# ===================== 训练后：再次采样对比 =====================\n",
    "print(\"\\n=== Sampling AFTER training ===\")\n",
    "sample_text(cfg.prompts, num=4)\n",
    "\n",
    "# 单条演示：看看奖励词是否更常出现\n",
    "demo_prompt = \"Write a short product review:\"\n",
    "enc = tokenizer(demo_prompt, return_tensors=\"pt\").to(cfg.device)\n",
    "out_ids, _ = model.generate(\n",
    "    enc.input_ids, enc.attention_mask,\n",
    "    max_new_tokens=cfg.max_new_tokens,\n",
    "    temperature=cfg.temperature,\n",
    "    top_k=cfg.top_k,\n",
    "    eos_token_id=cfg.eos_token_id,\n",
    ")\n",
    "print(\"[Final Demo]\")\n",
    "print(tokenizer.decode(out_ids[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
