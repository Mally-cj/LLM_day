{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033bc59c",
   "metadata": {},
   "source": [
    "## 题目\n",
    "给定模型和数据集，训练模型，使得其续写结果文艺"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a466d03",
   "metadata": {},
   "source": [
    "### 模型和数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2eaa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llamafactory/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用训练样本数: 100\n",
      "使用设备: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "# 加载模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "# 加载数据集（只使用100条）\n",
    "ds = load_dataset(\"Million/Chinese-Poems\")['train'].select(range(100))\n",
    "print(f\"使用训练样本数: {len(ds)}\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 创建优化器\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc1edc",
   "metadata": {},
   "source": [
    "### 数据集准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958a13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "def preprocess_function(examples):\n",
    "    texts = [f\"指令: {inst}\\n要求: {inp}\\n诗词: {out}\" \n",
    "             for inst, inp, out in zip(examples['instruction'], examples['input'], examples['output'])]\n",
    "    \n",
    "    tokenized = tokenizer(texts, max_length=256, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    # 只对诗词部分计算损失\n",
    "    labels = tokenized[\"input_ids\"].clone()\n",
    "    for i, text in enumerate(texts):\n",
    "        poetry_start = text.find(\"诗词:\") + 3\n",
    "        prefix_tokens = tokenizer.encode(text[:poetry_start], add_special_tokens=False)\n",
    "        labels[i, :len(prefix_tokens)+1] = -100\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# 应用预处理并创建DataLoader\n",
    "tokenized_ds = ds.map(preprocess_function, batched=True, remove_columns=ds.column_names)\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, data): self.data = data\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(item['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(item['labels'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 创建Dataset和DataLoader\n",
    "poetry_dataset = PoetryDataset(tokenized_ds)\n",
    "dataloader = DataLoader(poetry_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ca901",
   "metadata": {},
   "source": [
    "### 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0277b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Batch 1 详细过程 ===\n",
      "1. 输入数据形状:\n",
      "   input_ids: torch.Size([4, 256])\n",
      "   attention_mask: torch.Size([4, 256])\n",
      "   labels: torch.Size([4, 256])\n",
      "\n",
      "2. 前向传播过程:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   模型输出logits形状: torch.Size([4, 256, 50257])\n",
      "   自动计算的损失: 4.1500\n",
      "\n",
      "3. 手动损失计算:\n",
      "   展平后的logits形状: torch.Size([1024, 50257])\n",
      "   展平后的labels形状: torch.Size([1024])\n",
      "   有效位置数量: 376/1024\n",
      "> \u001b[0;32m/var/folders/h_/1gmxtvlj1x1d0hmgqrxtbt0r0000gn/T/ipykernel_73639/707460111.py\u001b[0m(52)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     50 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m        \u001b[0;31m# 手动计算交叉熵\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 52 \u001b[0;31m        \u001b[0mmanual_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mvalid_log_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     53 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     54 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   手动计算的损失: {manual_loss.item():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "   手动计算的损失: 9.5976\n",
      "   损失差异: 5.447605\n",
      "\n",
      "4. 反向传播过程:\n",
      "   总梯度范数: 26.8560\n",
      "   有梯度的参数数量: 148\n",
      "\n",
      "5. 参数更新过程:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   平均参数变化: 0.023281\n",
      "\n",
      "6. 训练效果分析:\n",
      "   当前batch损失: 4.1500\n",
      "   模型学会了什么:\n",
      "   - 根据指令和要求生成诗词\n",
      "   - 忽略前缀部分的重复模式\n",
      "   - 专注于内容生成质量\n",
      "\n",
      "=== Batch 2 详细过程 ===\n",
      "1. 输入数据形状:\n",
      "   input_ids: torch.Size([4, 256])\n",
      "   attention_mask: torch.Size([4, 256])\n",
      "   labels: torch.Size([4, 256])\n",
      "\n",
      "2. 前向传播过程:\n",
      "   模型输出logits形状: torch.Size([4, 256, 50257])\n",
      "   自动计算的损失: 3.8208\n",
      "\n",
      "3. 手动损失计算:\n",
      "   展平后的logits形状: torch.Size([1024, 50257])\n",
      "   展平后的labels形状: torch.Size([1024])\n",
      "   有效位置数量: 316/1024\n",
      "> \u001b[0;32m/var/folders/h_/1gmxtvlj1x1d0hmgqrxtbt0r0000gn/T/ipykernel_73639/707460111.py\u001b[0m(52)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     50 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m        \u001b[0;31m# 手动计算交叉熵\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 52 \u001b[0;31m        \u001b[0mmanual_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mvalid_log_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     53 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     54 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   手动计算的损失: {manual_loss.item():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "   手动计算的损失: 9.1342\n",
      "   损失差异: 5.313426\n",
      "\n",
      "4. 反向传播过程:\n",
      "   总梯度范数: 18.7012\n",
      "   有梯度的参数数量: 148\n",
      "\n",
      "5. 参数更新过程:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   平均参数变化: 0.017086\n",
      "\n",
      "=== Batch 3 详细过程 ===\n",
      "1. 输入数据形状:\n",
      "   input_ids: torch.Size([4, 256])\n",
      "   attention_mask: torch.Size([4, 256])\n",
      "   labels: torch.Size([4, 256])\n",
      "\n",
      "2. 前向传播过程:\n",
      "   模型输出logits形状: torch.Size([4, 256, 50257])\n",
      "   自动计算的损失: 3.5956\n",
      "\n",
      "3. 手动损失计算:\n",
      "   展平后的logits形状: torch.Size([1024, 50257])\n",
      "   展平后的labels形状: torch.Size([1024])\n",
      "   有效位置数量: 340/1024\n",
      "> \u001b[0;32m/var/folders/h_/1gmxtvlj1x1d0hmgqrxtbt0r0000gn/T/ipykernel_73639/707460111.py\u001b[0m(52)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     50 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m        \u001b[0;31m# 手动计算交叉熵\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 52 \u001b[0;31m        \u001b[0mmanual_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mvalid_log_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     53 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     54 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   手动计算的损失: {manual_loss.item():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [10:44:13<123:28:40, 19326.97s/it]\n",
      "  0%|          | 0/3 [10:44:13<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 训练循环 - 底层实现详解\n",
    "model.train()\n",
    "for epoch in tqdm(range(3)):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader)):\n",
    "        print(f\"\\n=== Batch {batch_idx + 1} 详细过程 ===\")\n",
    "        \n",
    "        # 1. 数据准备 - 底层张量操作\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        print(f\"1. 输入数据形状:\")\n",
    "        print(f\"   input_ids: {inputs['input_ids'].shape}\")\n",
    "        print(f\"   attention_mask: {inputs['attention_mask'].shape}\")\n",
    "        print(f\"   labels: {labels.shape}\")\n",
    "        \n",
    "        # 2. 前向传播 - 手动计算过程\n",
    "        print(f\"\\n2. 前向传播过程:\")\n",
    "        \n",
    "        # 2.1 获取模型输出\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        logits = outputs.logits  # [batch_size, seq_len, vocab_size]\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        print(f\"   模型输出logits形状: {logits.shape}\")\n",
    "        print(f\"   自动计算的损失: {loss.item():.4f}\")\n",
    "        \n",
    "        # 2.2 手动计算损失 - 底层实现\n",
    "        print(f\"\\n3. 手动损失计算:\")\n",
    "        \n",
    "        # 获取预测概率分布\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        logits_flat = logits.view(-1, vocab_size)  # [batch_size * seq_len, vocab_size]\n",
    "        labels_flat = labels.view(-1)  # [batch_size * seq_len]\n",
    "        \n",
    "        print(f\"   展平后的logits形状: {logits_flat.shape}\")\n",
    "        print(f\"   展平后的labels形状: {labels_flat.shape}\")\n",
    "        \n",
    "        # 计算交叉熵损失\n",
    "        log_probs = torch.log_softmax(logits_flat, dim=-1)\n",
    "        \n",
    "        # 创建掩码：忽略-100的位置\n",
    "        mask = (labels_flat != -100)\n",
    "        print(f\"   有效位置数量: {mask.sum().item()}/{len(labels_flat)}\")\n",
    "        \n",
    "        # 只对有效位置计算损失\n",
    "        valid_log_probs = log_probs[mask]\n",
    "        valid_labels = labels_flat[mask]\n",
    "        \n",
    "        pdb.set_trace()\n",
    "        # 手动计算交叉熵\n",
    "        manual_loss = -valid_log_probs[range(len(valid_labels)), valid_labels].mean()\n",
    "        \n",
    "        print(f\"   手动计算的损失: {manual_loss.item():.4f}\")\n",
    "        print(f\"   损失差异: {abs(loss.item() - manual_loss.item()):.6f}\")\n",
    "        \n",
    "        # 3. 反向传播 - 梯度计算过程\n",
    "        print(f\"\\n4. 反向传播过程:\")\n",
    "        \n",
    "        # 清零梯度\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 计算损失\n",
    "        loss.backward()\n",
    "        \n",
    "        # 检查梯度\n",
    "        total_grad_norm = 0\n",
    "        param_count = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.data.norm(2)\n",
    "                total_grad_norm += grad_norm.item() ** 2\n",
    "                param_count += 1\n",
    "        \n",
    "        total_grad_norm = total_grad_norm ** 0.5\n",
    "        print(f\"   总梯度范数: {total_grad_norm:.4f}\")\n",
    "        print(f\"   有梯度的参数数量: {param_count}\")\n",
    "        \n",
    "        # 4. 参数更新 - 优化器内部过程\n",
    "        print(f\"\\n5. 参数更新过程:\")\n",
    "        \n",
    "        # 获取更新前的参数\n",
    "        old_params = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                old_params[name] = param.data.clone()\n",
    "        \n",
    "        # 执行优化器步骤\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 计算参数变化\n",
    "        param_changes = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in old_params:\n",
    "                change = (param.data - old_params[name]).norm().item()\n",
    "                param_changes.append(change)\n",
    "        \n",
    "        avg_param_change = sum(param_changes) / len(param_changes) if param_changes else 0\n",
    "        print(f\"   平均参数变化: {avg_param_change:.6f}\")\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 只详细打印第一个batch\n",
    "        if batch_idx == 0:\n",
    "            print(f\"\\n6. 训练效果分析:\")\n",
    "            print(f\"   当前batch损失: {loss.item():.4f}\")\n",
    "            print(f\"   模型学会了什么:\")\n",
    "            print(f\"   - 根据指令和要求生成诗词\")\n",
    "            print(f\"   - 忽略前缀部分的重复模式\")\n",
    "            print(f\"   - 专注于内容生成质量\")\n",
    "        \n",
    "        if batch_idx >= 2:  # 只详细分析前3个batch\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} 完成, 平均损失: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained(\"./poetry_model_small\")\n",
    "tokenizer.save_pretrained(\"./poetry_model_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c39c338",
   "metadata": {},
   "source": [
    "### 推理生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a46376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 诗歌生成函数 - 使用 forward 方式\n",
    "def generate_poetry(instruction, input_text, max_length=200, temperature=0.8):\n",
    "    \"\"\"使用 forward 方式生成诗歌\"\"\"\n",
    "    prompt = f\"指令: {instruction}\\n要求: {input_text}\\n诗词:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    generated_tokens = inputs['input_ids'].clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - inputs['input_ids'].shape[1]):\n",
    "            # 前向传播\n",
    "            outputs = model.forward(\n",
    "                input_ids=generated_tokens,\n",
    "                attention_mask=torch.ones_like(generated_tokens).to(device)\n",
    "            )\n",
    "            \n",
    "            # 获取下一个token的logits\n",
    "            next_token_logits = outputs.logits[:, -1, :] / temperature\n",
    "            \n",
    "            # 应用top-p采样\n",
    "            if temperature > 0:\n",
    "                # 计算累积概率\n",
    "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                # 找到top-p阈值\n",
    "                sorted_indices_to_remove = cumulative_probs > 0.9\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                \n",
    "                # 移除低概率token\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                next_token_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # 采样下一个token\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n",
    "            \n",
    "            # 如果生成了结束token，停止生成\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    poetry = generated_text.split(\"诗词:\")[-1].strip()\n",
    "    return poetry\n",
    "\n",
    "# 测试生成\n",
    "print(\"\\n=== 诗歌生成测试 ===\")\n",
    "test_cases = [\n",
    "    (\"你是一个诗词创作的AI助手\", \"请你创作一首关于春天的诗\"),\n",
    "    (\"你是一个诗词创作的AI助手\", \"请你写一首表达思乡之情的诗\"),\n",
    "]\n",
    "\n",
    "for i, (instruction, input_text) in enumerate(test_cases, 1):\n",
    "    print(f\"\\n测试 {i}: {input_text}\")\n",
    "    try:\n",
    "        poetry = generate_poetry(instruction, input_text)\n",
    "        print(f\"生成: {poetry}\")\n",
    "    except Exception as e:\n",
    "        print(f\"生成失败: {e}\")\n",
    "\n",
    "print(\"\\n训练完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamafactory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
