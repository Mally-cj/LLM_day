


# LLM-Agent 工作流：分类、执行模式与能力提升路径

> 2025-11-30 16:23:21 Sunday 创建


## 按工作流划分 LLM-Agent
从工程视角，LLM-Agent 可以按 **工作流结构（workflow）** 粗分为两类：


### 1. 静态工作流（Static Workflow / Predefined Pipeline）

**定义：**
工作流在运行前就被 **完全确定**，实际运行时只沿着既定的步骤往前走。LLM 只是在这些固定节点上生成内容或调用工具，**中途不会改变流程结构**。

**特征：**

* 执行路径是预定义的链式或 DAG 结构
* 中间结果只影响“本步内容”，不影响“后续走哪一步”
* 更像“带 LLM 的数据流水线 / 程序”，而不是自主 agent

**典型场景特征：**

* 任务流程非常稳定，步骤清晰，**不需要根据环境动态决策**
* 更关心“每一步输出质量”，不关心“下一步选啥”

**示例：**

* 标准 RAG：检索 → 重写 query → 再检索 → 生成回答
* 固定工具链：解析 → 调用数据库查询 → 汇总结果 → 生成报告


### 2. 动态工作流（Dynamic Workflow / Action Workflow）

**定义：**
Agent 在执行过程中不断经历 **Observation → Decide → Act**，每一步的观察结果都会影响“接下来做什么”，**工作流结构本身是运行时动态生成的**。

**特征：**

* 每一步都读取当前 observation 决定下一步动作
* 执行轨迹可能分叉、回滚、提前终止
* 具有典型的 sequential decision-making 特征

**典型场景特征：**

* 环境不确定或信息不完全，需要 **边看边决策**
* 同一任务在不同环境下执行路径可能完全不同
* 用户希望 agent 自主探索、搜索、规划，而不是只跑固定脚本

**示例：**

* Web 研究 / 浏览器代理：根据搜索结果决定继续检索、改写 query 或开始写总结
* 复杂工具调用：先查 A 接口，如果失败再退回换 B 接口或改写需求


## 常见执行模式（均属于动态工作流）

下面这些是当前文献和工程里常被提到的几种 **执行模式**，都可以视为“动态工作流”的不同 instantiation。

### 1. ReAct 循环（Reason + Act）

* 核心流程：
  **Thought → Action → Observation → 下一步 Thought …**
* 不预先写出完整计划，而是每一步“先想再做”，根据 observation 调整后续行为。
* **最早系统化提出：**
  *ReAct: Synergizing Reasoning and Acting in Language Models*，Yao et al., ICLR 2023。


### 2. Planning / Plan-and-Execute

* 核心流程：
  先由模型生成一个高层计划（Plan），再逐步执行各步骤，必要时重规划。
* 强调：**显式计划表示 + 线性或分段执行**。
* **早期代表论文：**
  *Plan-and-Solve: Large Language Models are Zero-Shot Planners*（2023） 等。


### 3. 搜索式推理（Search-based / Tree-of-Thoughts）

* 核心思想：
  不只沿一条思路往下走，而是 **同时展开多个 reasoning 分支**，再比较、筛选、汇总。
* 执行上表现为：

  * 生成多个候选思路 / 子计划
  * 用启发式或模型评分挑选好的分支
* **最早系统化提出：**
  *Tree of Thoughts: Deliberate Problem Solving with Large Language Models*，Yao et al., 2023。


### 4. 图结构推理（Graph-of-Thoughts）

* 将思考过程表示为 **图结构（节点是子结论 / 子任务，边是依赖关系）**，
  而不是简单的线性链或树。
* 更适合有复杂依赖的 reasoning / 多文档总结。
* **来源论文：**
  *Graph of Thoughts: Solving Elaborate Problems with Large Language Models*，Besta et al., 2023。


### 5. 反思 / 自我修正（Reflexion-style）

* Agent 在执行过程或失败之后，会 **显式生成自我反思（reflection）**，
  再用这段反思来指导后续行为。
* 本质是把“错误经验”编码进一个可读的 memory，再循环使用。
* **来源论文：**
  *Reflexion: Language Agents with Verbal Reinforcement Learning*，Shinn et al., 2023。


### 6. 多智能体协作（Multi-Agent Workflow）

* 不再只用一个 agent，而是多个角色协作：
  比如 Planner / Critic / Executor、专家组、辩论双方等。
* 工作流由 **多 agent 之间的消息传递** 决定，是一种更高层的动态工作流。
* **早期代表：**
  *CAMEL: Communicative Agents for “Mind” Exploration*，Li et al., 2023。


## 提升 LLM-Agent 能力的两条主线

从抽象层面看，提升 agent 能力大致有两条 **本质不同** 的思路：


### 1. 改模型（Training-level）：把好策略“写进参数”

**本质：**
通过微调 / 强化学习，让模型**内生地学会更好的策略**：
包括如何规划、如何使用工具、如何搜索，而不是完全依赖 prompt 和外部控制。

* 优点：

  * 推理时更稳定，少 prompt 工程
  * 同样的 prompt 下表现更强
* 示例：

  * 用专家轨迹做监督微调，让模型学会 ReAct 或 Planner 的行为模式
  * 用 PPO / GRPO / RLAIF 在真实环境中优化“任务完成率”和“搜索效率”


### 2. 改推理（Inference-level）：在不改参数的前提下，换工作流和上下文

**本质：**
保持 base LLM 不变，通过 **更好的工作流设计 + 上下文结构**，
在推理阶段“放大”模型已有能力。

* 典型做法的共同点：

  * 用更好的 prompt 模板引导推理（例如 ReAct、ToT prompt）
  * 用 memory / 检索 / 工具调用扩展模型的输入信息
  * 用 planner-executor、多 agent、搜索式 workflow 在外部组织决策
* 示例：

  * 同一个基础模型，切换成 ReAct / Plan-and-Execute / ToT prompt，性能显著差异
  * 加一个检索 + memory 层，就能从“纯聊天”变成“research agent”


