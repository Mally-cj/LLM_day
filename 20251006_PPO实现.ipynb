{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b35e2b",
   "metadata": {},
   "source": [
    "# PPOçš„ä»£ç å®ç°\n",
    "ä»£ç æ¥è‡ªäºï¼Œhttps://github.com/nikhilbarhate99/PPO-PyTorch\n",
    "\n",
    "æ€»ä½“æµç¨‹ä¸ºï¼š\n",
    "\n",
    "    ç¯å¢ƒäº¤äº’:\n",
    "        â†“\n",
    "    PPO.select_action(state)\n",
    "        â†“\n",
    "    ç¯å¢ƒè¿”å› reward, done\n",
    "        â†“\n",
    "    æ•°æ®å­˜å…¥ RolloutBuffer\n",
    "        â†“\n",
    "    æ¯ N æ­¥å:\n",
    "        â†“\n",
    "    PPO.update()\n",
    "        â”œâ”€ è®¡ç®— G_t å’Œä¼˜åŠ¿ A_t\n",
    "        â”œâ”€ è®¡ç®— ratio = Ï€/Ï€_old\n",
    "        â”œâ”€ ä¼˜åŒ– min(surr1, surr2)\n",
    "        â”œâ”€ æ›´æ–° Critic\n",
    "        â””â”€ åŒæ­¥ policy_old â† policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec02671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "################################## set device ##################################\n",
    "print(\"============================================================================================\")\n",
    "# set device to cpu or cuda\n",
    "device = torch.device('cpu')\n",
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "\n",
    "################################## PPO Policy ##################################\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "        \n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "        # actor\n",
    "        if has_continuous_action_space :\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Tanh()\n",
    "                        )\n",
    "        else:\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 1)\n",
    "                    )\n",
    "        \n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def act(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_val = self.critic(state)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            \n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var).to(device)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "            \n",
    "            # For Single Action Environments.\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    def select_action(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "            \n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss  \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "   \n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        \n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abba93",
   "metadata": {},
   "source": [
    "## PPO ä»»åŠ¡ç¤ºä¾‹\n",
    "\n",
    "æˆ‘ä»¬è®© GPT-2 å­¦ä¼šç”Ÿæˆâ€œæ­£å‘æƒ…ç»ªâ€çš„å¥å­ã€‚\n",
    "ç³»ç»Ÿæä¾›ä¸€äº›ç®€å•çš„ **promptï¼ˆæç¤ºè¯­ï¼‰**ï¼Œä¾‹å¦‚ï¼š\n",
    "\n",
    "```text\n",
    "\"Write a short product review:\"\n",
    "\"Describe your day:\"\n",
    "\"Give quick feedback on this service:\"\n",
    "```\n",
    "\n",
    "æ¨¡å‹ç”Ÿæˆä¸€æ®µç»­å†™æ–‡æœ¬ã€‚\n",
    "ç„¶åæˆ‘ä»¬æ ¹æ®ç”Ÿæˆæ–‡æœ¬ä¸­çš„**æƒ…æ„Ÿè¯æ±‡**ç»™å‡ºå¥–åŠ±ï¼š\n",
    "\n",
    "| å¥–åŠ±è§„åˆ™                                                        | ç¤ºä¾‹   |\n",
    "| ----------------------------------------------------------- | ---- |\n",
    "| è‹¥æ–‡æœ¬ä¸­åŒ…å« `\"good\"`, `\"great\"`, `\"excellent\"`, `\"wonderful\"` ç­‰è¯ | +1.0 |\n",
    "| å¦åˆ™                                                          | âˆ’0.5 |\n",
    "\n",
    "å› æ­¤æ¨¡å‹ä¼šé€æ¸å­¦ä¼šåå‘ä½¿ç”¨è¿™äº›ç§¯æè¯æ±‡ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926fa26c",
   "metadata": {},
   "source": [
    "### âš™ï¸ æ¨¡å‹ç»“æ„ï¼ˆArchitectureï¼‰\n",
    "\n",
    "æ•´ä½“ä½¿ç”¨ä¸€ä¸ª **Actorâ€“Critic** æ¡†æ¶ï¼š\n",
    "\n",
    "```\n",
    "        +-----------------------------+\n",
    "        |        GPT-2 backbone       |\n",
    "        +-----------------------------+\n",
    "              â†™                   â†˜\n",
    "        Policy (actor)         Value (critic)\n",
    "         â†’ è¾“å‡º logits          â†’ çº¿æ€§å±‚é¢„æµ‹ V(s)\n",
    "```\n",
    "\n",
    "* **Actor**ï¼šGPT-2 çš„è¯­è¨€å»ºæ¨¡å¤´ï¼Œè¾“å‡ºæ¯ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒã€‚\n",
    "* **Critic**ï¼šåœ¨æœ€åä¸€å±‚ hidden ä¸ŠåŠ çº¿æ€§å±‚ï¼Œé¢„æµ‹çŠ¶æ€ä»·å€¼ (V(s_t))ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§© PPO ç®—æ³•è¦ç‚¹\n",
    "\n",
    "1. **Ratio (é‡è¦æ€§é‡‡æ ·æ¯”ç‡)**\n",
    "   $$r_t = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\text{old}}(a_t|s_t)}$$\n",
    "   \n",
    "\n",
    "2. **å‰ªåˆ‡ç›®æ ‡ (Clipped Surrogate Objective)**\n",
    "   \n",
    "   $$L^{clip} = \\min\\big(r_t A_t,, \\text{clip}(r_t,1-\\epsilon,1+\\epsilon) A_t\\big)$$\n",
    "   \n",
    "\n",
    "3. **Critic Loss (å€¼å‡½æ•°è¯¯å·®)**\n",
    "   \n",
    "   $$L^{V} = (V_\\phi(s_t)-R_t)^2$$\n",
    "\n",
    "4. **Entropy Bonus (ç†µæ­£åˆ™)**\n",
    "   é¼“åŠ±ç­–ç•¥ä¿æŒå¤šæ ·æ€§ã€‚\n",
    "\n",
    "5. **æœ€ç»ˆç›®æ ‡ï¼ˆè¦æœ€å¤§åŒ–ï¼‰**\n",
    "   \n",
    "   $$L = \\mathbb{E}[L^{clip} - c_1 L^{V} + c_2 \\mathcal{H}]$$\n",
    "   å®ç°æ—¶å–è´Ÿå·ä½œä¸º loss è¿›è¡Œæœ€å°åŒ–ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§® ä¼˜åŠ¿ä¸å›æŠ¥è®¡ç®—\n",
    "\n",
    "ä½¿ç”¨ GAE(Î»)ï¼ˆå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼‰ï¼š\n",
    "\n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "$$A_t = \\sum_l (\\gamma \\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "\n",
    "$$R_t = A_t + V(s_t)$$\n",
    "\n",
    "åœ¨è¿™ä¸ª demo ä¸­ï¼š\n",
    "\n",
    "* æ¯ä¸ªç”Ÿæˆåºåˆ—åªæœ‰**ä¸€ä¸ªç»ˆå±€å¥–åŠ±**ï¼›\n",
    "* å¥–åŠ±åˆ†é…åˆ°æœ€åä¸€ä¸ª tokenï¼Œå¹¶é€šè¿‡ GAE åå‘ä¼ æ’­ï¼›\n",
    "* éç”Ÿæˆéƒ¨åˆ†ï¼ˆpromptï¼‰mask æ‰ï¼Œä¸å‚ä¸æ›´æ–°ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§° è®­ç»ƒæµç¨‹\n",
    "\n",
    "1. **Rollout æ”¶é›†æ•°æ®**\n",
    "\n",
    "   * ç”¨å½“å‰ç­–ç•¥ GPT-2 ç”Ÿæˆæ–‡æœ¬ã€‚\n",
    "   * è®°å½•ï¼šinput_ids, attention_mask, logits, value, logp_oldã€‚\n",
    "   * è®¡ç®—å¥–åŠ±ï¼ˆæ­£å‘è¯æ£€æµ‹ï¼‰ã€‚\n",
    "\n",
    "2. **è®¡ç®—ä¼˜åŠ¿ä¸å›æŠ¥**\n",
    "\n",
    "   * ç”¨ critic çš„å€¼ä¼°è®¡ç»“åˆ reward ç®— GAEã€‚\n",
    "   * ç”Ÿæˆ `advantages` å’Œ `returns`ã€‚\n",
    "\n",
    "3. **PPO æ›´æ–°**\n",
    "\n",
    "   * å¤šæ¬¡å°æ‰¹é‡ï¼ˆminibatchï¼‰è®­ç»ƒã€‚\n",
    "   * è®¡ç®—æ–°ç­–ç•¥ logpï¼Œä¸æ—§ç­–ç•¥æ¯” ratioã€‚\n",
    "   * è¿›è¡Œ **å‰ªåˆ‡ç­–ç•¥ç›®æ ‡** + **value loss** + **entropy** ä¼˜åŒ–ã€‚\n",
    "   * ä½¿ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢å‘æ•£ã€‚\n",
    "\n",
    "4. **ç›‘æ§æŒ‡æ ‡**\n",
    "\n",
    "   * loss ç»„ä»¶ï¼ˆpolicy / value / entropyï¼‰\n",
    "   * KL æ•£åº¦ï¼ˆé˜²æ­¢ç­–ç•¥æ¼‚ç§»ï¼‰\n",
    "   * å¹³å‡å¥–åŠ±å˜åŒ–\n",
    "\n",
    "---\n",
    "\n",
    "### æ•°æ®æ•´ä½“æµåŠ¨é€»è¾‘æ€»ç»“\n",
    "prompt\n",
    "  â†“\n",
    "ç”Ÿæˆ token_1, token_2, ... token_T\n",
    "  â†“\n",
    "critic ç»™å‡ºæ¯æ­¥ V(s_t)\n",
    "  â†“\n",
    "reward ä»…åœ¨æœ€å token ç»™å‡º (+1 æˆ– -0.5)\n",
    "  â†“\n",
    "GAE æ ¹æ® V å’Œ r_t æŠŠâ€œæœªæ¥å¥½å¤„â€å¾€å‰ä¼ \n",
    "  â†“\n",
    "å¾—åˆ°æ¯æ­¥ä¼˜åŠ¿ A_t\n",
    "  â†“\n",
    "policy_loss ç”¨ A_t æ›´æ–° actorï¼ˆGPT2 logitsï¼‰\n",
    "value_loss ç”¨ (R_t - V(s_t))^2 æ›´æ–° critic\n",
    "\n",
    "\n",
    "### ğŸ“Š ä»£ç ç»“æ„æ¦‚è§ˆ\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ CFG                # å‚æ•°ä¸ä»»åŠ¡é…ç½®\n",
    "â”œâ”€â”€ ActorCritic        # GPT2 + Value Head æ¨¡å‹\n",
    "â”œâ”€â”€ reward_fn()        # ç®€æ˜“å¥–åŠ±å‡½æ•°\n",
    "â”œâ”€â”€ compute_logprobs() # tokençº§logæ¦‚ç‡\n",
    "â”œâ”€â”€ compute_returns_advantages() # GAE\n",
    "â”œâ”€â”€ iterate_minibatches()        # å°æ‰¹é‡è¿­ä»£å™¨\n",
    "â”‚\n",
    "â”œâ”€â”€ rollout & PPO loop           # ä¸»è®­ç»ƒå¾ªç¯\n",
    "â”‚\n",
    "â””â”€â”€ sample_text()                # å‰åç”Ÿæˆå¯¹æ¯”\n",
    "```\n",
    "\n",
    "---\n",
    "t2_minimal_demo.py\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ§­ ç»“æœç¤ºä¾‹ï¼ˆç¤ºæ„ï¼‰\n",
    "\n",
    "```\n",
    "=== Sampling BEFORE training ===\n",
    "Prompt: Write a short product review:\n",
    "Write a short product review: It was a terrible day, the food was cold...\n",
    "\n",
    "=== Sampling AFTER training ===\n",
    "Prompt: Write a short product review:\n",
    "Write a short product review: The service was great and the food was excellent!\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b2785",
   "metadata": {},
   "source": [
    "### å‚æ•°ä¸ä»»åŠ¡é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo_gpt2_minimal_demo.py\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast as GPT2Tokenizer\n",
    "\n",
    "# ===================== é…ç½® =====================\n",
    "@dataclass\n",
    "class CFG:\n",
    "    model_name: str = \"gpt2\"\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 42\n",
    "\n",
    "    # rollout\n",
    "    prompts: List[str] = (\n",
    "        \"Write a short product review:\",\n",
    "        \"Describe your day:\",\n",
    "        \"Tell me something about your favorite movie:\",\n",
    "        \"How was your meal?\",\n",
    "        \"Give quick feedback on this service:\",\n",
    "    )\n",
    "    max_new_tokens: int = 24\n",
    "    eos_token_id: int = None   # è‡ªåŠ¨ä»tokenizeré‡Œæ‹¿\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 50\n",
    "\n",
    "    # PPO\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_eps: float = 0.2\n",
    "    value_clip_eps: float = 0.2\n",
    "    c1_value: float = 0.5\n",
    "    c2_entropy: float = 0.01\n",
    "    lr: float = 1e-5\n",
    "    max_grad_norm: float = 0.5\n",
    "\n",
    "    # è®­ç»ƒ\n",
    "    rollout_batch_size: int = 6     # æ¯è½®æ”¶é›†å¤šå°‘æ¡åºåˆ—\n",
    "    ppo_epochs: int = 3             # å¯¹åŒä¸€æ‰¹æ•°æ®åšå‡ æ¬¡å°æ‰¹é‡æ›´æ–°\n",
    "    minibatch_size: int = 3\n",
    "    train_steps: int = 5            # æ€»å…±åšå¤šå°‘ä¸ª PPO outer stepsï¼ˆæ¼”ç¤ºï¼šå¾ˆå°ï¼‰\n",
    "    print_every: int = 1\n",
    "\n",
    "    # ç®€å•å¥–åŠ±è¯è¡¨ï¼ˆå‘½ä¸­åŠ åˆ†ï¼‰\n",
    "    pos_words: Tuple[str,...] = (\"good\", \"great\", \"excellent\", \"wonderful\")\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# ===================== éšæœºç§å­ =====================\n",
    "random.seed(cfg.seed)\n",
    "torch.manual_seed(cfg.seed)\n",
    "torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "# ===================== æ¨¡å‹ & tokenizer =====================\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(cfg.model_name)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "cfg.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b98231",
   "metadata": {},
   "source": [
    "### å¥–åŠ±å‡½æ•°å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea27e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€æ˜“å¥–åŠ±å‡½æ•°ï¼šç”Ÿæˆæ–‡æœ¬åŒ…å«æ­£å‘è¯åˆ™é«˜å¥–åŠ±ï¼Œå¦åˆ™ä½å¥–åŠ±\n",
    "def reward_fn(texts: List[str], pos_words: Tuple[str,...]) -> torch.Tensor:\n",
    "    rewards = []\n",
    "    for t in texts:\n",
    "        t_low = t.lower()\n",
    "        matched = any(w in t_low for w in pos_words)\n",
    "        rewards.append(1.0 if matched else -0.5)\n",
    "    return torch.tensor(rewards, dtype=torch.float32)\n",
    "\n",
    "# æŠŠåºåˆ—æœ«ç«¯å¥–åŠ±åˆ†é…åˆ°åŠ¨ä½œtokenä¸Šï¼Œå¹¶åš GAE\n",
    "def compute_returns_advantages(\n",
    "    rewards: torch.Tensor,              # [B] æ¯æ¡åºåˆ—ä¸€ä¸ªç»ˆå±€å¥–åŠ±\n",
    "    values: torch.Tensor,               # [B, T] æ¯ä¸ªä½ç½®çš„V(s_t)\n",
    "    dones: torch.Tensor,                # [B] ç»ˆæ­¢æ ‡å¿—ï¼ˆè¿™é‡Œå…¨1ï¼‰\n",
    "    action_mask: torch.Tensor,          # [B, T] åªå¯¹ç”Ÿæˆéƒ¨åˆ†ä¸º1ï¼Œpromptéƒ¨åˆ†ä¸º0\n",
    "    gamma: float,\n",
    "    lam: float\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # æˆ‘ä»¬è®©æ¯æ¡åºåˆ—çš„æœ€åä¸€ä¸ª action token æ¥æ”¶ rewardï¼Œå…¶å®ƒ action token ä¹Ÿèƒ½é€šè¿‡ GAE å›ä¼ \n",
    "    B, T = values.shape\n",
    "    device = values.device\n",
    "\n",
    "    # æ„å»º per-step rewardï¼šé»˜è®¤ä¸º0ï¼Œåªåœ¨æ¯æ¡åºåˆ—æœ€åä¸€ä¸ªæœ‰æ•ˆ action ä½ç½®æ”¾å…¥ç»ˆå±€å¥–åŠ±\n",
    "    step_rewards = torch.zeros_like(values)\n",
    "    last_idx = (action_mask.cumsum(dim=1) == action_mask.sum(dim=1, keepdim=True)).long().argmax(dim=1)  # æ¯æ¡åºåˆ—æœ€åä¸€ä¸ªactionç´¢å¼•\n",
    "    for b in range(B):\n",
    "        step_rewards[b, last_idx[b]] = rewards[b]\n",
    "\n",
    "    # è®¡ç®— deltas ä¸ GAEï¼Œä»…åœ¨ action_mask==1 çš„ä½ç½®æœ‰æ•ˆ\n",
    "    deltas = step_rewards + gamma * F.pad(values[:, 1:], (0,1)) * (1.0 - 0.0) - values\n",
    "    deltas = deltas * action_mask\n",
    "\n",
    "    advantages = torch.zeros_like(values)\n",
    "    last_gae = torch.zeros(B, device=device)\n",
    "    for t in reversed(range(T)):\n",
    "        mask_t = action_mask[:, t]\n",
    "        delta_t = deltas[:, t]\n",
    "        last_gae = delta_t + gamma * lam * last_gae\n",
    "        advantages[:, t] = last_gae * mask_t  # éactionä½ç½®ä¸º0\n",
    "\n",
    "        # åœ¨éactionä½ç½®ç»´æŒ last_gae ä¸è¢«æ— æ„ä¹‰åœ°ä¼ æ’­\n",
    "        last_gae = last_gae * (mask_t > 0).float() + last_gae * (mask_t == 0).float()\n",
    "\n",
    "    returns = advantages + values\n",
    "    return returns.detach(), advantages.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d50ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/trl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sampling BEFORE training ===\n",
      "\n",
      "[Before] Prompt: Write a short product review:\n",
      "Write a short product review:\n",
      "\n",
      "Product Reviews Write about the products of the following company: CXV, General Dynamics, Intel\n",
      "\n",
      "This\n",
      "\n",
      "\n",
      "[Before] Prompt: Describe your day:\n",
      "Describe your day: What was it like for you? A? B ? I. B a? Or C ? I ? E ? I\n",
      "\n",
      "\n",
      "=== Sampling AFTER training ===\n",
      "\n",
      "[Before] Prompt: Write a short product review:\n",
      "Write a short product review: www.goodreads.com/reviews/lulu-coco-coco-no...\n",
      "\n",
      "\n",
      "[Before] Prompt: Describe your day:\n",
      "Describe your day: what does the man know? What is that one thing?\n",
      "\n",
      "Do you feel like a man? What is life\n",
      "\n",
      "\n",
      "[Before] Prompt: Tell me something about your favorite movie:\n",
      "Tell me something about your favorite movie: \"I'm like a woman, you know.\" There's an intense intensity to the character, and if you're young\n",
      "\n",
      "\n",
      "[Before] Prompt: How was your meal?\n",
      "How was your meal? How many bottles are there? I've found to be more expensive than you're probably expecting.\n",
      "\n",
      "My first night\n",
      "\n",
      "[Final Demo]\n",
      "Write a short product review:\n",
      "\n",
      "You'll get no more time than you need to write and review reviews for every product.\n",
      "\n",
      "We offer\n"
     ]
    }
   ],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, base_model: GPT2LMHeadModel):\n",
    "        super().__init__()\n",
    "        self.actor = base_model\n",
    "        hidden = base_model.config.n_embd\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # ä¸ºäº†æ‹¿åˆ°hidden statesï¼Œæ‰“å¼€è¾“å‡º\n",
    "        out = self.actor(input_ids=input_ids,\n",
    "                         attention_mask=attention_mask,\n",
    "                         output_hidden_states=True,\n",
    "                         return_dict=True)\n",
    "        logits = out.logits                                  # [B, T, V]\n",
    "        last_hidden = out.hidden_states[-1]                  # [B, T, H]\n",
    "        values = self.value_head(last_hidden).squeeze(-1)    # [B, T]\n",
    "        return logits, values\n",
    "\n",
    "    def generate(self, input_ids, attention_mask, max_new_tokens, temperature=1.0, top_k=50, eos_token_id=None):\n",
    "        # æ‰‹å†™ samplingï¼ˆä¹Ÿå¯ç”¨ model.generateï¼Œè¿™é‡Œä¸ºäº†æ›´æ¸…æ™°æ‹¿logprobï¼‰\n",
    "        self.eval()\n",
    "        B = input_ids.size(0)\n",
    "        cur_input_ids = input_ids\n",
    "        cur_attn = attention_mask\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits, _ = self.forward(cur_input_ids, cur_attn)\n",
    "                next_logits = logits[:, -1, :] / max(1e-6, temperature)\n",
    "                if top_k is not None and top_k > 0:\n",
    "                    topv, topi = torch.topk(next_logits, top_k)\n",
    "                    mask = next_logits < topv[:, -1].unsqueeze(-1)\n",
    "                    next_logits = next_logits.masked_fill(mask, -float(\"inf\"))\n",
    "                probs = F.softmax(next_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)  # [B,1]\n",
    "                cur_input_ids = torch.cat([cur_input_ids, next_token], dim=1)\n",
    "                cur_attn = torch.ones_like(cur_input_ids, device=cur_input_ids.device)\n",
    "                if eos_token_id is not None:\n",
    "                    # å¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½äº§ç”Ÿäº†eosï¼Œæå‰ç»ˆæ­¢\n",
    "                    if (next_token.squeeze(-1) == eos_token_id).all():\n",
    "                        break\n",
    "        self.train()\n",
    "        return cur_input_ids, cur_attn\n",
    "\n",
    "# logprob å·¥å…·\n",
    "def compute_logprobs(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "    # logits: [B,T,V], labels: [B,T]ï¼Œè¿”å›æ¯ä¸ªä½ç½® token çš„ log p(token)\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    lp = logp.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)  # [B,T]\n",
    "    return lp\n",
    "\n",
    "\n",
    "# mini-batch è¿­ä»£å™¨\n",
    "def iterate_minibatches(batch: Dict[str, torch.Tensor], mb_size: int):\n",
    "    N = batch[\"input_ids\"].size(0)\n",
    "    idx = torch.randperm(N)\n",
    "    for start in range(0, N, mb_size):\n",
    "        mb_idx = idx[start : start + mb_size]\n",
    "        yield {k: v[mb_idx] for k, v in batch.items()}\n",
    "\n",
    "# ===================== æ„å»ºæ¨¡å‹ä¸ä¼˜åŒ–å™¨ =====================\n",
    "base = GPT2LMHeadModel.from_pretrained(cfg.model_name)\n",
    "model = ActorCritic(base).to(cfg.device)\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "\n",
    "\n",
    "# ===================== PPO è®­ç»ƒå¾ªç¯ =====================\n",
    "for step in range(1, cfg.train_steps + 1):\n",
    "    # -------- Rollout æ”¶é›† on-policy è½¨è¿¹ --------\n",
    "    batch_prompts = [random.choice(cfg.prompts) for _ in range(cfg.rollout_batch_size)]\n",
    "    enc = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True).to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # æ—§ç­–ç•¥ä¸‹é‡‡æ ·åºåˆ—\n",
    "        full_ids, full_attn = model.generate(\n",
    "            enc.input_ids, enc.attention_mask,\n",
    "            max_new_tokens=cfg.max_new_tokens,\n",
    "            temperature=cfg.temperature,\n",
    "            top_k=cfg.top_k,\n",
    "            eos_token_id=cfg.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # è®¡ç®—æ—§ç­–ç•¥çš„ logits/values å’Œ logp_old\n",
    "        logits_old, values_old = model(full_ids, attention_mask=full_attn)\n",
    "        # labels = ä¸‹ä¸€ä¸ªtokenï¼ˆè‡ªå›å½’ï¼‰ï¼Œç”¨ -100 å±è”½æœ€åä¸€ä¸ªä½ç½®\n",
    "        labels = full_ids[:, 1:].contiguous()\n",
    "        logits_old_trim = logits_old[:, :-1, :].contiguous()\n",
    "        values_old_trim = values_old[:, :-1].contiguous()\n",
    "        attn_trim = full_attn[:, :-1].contiguous()\n",
    "        logp_old = compute_logprobs(logits_old_trim, labels)\n",
    "\n",
    "    # æ„é€  action_maskï¼šåªå¯¹ç”Ÿæˆå‡ºçš„æ–°tokenéƒ¨åˆ†ä¸º1ï¼ˆå»æ‰prompt tokenéƒ¨åˆ†ï¼‰\n",
    "    prompt_len = (enc.attention_mask.sum(dim=1)).to(full_ids.device)  # [B]\n",
    "    T = labels.size(1)\n",
    "    action_mask = torch.zeros_like(values_old_trim, dtype=torch.float32)\n",
    "    for b in range(cfg.rollout_batch_size):\n",
    "        # å¯è®­ç»ƒçš„ token èŒƒå›´ï¼šä» prompt_len[b]-1 èµ·åˆ°æœ«å°¾ï¼ˆå› ä¸º labels å¯¹åº”çš„æ˜¯é¢„æµ‹ç¬¬ t+1 ä¸ª tokenï¼‰\n",
    "        start = int(prompt_len[b].item()) - 1\n",
    "        if start < 0: start = 0\n",
    "        action_mask[b, start:] = 1.0\n",
    "\n",
    "    # æ–‡æœ¬è§£ç ç”¨äºå¥–åŠ±\n",
    "    decoded = tokenizer.batch_decode(full_ids, skip_special_tokens=True)\n",
    "    rewards = reward_fn(decoded, cfg.pos_words).to(cfg.device)          # [B]\n",
    "    dones = torch.ones_like(rewards)\n",
    "\n",
    "    # è®¡ç®— returns & advantagesï¼ˆåªåœ¨ action_mask==1 çš„ä½ç½®ï¼‰\n",
    "    returns, advantages = compute_returns_advantages(\n",
    "        rewards=rewards,\n",
    "        values=values_old_trim,\n",
    "        dones=dones,\n",
    "        action_mask=action_mask,\n",
    "        gamma=cfg.gamma,\n",
    "        lam=cfg.gae_lambda,\n",
    "    )\n",
    "\n",
    "    # --------- æŠŠ rollout æ‰“åŒ…ä¸ºè®­ç»ƒ batch ----------\n",
    "    train_batch = {\n",
    "        \"input_ids\": full_ids[:, :-1].detach(),          # å’Œ logits_old_trim å¯¹é½\n",
    "        \"attention_mask\": full_attn[:, :-1].detach(),\n",
    "        \"labels\": labels.detach(),\n",
    "        \"logp_old\": logp_old.detach(),\n",
    "        \"values_old\": values_old_trim.detach(),\n",
    "        \"returns\": returns.detach(),\n",
    "        \"advantages\": advantages.detach(),\n",
    "        \"action_mask\": action_mask.detach(),\n",
    "    }\n",
    "\n",
    "    # --------- PPO å¤šè½®å°æ‰¹é‡æ›´æ–° ----------\n",
    "    for epoch in range(cfg.ppo_epochs):\n",
    "        for mb in iterate_minibatches(train_batch, cfg.minibatch_size):\n",
    "            logits, values = model(mb[\"input_ids\"].to(cfg.device),\n",
    "                                   attention_mask=mb[\"attention_mask\"].to(cfg.device))\n",
    "            # å¯¹é½\n",
    "            logits = logits\n",
    "            values = values\n",
    "\n",
    "            # é‡æ–°è®¡ç®—æ–°ç­–ç•¥çš„ logp\n",
    "            logp_new = compute_logprobs(logits, mb[\"labels\"].to(cfg.device))\n",
    "\n",
    "            # ratio & ç­–ç•¥æŸå¤±ï¼ˆclipï¼‰\n",
    "            ratio = torch.exp(logp_new - mb[\"logp_old\"].to(cfg.device))                   # [B,T]\n",
    "            adv = mb[\"advantages\"].to(cfg.device)\n",
    "            # æ ‡å‡†åŒ–ä¼˜åŠ¿ï¼ˆä»…å¯¹ action_mask ç”Ÿæ•ˆï¼‰\n",
    "            mask = mb[\"action_mask\"].to(cfg.device)\n",
    "            adv_masked = adv[mask > 0]\n",
    "            adv_norm = (adv - adv_masked.mean()) / (adv_masked.std() + 1e-8)\n",
    "\n",
    "            pg1 = ratio * adv_norm\n",
    "            pg2 = torch.clamp(ratio, 1.0 - cfg.clip_eps, 1.0 + cfg.clip_eps) * adv_norm\n",
    "            policy_loss = -(torch.min(pg1, pg2) * mask).sum() / (mask.sum() + 1e-8)\n",
    "\n",
    "            # value æŸå¤±ï¼ˆå¸¦ value clippingï¼‰\n",
    "            values_old = mb[\"values_old\"].to(cfg.device)\n",
    "            returns = mb[\"returns\"].to(cfg.device)\n",
    "            v_clipped = values_old + (values - values_old).clamp(-cfg.value_clip_eps, cfg.value_clip_eps)\n",
    "            v_loss1 = (values - returns) ** 2\n",
    "            v_loss2 = (v_clipped - returns) ** 2\n",
    "            value_loss = 0.5 * torch.max(v_loss1, v_loss2)\n",
    "            value_loss = (value_loss * mask).sum() / (mask.sum() + 1e-8)\n",
    "\n",
    "            # ç†µå¥–åŠ±ï¼ˆè¶Šå¤§è¶Šå¥½ï¼Œè¿™é‡Œç”¨è´Ÿå·å¹¶å…¥lossï¼‰\n",
    "            with torch.no_grad():\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            entropy = -(probs * log_probs).sum(dim=-1)          # [B,T]\n",
    "            entropy_loss = -(entropy * mask).sum() / (mask.sum() + 1e-8)\n",
    "\n",
    "            loss = policy_loss + cfg.c1_value * value_loss + cfg.c2_entropy * entropy_loss\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "            optim.step()\n",
    "\n",
    "    if step % cfg.print_every == 1:\n",
    "        with torch.no_grad():\n",
    "            # ç›‘æ§ä¸€ä¸‹ KLï¼ˆç»éªŒä¸Šç›‘æ§æ—§â†’æ–°ï¼‰\n",
    "            kl = (mb[\"logp_old\"].to(cfg.device) - logp_new).mean().item()\n",
    "            avg_ret = rewards.mean().item()\n",
    "            print(f\"[Step {step}] loss={loss.item():.4f}  policy={policy_loss.item():.4f}  \"\n",
    "                  f\"value={value_loss.item():.4f}  entropy={-entropy_loss.item():.4f}  \"\n",
    "                  f\"KL={kl:.4f}  avg_reward={avg_ret:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089692f0",
   "metadata": {},
   "source": [
    "### è®­ç»ƒç»“æœè§‚å¯Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e72b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sampling BEFORE training ===\n",
      "\n",
      "[Before] Prompt: Write a short product review:\n",
      "Write a short product review: https://www.groupon.org/medicine/apparel\n",
      "\n",
      "Donate to Planned Parenthood - http://\n",
      "\n",
      "\n",
      "[Before] Prompt: Describe your day:\n",
      "Describe your day:\n",
      "\n",
      "Monday, September 13\n",
      "\n",
      "The first thing you should know is that, without an actual work day in the\n",
      "\n",
      "\n",
      "=== Sampling AFTER training ===\n",
      "\n",
      "[Before] Prompt: Write a short product review:\n",
      "Write a short product review:\n",
      "\n",
      "Review a short product review:\n",
      "\n",
      "Write a short product review:\n",
      "\n",
      "Write a short product review:\n",
      "\n",
      "\n",
      "[Before] Prompt: Describe your day:\n",
      "Describe your day: In my opinion, this is a recipe for good wine.Â  It tastes like nothing but sherry.\n",
      "So far\n",
      "\n",
      "\n",
      "[Before] Prompt: Tell me something about your favorite movie:\n",
      "Tell me something about your favorite movie: One where you have children, and you play in an army?\n",
      "\n",
      "Advertisement\n",
      "\n",
      "A: This was a great\n",
      "\n",
      "\n",
      "[Before] Prompt: How was your meal?\n",
      "How was your meal?\n",
      "\n",
      "We ate from lunch with friends in Houston. We got to know each other better than most and I think we\n",
      "\n",
      "[Final Demo]\n",
      "Write a short product review:\n",
      "\n",
      "Review:\n",
      "\n",
      "Review: *What can I add to this place? *Please send me feedback through the\n"
     ]
    }
   ],
   "source": [
    "# ===================== è®­ç»ƒå‰ï¼šè§‚å¯Ÿä¸€æ¬¡ç”Ÿæˆ =====================\n",
    "def sample_text(prompts: List[str], num=2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for p in prompts[:num]:\n",
    "            enc = tokenizer(p, return_tensors=\"pt\").to(cfg.device)\n",
    "            out_ids, _ = model.generate(\n",
    "                enc.input_ids, enc.attention_mask,\n",
    "                max_new_tokens=cfg.max_new_tokens,\n",
    "                temperature=cfg.temperature,\n",
    "                top_k=cfg.top_k,\n",
    "                eos_token_id=cfg.eos_token_id,\n",
    "            )\n",
    "            text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "            print(f\"\\n[Before] Prompt: {p}\\n{text}\\n\")\n",
    "    model.train()\n",
    "\n",
    "print(\"=== Sampling BEFORE training ===\")\n",
    "sample_text(cfg.prompts, num=2)\n",
    "\n",
    "\n",
    "# ===================== è®­ç»ƒåï¼šå†æ¬¡é‡‡æ ·å¯¹æ¯” =====================\n",
    "print(\"\\n=== Sampling AFTER training ===\")\n",
    "sample_text(cfg.prompts, num=4)\n",
    "\n",
    "# å•æ¡æ¼”ç¤ºï¼šçœ‹çœ‹å¥–åŠ±è¯æ˜¯å¦æ›´å¸¸å‡ºç°\n",
    "demo_prompt = \"Write a short product review:\"\n",
    "enc = tokenizer(demo_prompt, return_tensors=\"pt\").to(cfg.device)\n",
    "out_ids, _ = model.generate(\n",
    "    enc.input_ids, enc.attention_mask,\n",
    "    max_new_tokens=cfg.max_new_tokens,\n",
    "    temperature=cfg.temperature,\n",
    "    top_k=cfg.top_k,\n",
    "    eos_token_id=cfg.eos_token_id,\n",
    ")\n",
    "print(\"[Final Demo]\")\n",
    "print(tokenizer.decode(out_ids[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
