{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb09bbc6",
   "metadata": {},
   "source": [
    "\n",
    "### LoRA 是什么？\n",
    "\n",
    "是一种不直接修改预训练权重（$W$），仅通过低秩增量（$\\Delta W$）调整模型效果的微调方法，公式为$W' = W + \\Delta W = W + \\frac{\\alpha}{r}A \\times B$。\n",
    "\n",
    "\n",
    "其中$\\alpha$ 用于控制对原始模型权重更新的强度，过大，会很快适应新任务，但也可能导致过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18b2ff",
   "metadata": {},
   "source": [
    "### lora的优点？\n",
    "1. 微调效率高\n",
    "\n",
    "2. 保留了原始模型的能力。直接修改原始模型W可能会导致在原始任务上灾难性遗忘，lora通过冻结，保留了原始模型的能力。\n",
    "\n",
    "3. 快速切换。任务切换只需要加载不同Lora文件，方便快速。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51838d8d",
   "metadata": {},
   "source": [
    "### Lora为什么有效？\n",
    "\n",
    "大模型预训练权重已经包含了通用知识，微调时只需要对权重矩阵做任务特定的稀疏调整（ΔW）。LoRA通过低秩矩阵A和B来逼近这个更新（ΔW≈A·B），这类似于用可训练参数实现了一种数据驱动的低秩近似，而不需要显式计算SVD分解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a89eee",
   "metadata": {},
   "source": [
    "### LoRA 矩阵初始化方式?\n",
    "下采样矩阵A为随机高斯初始化（非零）；上采样矩阵 B为全零初始化。\n",
    "\n",
    "如此实现，$\\Delta W = A \\cdot B = 0$（因 B 全零），模型初始权重 = 预训练权重 $W$。\n",
    "\n",
    "\n",
    "- A和B可以都为0吗？ 不可以，因为那样会导致梯度都为0，无法更新。\n",
    "- 可以反过来吗？ 有篇论文（The Impact of Initialization on LoRA Finetuning Dynamics）证明了可以，但是A初始有值效果会更好。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e29c1",
   "metadata": {},
   "source": [
    "## lora代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b3f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 10, 12])\n",
      "输出形状: torch.Size([2, 10, 12])\n",
      "训练后LoRA B参数示例（非零，说明更新成功）: \n",
      "tensor([[-1.0000e-04, -1.0000e-04],\n",
      "        [-1.0000e-04, -1.0000e-04]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank=4, alpha=8):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.rank = rank\n",
    "        # A: 随机高斯初始化（标准差=1/√rank）\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) / torch.sqrt(torch.tensor(rank)))\n",
    "        # B: 全零初始化\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ΔW = (A·B) * (alpha/rank)，缩放保证训练稳定性\n",
    "        return self.alpha / self.rank * (x @ self.A @ self.B)\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear_layer, rank=4, alpha=8):\n",
    "        super().__init__()\n",
    "        self.linear = linear_layer  # 冻结的预训练线性层\n",
    "        self.lora = LoRALayer(\n",
    "            in_dim=linear_layer.in_features,\n",
    "            out_dim=linear_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "# --------------------------- 测试运行 ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 模拟一个预训练线性层（例如Transformer中的QKV投影层）\n",
    "    pretrained_linear = nn.Linear(in_features=12, out_features=12)\n",
    "    # 2. 包装LoRA（rank=4, alpha=8）\n",
    "    lora_linear = LinearWithLoRA(pretrained_linear, rank=4, alpha=8)\n",
    "    # 3. 随机输入（batch_size=2, seq_len=10, hidden_dim=128）\n",
    "    x = torch.randn(2, 10, 12)\n",
    "    # 4. 前向传播（测试输出形状）\n",
    "    output = lora_linear(x)\n",
    "    print(f\"输入形状: {x.shape}\")       # torch.Size([2, 10, 12])\n",
    "    print(f\"输出形状: {output.shape}\")  # torch.Size([2, 10, 12]) （形状匹配）\n",
    "\n",
    "    # 5. 简化训练逻辑（仅示意参数更新）\n",
    "    optimizer = optim.Adam(lora_linear.lora.parameters(), lr=1e-4)  # 仅优化LoRA参数\n",
    "    loss_fn = nn.MSELoss()\n",
    "    target = torch.randn_like(output)  # 模拟目标值\n",
    "\n",
    "    # 前向+反向+更新\n",
    "    optimizer.zero_grad()\n",
    "    pred = lora_linear(x)\n",
    "    loss = loss_fn(pred, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"训练后LoRA B参数示例（非零，说明更新成功）: \\n{lora_linear.lora.B[:2, :2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
