{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9665fc1c",
   "metadata": {},
   "source": [
    "## 1. 多头注意力机制\n",
    "\n",
    "**为什么多头注意力（Multi-Head Attention）有效？**\n",
    "\n",
    "多头注意力通过并行计算多个独立的注意力头，让模型从不同角度（如局部依赖、长程依赖、语法结构等）学习输入序列的多样化特征，从而提升模型的表达能力和泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1817b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 18])\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, hidden_dim=5, head=2,**kwargs):\n",
    "        super().__init__()\n",
    "        self.qkv_proj = nn.Linear(hidden_dim, 3 * hidden_dim)  # 合并QKV投影以提高效率\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.head=head\n",
    "\n",
    "    def softmax(self,x):\n",
    "        x_max=torch.max(x,dim=-1,keepdim=True).values\n",
    "        x_exp=torch.exp(x-x_max)\n",
    "        x_exp_sum=torch.sum(x_exp,dim=-1,keepdim=True)\n",
    "        \n",
    "        return x_exp/x_exp_sum\n",
    "\n",
    "    def forward(self,x):\n",
    "        bs,l,dim=x.shape\n",
    "        assert dim ==self.hidden_dim\n",
    "        qkv=self.qkv_proj(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # 正确方式\n",
    "        # q,k,v=qkv.unbind(2)\n",
    "        dim0=dim//self.head\n",
    "        q=q.reshape(bs,l,self.head,dim0).transpose(1,2)\n",
    "        k=k.reshape(bs,l,self.head,dim0).transpose(1,2)\n",
    "        v=v.reshape(bs,l,self.head,dim0).transpose(1,2)\n",
    "\n",
    "        qk=torch.matmul(q,k.transpose(2,3))/(dim0**0.5)\n",
    "        # qk=qk/torch.sqrt(dim0)\n",
    "\n",
    "        qk=self.softmax(qk)\n",
    "\n",
    "        atten=torch.matmul(qk,v)    \n",
    "        return atten.transpose(1,2).reshape(bs,l,dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "x=torch.randn(4,10,6)  \n",
    "mha=MHA(hidden_dim=6,head=2)\n",
    "out=mha(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a9646",
   "metadata": {},
   "source": [
    "## 2. 稀疏注意力（Sparse Attention）在保持模型性能的同时，通过减少计算量来提高效率，其效果不会显著下降的原因可以归结为以下几点？\n",
    "\n",
    "1. 在标准注意力中，大部分位置的注意力权重接近于零，只有少数位置对当前token有显著贡献。\n",
    "2. 稀疏注意力可视为对稠密注意力矩阵的低秩近似（保留主要特征，忽略次要特征），理论证明这种近似在多数任务中误差可控。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26da0e",
   "metadata": {},
   "source": [
    "## 3. 局部窗口类稀疏机制\n",
    "\n",
    "每个 Query 仅与前后 k 个相邻元素计算注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa993e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 1024, 512])\n",
      "Output shape: torch.Size([4, 1024, 512])\n",
      "理论显存节省比例: 99.32%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EfficientBandMHA(nn.Module):\n",
    "    def __init__(self, hidden_dim=5, head=4, band_width=2, **kwargs):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head = head\n",
    "        self.head_dim = hidden_dim // head\n",
    "        self.band_width = band_width  # 每个位置仅关注前后band_width个位置\n",
    "        \n",
    "        # 投影层\n",
    "        self.qkv_proj = nn.Linear(hidden_dim, 3 * hidden_dim)  # 合并QKV投影以提高效率\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        assert hidden_dim % head == 0, \"hidden_dim must be divisible by head\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, dim = x.shape\n",
    "        \n",
    "        # 一次性计算QKV并拆分 (优化内存访问)\n",
    "        qkv = self.qkv_proj(x).reshape(bs, seq_len, 3, self.head, self.head_dim)\n",
    "        q, k, v = qkv.unbind(2)  # 拆分出Q, K, V，形状均为 (bs, seq_len, head, head_dim)\n",
    "        \n",
    "        # 转置为 (bs, head, seq_len, head_dim)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # 计算带状注意力的关键：只生成有效区域的注意力分数\n",
    "        # 1. 确定每个位置i的有效注意力范围\n",
    "        # 2. 仅计算这些位置的Q-K乘积，避免完整的n×n矩阵\n",
    "        \n",
    "        # 存储每个头的注意力结果\n",
    "        attn_outputs = []\n",
    "        \n",
    "        for h in range(self.head):\n",
    "            # 当前头的Q, K, V\n",
    "            q_h = q[:, h]  # (bs, seq_len, head_dim)\n",
    "            k_h = k[:, h]  # (bs, seq_len, head_dim)\n",
    "            v_h = v[:, h]  # (bs, seq_len, head_dim)\n",
    "            \n",
    "            # 存储当前头的注意力分数\n",
    "            sparse_attn_scores = []\n",
    "            \n",
    "            for i in range(seq_len):\n",
    "                # 计算位置i的有效范围 [start, end)\n",
    "                start = max(0, i - self.band_width)\n",
    "                end = min(seq_len, i + self.band_width + 1)\n",
    "                band_len = end - start\n",
    "                \n",
    "                # 仅计算Q[i]与K[start:end]的乘积 (节约计算)\n",
    "                q_i = q_h[:, i:i+1, :]  # (bs, 1, head_dim)\n",
    "                k_band = k_h[:, start:end, :]  # (bs, band_len, head_dim)\n",
    "                \n",
    "                # 计算注意力分数 (bs, 1, band_len)\n",
    "                scores = torch.matmul(q_i, k_band.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "                \n",
    "                # 对带状区域内的分数做softmax\n",
    "                scores_softmax = F.softmax(scores, dim=-1)  # (bs, 1, band_len)\n",
    "                \n",
    "                # 计算注意力加权和 (bs, 1, head_dim)\n",
    "                v_band = v_h[:, start:end, :]  # (bs, band_len, head_dim)\n",
    "                attn_i = torch.matmul(scores_softmax, v_band)  # (bs, 1, head_dim)\n",
    "                \n",
    "                sparse_attn_scores.append(attn_i)\n",
    "            \n",
    "            # 拼接当前头的所有位置结果 (bs, seq_len, head_dim)\n",
    "            attn_head = torch.cat(sparse_attn_scores, dim=1)\n",
    "            attn_outputs.append(attn_head)\n",
    "        \n",
    "        # 合并所有头 (bs, seq_len, hidden_dim)\n",
    "        attn_output = torch.cat(attn_outputs, dim=-1)\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# 测试显存占用对比\n",
    "if __name__ == \"__main__\":\n",
    "    # 生成较长序列以观察显存差异\n",
    "    bs, seq_len, hidden_dim = 4, 1024, 512\n",
    "    x = torch.randn(bs, seq_len, hidden_dim)  # 使用GPU测试显存\n",
    "        \n",
    "    # 高效带状注意力\n",
    "    band_width=3\n",
    "    efficient_band_mha = EfficientBandMHA(\n",
    "        hidden_dim=hidden_dim, \n",
    "        head=8, \n",
    "        band_width=band_width\n",
    "    )\n",
    "    \n",
    "    # 测试前向传播\n",
    "    with torch.no_grad():\n",
    "        out = efficient_band_mha(x)\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        print(f\"Output shape: {out.shape}\")\n",
    "        print(f\"理论显存节省比例: {1 - (2*band_width + 1)/seq_len:.2%}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ab86b3",
   "metadata": {},
   "source": [
    "## 4. 多查询注意力机制\n",
    "MQA 的核心特点是所有注意力头共享同一组 Key 和 Value，只保留独立的 Query，这样可以显著减少参数量和计算量，同时降低推理时的显存占用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc232e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 128, 512])\n",
      "Output shape: torch.Size([4, 128, 512])\n",
      "MQA参数数量: 590976\n",
      "标准MHA参数数量: 1050624\n",
      "参数减少比例: 43.75%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MQA(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, num_heads=8, **kwargs):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # 关键区别：MQA中所有头共享K和V\n",
    "        # Query需要为每个头单独投影\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)  # (hidden_dim, num_heads * head_dim)\n",
    "        \n",
    "        # Key和Value只需要一组投影（所有头共享）\n",
    "        self.k_proj = nn.Linear(hidden_dim, self.head_dim)  # (hidden_dim, head_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, self.head_dim)  # (hidden_dim, head_dim)\n",
    "        \n",
    "        # 输出投影\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        bs, seq_len, dim = x.shape\n",
    "        assert dim == self.hidden_dim\n",
    "        \n",
    "        # 计算Query、Key、Value\n",
    "        q = self.q_proj(x)  # (bs, seq_len, hidden_dim)\n",
    "        k = self.k_proj(x)  # (bs, seq_len, head_dim) - 所有头共享\n",
    "        v = self.v_proj(x)  # (bs, seq_len, head_dim) - 所有头共享\n",
    "        \n",
    "        # 重塑Query以适应多头 (每个头有独立的Query)\n",
    "        # (bs, seq_len, num_heads, head_dim) -> (bs, num_heads, seq_len, head_dim)\n",
    "        q = q.view(bs, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 为Key和Value增加头维度（所有头共享同一组K/V）\n",
    "        # (bs, seq_len, head_dim) -> (bs, 1, seq_len, head_dim) -> (bs, num_heads, seq_len, head_dim)\n",
    "        k = k.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
    "        v = v.unsqueeze(1).repeat(1, self.num_heads, 1, 1)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        # (bs, num_heads, seq_len, seq_len)\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # 应用掩码（如果提供）\n",
    "        if attn_mask is not None:\n",
    "            attn_scores = attn_scores + attn_mask\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # 应用注意力到Value\n",
    "        # (bs, num_heads, seq_len, head_dim)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # 合并多头\n",
    "        # (bs, seq_len, num_heads, head_dim) -> (bs, seq_len, hidden_dim)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(bs, seq_len, self.hidden_dim)\n",
    "        \n",
    "        # 输出投影\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置\n",
    "    batch_size = 4\n",
    "    seq_length = 128\n",
    "    hidden_dim = 512\n",
    "    num_heads = 8\n",
    "    \n",
    "    # 创建随机输入\n",
    "    x = torch.randn(batch_size, seq_length, hidden_dim)\n",
    "    \n",
    "    # 初始化MQA\n",
    "    mqa = MQA(hidden_dim=hidden_dim, num_heads=num_heads)\n",
    "    \n",
    "    # 前向传播\n",
    "    output = mqa(x)\n",
    "    \n",
    "    # 验证输出形状\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    assert output.shape == x.shape, \"Output shape mismatch\"\n",
    "    \n",
    "    # 计算参数量对比（与标准MHA相比）\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    mqa_params = count_parameters(mqa)\n",
    "    \n",
    "    # 标准多头注意力作为对比\n",
    "    class StandardMHA(nn.Module):\n",
    "        def __init__(self, hidden_dim, num_heads):\n",
    "            super().__init__()\n",
    "            self.mha = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        def forward(self, x):\n",
    "            return self.mha(x, x, x)[0]\n",
    "    \n",
    "    standard_mha = StandardMHA(hidden_dim, num_heads)\n",
    "    standard_params = count_parameters(standard_mha)\n",
    "    \n",
    "    print(f\"MQA参数数量: {mqa_params}\")\n",
    "    print(f\"标准MHA参数数量: {standard_params}\")\n",
    "    print(f\"参数减少比例: {1 - mqa_params/standard_params:.2%}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495b7f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入提示: [[10, 52, 93, 47, 50]]\n",
      "生成结果: [[10, 52, 93, 47, 50, 759, 900, 188, 837, 106, 328, 401, 176, 204, 185, 567, 853, 29, 246, 457, 586, 649, 129, 615, 339, 615, 33, 277, 904, 752, 866, 229, 91, 971, 507, 295, 370, 27, 706, 751, 761, 800, 303, 760, 681, 486, 446, 820, 596, 239, 824, 447, 748, 336, 121, 540, 746, 752, 142, 387, 739, 177, 688, 637, 0]]\n",
      "生成序列长度: 65, 成功避免缓存越界问题!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class KVCache:\n",
    "    \"\"\"动态扩展的KV缓存管理\"\"\"\n",
    "    def __init__(self, max_size=None):\n",
    "        self.k_cache = None  # 存储Key缓存\n",
    "        self.v_cache = None  # 存储Value缓存\n",
    "        self.cur_len = 0     # 当前缓存长度\n",
    "        self.max_size = max_size  # 最大缓存长度，None表示无限制\n",
    "\n",
    "    def update(self, k, v):\n",
    "        \"\"\"\n",
    "        更新缓存，动态扩展容量\n",
    "        k: 新的Key，形状 [batch, n_heads, 1, head_dim]\n",
    "        v: 新的Value，形状 [batch, n_heads, 1, head_dim]\n",
    "        \"\"\"\n",
    "        batch, n_heads, _, head_dim = k.shape\n",
    "        \n",
    "        # 初始化缓存（如果尚未初始化）\n",
    "        if self.k_cache is None:\n",
    "            # 初始容量设为100，或根据max_size调整\n",
    "            init_size = self.max_size if self.max_size else 100\n",
    "            self.k_cache = torch.empty(\n",
    "                (batch, n_heads, init_size, head_dim), \n",
    "                device=k.device, \n",
    "                dtype=k.dtype\n",
    "            )\n",
    "            self.v_cache = torch.empty(\n",
    "                (batch, n_heads, init_size, head_dim), \n",
    "                device=v.device, \n",
    "                dtype=v.dtype\n",
    "            )\n",
    "        \n",
    "        # 检查是否需要扩展缓存\n",
    "        if self.cur_len >= self.k_cache.shape[2]:\n",
    "            if self.max_size:\n",
    "                # 如果设置了最大长度，超出则滚动覆盖最旧的元素\n",
    "                self.k_cache = torch.cat([self.k_cache[:, :, 1:], k], dim=2)\n",
    "                self.v_cache = torch.cat([self.v_cache[:, :, 1:], v], dim=2)\n",
    "            else:\n",
    "                # 动态扩展缓存容量（翻倍）\n",
    "                new_size = self.k_cache.shape[2] * 2\n",
    "                new_k_cache = torch.empty(\n",
    "                    (batch, n_heads, new_size, head_dim),\n",
    "                    device=k.device,\n",
    "                    dtype=k.dtype\n",
    "                )\n",
    "                new_v_cache = torch.empty(\n",
    "                    (batch, n_heads, new_size, head_dim),\n",
    "                    device=v.device,\n",
    "                    dtype=v.dtype\n",
    "                )\n",
    "                new_k_cache[:, :, :self.cur_len] = self.k_cache\n",
    "                new_v_cache[:, :, :self.cur_len] = self.v_cache\n",
    "                self.k_cache = new_k_cache\n",
    "                self.v_cache = new_v_cache\n",
    "        \n",
    "        # 更新缓存内容\n",
    "        self.k_cache[:, :, self.cur_len:self.cur_len+1] = k\n",
    "        self.v_cache[:, :, self.cur_len:self.cur_len+1] = v\n",
    "        self.cur_len += 1\n",
    "        \n",
    "        # 返回当前完整缓存\n",
    "        return self.k_cache[:, :, :self.cur_len], self.v_cache[:, :, :self.cur_len]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"重置缓存\"\"\"\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "        self.cur_len = 0\n",
    "\n",
    "\n",
    "class AttentionWithCache(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hidden_dim // n_heads\n",
    "        \n",
    "        # MQA: 所有头共享K和V\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, self.head_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, self.head_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, cache=None):\n",
    "        batch, seq_len, _ = x.shape\n",
    "        \n",
    "        # 计算Q, K, V\n",
    "        q = self.q_proj(x).view(batch, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch, seq_len, 1, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch, seq_len, 1, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 扩展K和V到所有头（共享）\n",
    "        k = k.repeat(1, self.n_heads, 1, 1)\n",
    "        v = v.repeat(1, self.n_heads, 1, 1)\n",
    "        \n",
    "        # 处理缓存\n",
    "        if cache is not None:\n",
    "            # 更新缓存并获取所有历史KV\n",
    "            k, v = cache.update(k[:, :, -1:], v[:, :, -1:])\n",
    "        \n",
    "        # 计算注意力\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # 应用因果掩码\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, k.size(2), device=x.device), diagonal=1).bool()\n",
    "        attn_scores = attn_scores.masked_fill(causal_mask[None, None, :, :], float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # 合并多头\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch, seq_len, self.hidden_dim)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.attn = AttentionWithCache(hidden_dim, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 4 * hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cache=None):\n",
    "        # 自注意力 + 残差连接\n",
    "        x = x + self.attn(self.norm1(x), cache)\n",
    "        # 前馈网络 + 残差连接\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=128, n_layers=2, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(hidden_dim, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.head = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, cache=None):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cache)\n",
    "        x = self.norm(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "def generate(model, prompt, max_length=50, temperature=1.0, top_k=30):\n",
    "    \"\"\"生成函数，使用KV缓存加速推理\"\"\"\n",
    "    model.eval()\n",
    "    generated = prompt.clone()\n",
    "    batch_size = prompt.size(0)\n",
    "    \n",
    "    # 初始化缓存\n",
    "    cache = KVCache()\n",
    "    \n",
    "    # 首先处理初始提示\n",
    "    with torch.no_grad():\n",
    "        # 计算初始提示的输出和缓存\n",
    "        logits = model(generated, cache)\n",
    "        \n",
    "    # 自回归生成后续token\n",
    "    for _ in range(max_length - generated.size(1)):\n",
    "        # 只关注最后一个token的输出\n",
    "        next_logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Top-k采样\n",
    "        if top_k is not None:\n",
    "            top_k_values, top_k_indices = torch.topk(next_logits, top_k)\n",
    "            next_logits = torch.full_like(next_logits, float('-inf'))\n",
    "            next_logits.scatter_(1, top_k_indices, top_k_values)\n",
    "        \n",
    "        # 计算概率并采样\n",
    "        probs = F.softmax(next_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # 追加到生成序列\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        \n",
    "        # 使用缓存进行下一步预测\n",
    "        with torch.no_grad():\n",
    "            logits = model(next_token, cache)\n",
    "        \n",
    "        # 检查是否生成结束符（假设0是结束符）\n",
    "        if (next_token == 0).all():\n",
    "            break\n",
    "    \n",
    "    return generated\n",
    "\n",
    "\n",
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置参数\n",
    "    VOCAB_SIZE = 1000\n",
    "    HIDDEN_DIM = 128\n",
    "    N_LAYERS = 2\n",
    "    N_HEADS = 4\n",
    "    MAX_LENGTH = 100  # 测试长序列生成，验证缓存扩展功能\n",
    "    \n",
    "    # 创建模型\n",
    "    model = SimpleTransformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        n_layers=N_LAYERS,\n",
    "        n_heads=N_HEADS\n",
    "    )\n",
    "    \n",
    "    # 创建初始提示（随机整数序列）\n",
    "    prompt = torch.randint(1, 100, (1, 5))  # 避免使用0（假设0是结束符）\n",
    "    print(\"输入提示:\", prompt.tolist())\n",
    "    \n",
    "    # 生成文本\n",
    "    generated = generate(model, prompt, max_length=MAX_LENGTH)\n",
    "    \n",
    "    print(\"生成结果:\", generated.tolist())\n",
    "    print(f\"生成序列长度: {generated.size(1)}, 成功避免缓存越界问题!\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamafactory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
