{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9665fc1c",
   "metadata": {},
   "source": [
    "## 1. 多头注意力机制\n",
    "\n",
    "**为什么多头注意力（Multi-Head Attention）有效？**\n",
    "\n",
    "多头注意力通过并行计算多个独立的注意力头，让模型从不同角度（如局部依赖、长程依赖、语法结构等）学习输入序列的多样化特征，从而提升模型的表达能力和泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1817b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 18])\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, hidden_dim=5, head=2,**kwargs):\n",
    "        super().__init__()\n",
    "        self.qkv_proj = nn.Linear(hidden_dim, 3 * hidden_dim)  # 合并QKV投影以提高效率\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.head=head\n",
    "\n",
    "    def softmax(self,x):\n",
    "        x_max=torch.max(x,dim=-1,keepdim=True).values\n",
    "        x_exp=torch.exp(x-x_max)\n",
    "        x_exp_sum=torch.sum(x_exp,dim=-1,keepdim=True)\n",
    "        \n",
    "        return x_exp/x_exp_sum\n",
    "\n",
    "    def forward(self,x):\n",
    "        bs,l,dim=x.shape\n",
    "        assert dim ==self.hidden_dim\n",
    "        qkv=self.qkv_proj(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # 正确方式\n",
    "        # q,k,v=qkv.unbind(2)\n",
    "        dim0=dim//self.head\n",
    "        q=q.reshape(bs,l,self.head,dim0).transpose(1,2)\n",
    "        k=k.reshape(bs,l,self.head,dim0).transpose(1,2)\n",
    "        v=v.reshape(bs,l,self.head,dim0).transpose(1,2)\n",
    "\n",
    "        qk=torch.matmul(q,k.transpose(2,3))/(dim0**0.5)\n",
    "        # qk=qk/torch.sqrt(dim0)\n",
    "\n",
    "        qk=self.softmax(qk)\n",
    "\n",
    "        atten=torch.matmul(qk,v)    \n",
    "        return atten.transpose(1,2).reshape(bs,l,dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "x=torch.randn(4,10,6)  \n",
    "mha=MHA(hidden_dim=6,head=2)\n",
    "out=mha(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a9646",
   "metadata": {},
   "source": [
    "## 2. 稀疏注意力（Sparse Attention）在保持模型性能的同时，通过减少计算量来提高效率，其效果不会显著下降的原因可以归结为以下几点？\n",
    "\n",
    "1. 在标准注意力中，大部分位置的注意力权重接近于零，只有少数位置对当前token有显著贡献。\n",
    "2. 稀疏注意力可视为对稠密注意力矩阵的低秩近似（保留主要特征，忽略次要特征），理论证明这种近似在多数任务中误差可控。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26da0e",
   "metadata": {},
   "source": [
    "## 3. 局部窗口类稀疏机制\n",
    "\n",
    "每个 Query 仅与前后 k 个相邻元素计算注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa993e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 1024, 512])\n",
      "Output shape: torch.Size([4, 1024, 512])\n",
      "理论显存节省比例: 99.32%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EfficientBandMHA(nn.Module):\n",
    "    def __init__(self, hidden_dim=5, head=4, band_width=2, **kwargs):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head = head\n",
    "        self.head_dim = hidden_dim // head\n",
    "        self.band_width = band_width  # 每个位置仅关注前后band_width个位置\n",
    "        \n",
    "        # 投影层\n",
    "        self.qkv_proj = nn.Linear(hidden_dim, 3 * hidden_dim)  # 合并QKV投影以提高效率\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        assert hidden_dim % head == 0, \"hidden_dim must be divisible by head\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, dim = x.shape\n",
    "        \n",
    "        # 一次性计算QKV并拆分 (优化内存访问)\n",
    "        qkv = self.qkv_proj(x).reshape(bs, seq_len, 3, self.head, self.head_dim)\n",
    "        q, k, v = qkv.unbind(2)  # 拆分出Q, K, V，形状均为 (bs, seq_len, head, head_dim)\n",
    "        \n",
    "        # 转置为 (bs, head, seq_len, head_dim)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # 计算带状注意力的关键：只生成有效区域的注意力分数\n",
    "        # 1. 确定每个位置i的有效注意力范围\n",
    "        # 2. 仅计算这些位置的Q-K乘积，避免完整的n×n矩阵\n",
    "        \n",
    "        # 存储每个头的注意力结果\n",
    "        attn_outputs = []\n",
    "        \n",
    "        for h in range(self.head):\n",
    "            # 当前头的Q, K, V\n",
    "            q_h = q[:, h]  # (bs, seq_len, head_dim)\n",
    "            k_h = k[:, h]  # (bs, seq_len, head_dim)\n",
    "            v_h = v[:, h]  # (bs, seq_len, head_dim)\n",
    "            \n",
    "            # 存储当前头的注意力分数\n",
    "            sparse_attn_scores = []\n",
    "            \n",
    "            for i in range(seq_len):\n",
    "                # 计算位置i的有效范围 [start, end)\n",
    "                start = max(0, i - self.band_width)\n",
    "                end = min(seq_len, i + self.band_width + 1)\n",
    "                band_len = end - start\n",
    "                \n",
    "                # 仅计算Q[i]与K[start:end]的乘积 (节约计算)\n",
    "                q_i = q_h[:, i:i+1, :]  # (bs, 1, head_dim)\n",
    "                k_band = k_h[:, start:end, :]  # (bs, band_len, head_dim)\n",
    "                \n",
    "                # 计算注意力分数 (bs, 1, band_len)\n",
    "                scores = torch.matmul(q_i, k_band.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "                \n",
    "                # 对带状区域内的分数做softmax\n",
    "                scores_softmax = F.softmax(scores, dim=-1)  # (bs, 1, band_len)\n",
    "                \n",
    "                # 计算注意力加权和 (bs, 1, head_dim)\n",
    "                v_band = v_h[:, start:end, :]  # (bs, band_len, head_dim)\n",
    "                attn_i = torch.matmul(scores_softmax, v_band)  # (bs, 1, head_dim)\n",
    "                \n",
    "                sparse_attn_scores.append(attn_i)\n",
    "            \n",
    "            # 拼接当前头的所有位置结果 (bs, seq_len, head_dim)\n",
    "            attn_head = torch.cat(sparse_attn_scores, dim=1)\n",
    "            attn_outputs.append(attn_head)\n",
    "        \n",
    "        # 合并所有头 (bs, seq_len, hidden_dim)\n",
    "        attn_output = torch.cat(attn_outputs, dim=-1)\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# 测试显存占用对比\n",
    "if __name__ == \"__main__\":\n",
    "    # 生成较长序列以观察显存差异\n",
    "    bs, seq_len, hidden_dim = 4, 1024, 512\n",
    "    x = torch.randn(bs, seq_len, hidden_dim)  # 使用GPU测试显存\n",
    "        \n",
    "    # 高效带状注意力\n",
    "    band_width=3\n",
    "    efficient_band_mha = EfficientBandMHA(\n",
    "        hidden_dim=hidden_dim, \n",
    "        head=8, \n",
    "        band_width=band_width\n",
    "    )\n",
    "    \n",
    "    # 测试前向传播\n",
    "    with torch.no_grad():\n",
    "        out = efficient_band_mha(x)\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        print(f\"Output shape: {out.shape}\")\n",
    "        print(f\"理论显存节省比例: {1 - (2*band_width + 1)/seq_len:.2%}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamafactory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
