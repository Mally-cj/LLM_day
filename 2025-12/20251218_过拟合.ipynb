{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f788ed6",
   "metadata": {},
   "source": [
    "- **Description**：当数据量少，训练一个大参数模型，如何避免过拟合\n",
    "\n",
    "- **Log**\n",
    "  - 2025-12-18 23:30:01 Thursday：first init\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c593b25",
   "metadata": {},
   "source": [
    "凝练答案：\n",
    "> 小数据下训练大模型容易过拟合，本质是模型容量远大于数据的信息量。工程上通常从三层入手：\n",
    "> \n",
    "> 第一，通过数据增强或多任务学习增加有效信息；\n",
    "> \n",
    "> 第二，在模型层面通过预训练和参数高效微调显式限制可学习自由度；\n",
    "> \n",
    "> 第三，在训练过程中通过正则化和 early stopping 约束优化，防止模型记忆训练集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4839b3ea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 一、数据层面：增加“有效信息量”（Information）\n",
    "\n",
    "> 核心目标：**让数据对模型的约束变强**\n",
    "\n",
    "### 1. 数据增强 / 合成数据\n",
    "\n",
    "* 在**不改变语义/标签**的前提下扩充样本：\n",
    "\n",
    "  * CV：旋转、裁剪、噪声\n",
    "  * NLP：回译、同义替换\n",
    "  * 物理/时序问题：基于规则或正演的扰动\n",
    "* 本质：人为引入**等价样本约束**\n",
    "\n",
    "### 2. 半监督 / 弱监督\n",
    "\n",
    "* 利用无标签数据（伪标签、多视角一致性）\n",
    "* 本质：**提高样本覆盖面，而非记忆单点**\n",
    "\n",
    "### 3. 多任务学习（也可放这里）\n",
    "\n",
    "* 引入相关辅助任务\n",
    "* 本质：用额外任务**约束表征空间**\n",
    "\n",
    "---\n",
    "\n",
    "## 二、模型层面：约束“模型容量”（Capacity）\n",
    "\n",
    "> 核心目标：**减少可学习自由度，而不是削弱表达能力**\n",
    "\n",
    "### 1. 预训练 + 冻结（强先验）\n",
    "\n",
    "* 使用大规模预训练权重\n",
    "* 冻结主干，只做小幅调整\n",
    "* 本质：将“从零学习”变成“分布对齐”\n",
    "\n",
    "### 2. 参数高效微调（PEFT）\n",
    "\n",
    "* LoRA / Adapter / Prefix Tuning\n",
    "* 只允许模型在**低秩或低维子空间中更新**\n",
    "* 本质：**显式降低参数自由度**\n",
    "\n",
    "### 3. 架构级容量控制\n",
    "\n",
    "* 缩小模型规模（层数 / 宽度 / latent 维度）\n",
    "* 使用更强 inductive bias 的结构（如 CNN、物理约束模块）\n",
    "* 本质：**直接收缩假设空间**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、训练层面：约束“优化过程”（Optimization）\n",
    "\n",
    "> 核心目标：**防止优化器把有限数据“记死”**\n",
    "\n",
    "### 1. 显式正则化\n",
    "\n",
    "* L2 / Weight Decay\n",
    "* L1（少用，但可提）\n",
    "* 本质：偏好简单解\n",
    "\n",
    "### 2. 隐式正则化\n",
    "\n",
    "* Dropout\n",
    "* 随机性训练策略\n",
    "* 本质：破坏共适应，提升鲁棒性\n",
    "\n",
    "### 3. Early Stopping（非常关键）\n",
    "\n",
    "* 在验证集性能开始下降前停止训练\n",
    "* **不改变模型容量，但限制有效拟合复杂度**\n",
    "* 小数据场景下性价比极高\n",
    "\n",
    "---\n",
    "\n",
    "## 四、工程决策顺序\n",
    "\n",
    "```text\n",
    "小数据 + 大模型\n",
    "        ↓\n",
    "① 是否能用预训练？\n",
    "   → 能：冻结 + PEFT\n",
    "   → 不能：缩模型\n",
    "        ↓\n",
    "② 容量是否仍过大？\n",
    "   → LoRA / Adapter / 更强结构先验\n",
    "        ↓\n",
    "③ 训练是否不稳？\n",
    "   → 正则化 + Early Stopping + 数据增强\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
